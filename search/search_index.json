{"config":{"lang":["en","fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p> Welcome around and thank you for visiting!</p>"},{"location":"#open-source-community-service","title":"Open Source community service","text":""},{"location":"#gentoo-linux-developer","title":"Gentoo Linux developer","text":"<p>I've been a Gentoo Linux developer for more than 10 years now. I mainly focus on packaging distributed databases ecosystems and cluster related projects. I'm also part of the team who provides gentoo linux docker containers.</p>"},{"location":"#open-source-author-and-contributor","title":"Open Source author and contributor","text":"<p>I'm strongly involved in various Open Source communities and am spending a fair amount of my time contributing to Open Source projects. GitHub recognized me as one of the 900 Open Source maintainers they rely on to run their platform.</p> <p>I am the author of several Open Source projects, for instance:</p> <ul> <li>py3status: an i3wm i3status wrapper written in Python</li> <li>uhashring: a full featured consistent hashing Python library compatible with ketama, this project has been designated a critical project on PyPI</li> <li>mkdocs-static-i18n: a MkDocs plugin that allows you to create localized versions of your documentation (or even websites such as mine)</li> <li>matterhook: a Python library to interact with Mattermost Webhooks</li> </ul> <p>This is a quick list of projects I also contribute to:</p> <ul> <li>MkDocs</li> <li>Apache Airflow</li> <li>Apache Avro (Rust)</li> <li>MongoDB / PyMongo</li> <li>Python asyncio Kafka library</li> <li>Scylla / Scylla Python Driver</li> </ul>"},{"location":"#tech-speaker-writer","title":"Tech speaker &amp; writer","text":"<p>I've been fortunate since some of my talk proposals have been accepted over the last years at tech conferences where I got to participate as a speaker.</p> <p>This is very important for me because I believe that real world experience is never as useful as when it's shared with your peers and community.</p> <p>I also write blog posts about some of the technical challenges I face and some Open Source news in the hope it is of interest or help to someone.</p>"},{"location":"#awarded-community-member","title":"Awarded community member","text":"<p>I've been very humbled to be interviewed and awarded for my community work and professional experience.</p>"},{"location":"#fpv-hobby","title":"FPV Hobby","text":"<p>When I'm not hacking computers and code I like to hack quadcopters and to fly them in FPV!</p> <p>I share my modest experience on this hobby through my FPV Handbook and FPV Builds guides.</p>"},{"location":"#support-me","title":"Support me","text":"<p>If my work impacts you and make you happy you can say thank you by starring  one of my repositories:</p> <p></p> <p>You can also consider tipping me via Paypal or Github Sponsor.</p>"},{"location":"#contact-me","title":"Contact me","text":"<ul> <li>On Twitter</li> <li>On Mastodon</li> <li>On IRC Libera.Chat #gentoo-containers or IRC OFTC #py3status</li> <li>On Discord ultrabug</li> </ul>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/","title":"Apex 5\" HD on base frame kit","text":""},{"location":"FPV%20Builds/ImpulseRC%20Apex/#a-digital-hd-build-on-the-apex-base-frame-kit","title":"A digital HD build on the Apex Base Frame Kit","text":"<p> For my first DIY build ever I wanted a strong 5\" frame.</p> <p>I chose the ImpulseRC Apex Base Frame Kit because I did not want to rely on a HD specific frame and loose the space made for the DJI Air Unit.</p> <p>It was quite a challenge to get everything to fit properly but I love the result and I hope you'll appreciate the density and great balance of this build.</p> <p>This ends up as a compact (but not too much) build without sacrificing the space for the electronics. It is clean (to my standards at least) and very powerful.</p> <p>I enjoyed working on it very much and I hope you'll get a sense of it reading this build guide. Enjoy!</p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#hardware-list","title":"Hardware list","text":"<p>Note</p> <p>Those links are NOT affiliate links. I just share the links where I actually bought the stuff needed for this build.</p> <ul> <li> ImpulseRC Apex Base Frame Kit<ul> <li> T-Motor Combo F7 HD FC + F55A PRO II</li> <li> TBS Motor Steele Ethix Stout V3</li> <li> TBS Tracer Nano RX</li> <li> Caddx Vista HD System (see improvement notes below)</li> <li> Apex HD CAMERA SIDE PLATE KIT (see improvement notes below)</li> <li> 20cm Coaxial Cable for Caddx Vista (20cm !)</li> <li> Vifly Finder Mini - Buzzer (lightweight)</li> <li> XT60 14AWG 10cm cable (longer)</li> <li> Apex DJI antenna TPU holder (will fit tracer immortal-t)</li> </ul> </li> <li> HQProp ETHIX P3 Peanut Butter &amp; Jelly 5.1x3x3 - PC (2x CW + 2xCCW)</li> <li> TrueRC Singularity U.FL Lite 112mm 5.8GHz - LHCP</li> <li> Strap Lipo KEVLAR 240x16mm - DFR</li> </ul>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#notes-to-self-for-future-improvements","title":"Notes to self for future improvements","text":"<ul> <li> Switch to the Caddx Nebula Pro Vista Kit so we do not need the Apex HD CAMERA SIDE PLATE KIT any more, save 3 grams and money?</li> </ul>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#build-steps","title":"Build steps","text":""},{"location":"FPV%20Builds/ImpulseRC%20Apex/#frame-assembly","title":"Frame assembly","text":"<p>Follow the official ImpulseRC Apex guide to assemble the frame.</p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#caddx-vista-on-the-20x20-rear-mounting-holes","title":"Caddx Vista on the 20x20 rear mounting holes","text":"<p>Note</p> <p>You will need 4 long M2 screws that are not provided with the kit. Nuts are the ones provided with the kit.</p> <p></p> <p></p> <p>The 20cm cable for Caddx Vista is better protected than the standard one.</p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#caddx-vista-vtx-antenna-mounting","title":"Caddx Vista VTX antenna mounting","text":"<p>Use the Apex DJI antenna TPU holder on the rear and make sure to zip tie the U.FL antenna to the standoff.</p> <p>The Tracer Immortal T antennas fit well: one is zip tied in the lower plate by the provided clamp and the other is sticked in the vertical section of the TPU.</p> <p></p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#esc-and-xt60-mounting","title":"ESC and XT60 mounting","text":"<p>In my first version I cut short the XT60 leads but it held the XT60 too close to the motors. Instead I'm now using a 10cm XT60 cable zip tied to the front standoff.</p> <p>Warning</p> <p>Be mindful of the fact that the ESC mounted in reverse positionto leverage the space at the front for the capacitor. You will need to reallocate the motor resources on the CLI (see the betaflight section below).</p> <p></p> <p></p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#rx-wiring","title":"RX wiring","text":"<p>Nothing surprising here, wiring the TBS Tracer Nano RX is straightforward!</p> <p></p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#fc-mounting","title":"FC mounting","text":"<p>Use the longest of the provided FC-to-ESC cables (it connects perfectly, mind the wire direction) and connect the Caddx Vista.</p> <p></p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#rx-placement","title":"RX placement","text":"<p>Once protected, the RX is stuck on top of the Caddx Vista using double sided tape.</p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#buzzer","title":"Buzzer","text":"<p>I chose the Vifly finder mini to get a lightweight yet powerful buzzer that fits perfectly at the front.</p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#finishing-the-build","title":"Finishing the build","text":"<p>Finish the build by placing the top plate and the plastic provided covers on the arms.</p> <p></p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#betaflight-configuration","title":"Betaflight configuration","text":"<ul> <li>Betaflight target: TMTR/TMOTORF7(STM32F7X2)</li> </ul> <p>Ports:</p> <p></p> <p>Bi-directional DShot is enabled and supported natively by the BLHeli32 ESC:</p> <p></p> <p>Filter settings to accommodate the bi-directional DShot:</p> <p></p> <p>Modes:</p> <p></p> <p>Joshua Bardwell's OSD for DJI FPV Google settings:</p> <p></p>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#motor-resource-reallocation","title":"Motor resource reallocation","text":"<p>Since the ESC is mounted in reverse position, the motors need to be reassigned on the CLI. Depending on your wiring, IDs may change.</p> <pre><code># resources\nresource MOTOR 1 B01\nresource MOTOR 2 B00\nresource MOTOR 3 C07\nresource MOTOR 4 C06\nsave\n</code></pre>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#minimal-cli-diff","title":"Minimal CLI diff","text":"<p>This is the minimal CLI diff that I initially set up without advanced tuning. To get my current tune, see the next section.</p> <pre><code>#\n# Building AutoComplete Cache ... Done!\n#\n# diff all\n\n# version\n# Betaflight / STM32F7X2 (S7X2) 4.2.6 Jan  5 2021 / 19:08:42 (a4b6db1e7) MSP API: 1.43\n# config: manufacturer_id: TMTR, board_name: TMOTORF7, version: e02dd6f2, date: 2020-11-04T11:31:26Z\n\n# start the command batch\nbatch start\n\n# reset configuration to default settings\ndefaults nosave\n\nboard_name TMOTORF7\nmanufacturer_id TMTR\nmcu_id 002f00263338510639393832\nsignature\n\n# resources\nresource MOTOR 1 B01\nresource MOTOR 2 B00\nresource MOTOR 3 C07\nresource MOTOR 4 C06\n\n# feature\nfeature -RX_PARALLEL_PWM\nfeature RX_SERIAL\nfeature TELEMETRY\n\n# beacon\nbeacon RX_LOST\nbeacon RX_SET\n\n# serial\nserial 1 1 115200 57600 0 115200\nserial 4 64 115200 57600 0 115200\n\n# aux\naux 0 0 0 1800 2100 0 0\naux 1 1 2 1300 1700 0 0\naux 2 13 1 1300 2100 0 0\naux 3 35 2 1800 2100 0 0\n\n# rxfail\nrxfail 7 s 750\n\n# master\nset gyro_lowpass2_hz = 375\nset dyn_notch_width_percent = 0\nset dyn_notch_q = 250\nset dyn_notch_min_hz = 90\nset dyn_notch_max_hz = 350\nset dyn_lpf_gyro_min_hz = 300\nset dyn_lpf_gyro_max_hz = 750\nset acc_calibration = 26,3,238,1\nset min_check = 1000\nset rssi_channel = 8\nset serialrx_provider = CRSF\nset dshot_bidir = ON\nset motor_pwm_protocol = DSHOT600\nset small_angle = 180\nset osd_warn_rssi = ON\nset osd_rssi_alarm = 40\nset osd_vbat_pos = 257\nset osd_rssi_pos = 2486\nset osd_rssi_dbm_pos = 161\nset osd_tim_1_pos = 353\nset osd_tim_2_pos = 321\nset osd_flymode_pos = 2241\nset osd_throttle_pos = 313\nset osd_vtx_channel_pos = 193\nset osd_craft_name_pos = 33\nset osd_gps_speed_pos = 161\nset osd_gps_lon_pos = 33\nset osd_gps_lat_pos = 1\nset osd_gps_sats_pos = 65\nset osd_home_dir_pos = 2275\nset osd_home_dist_pos = 2145\nset osd_flight_dist_pos = 184\nset osd_altitude_pos = 2177\nset osd_warnings_pos = 2441\nset osd_avg_cell_voltage_pos = 2516\nset osd_disarmed_pos = 2411\nset osd_flip_arrow_pos = 65\nset osd_core_temp_pos = 248\nset osd_log_status_pos = 97\nset osd_gps_sats_show_hdop = OFF\n\nprofile 0\n\n# profile 0\nset dyn_lpf_dterm_min_hz = 105\nset dyn_lpf_dterm_max_hz = 255\nset dterm_lowpass2_hz = 225\nset d_pitch = 32\nset d_roll = 30\nset d_min_roll = 0\nset d_min_pitch = 0\n\nprofile 1\n\nprofile 2\n\n# restore original profile selection\nprofile 0\n\nrateprofile 0\n\nrateprofile 1\n\nrateprofile 2\n\nrateprofile 3\n\nrateprofile 4\n\nrateprofile 5\n\n# restore original rateprofile selection\nrateprofile 2\n\n# save configuration\nsave\n</code></pre>"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#tuned-cli-diff","title":"Tuned CLI diff","text":"<p>This CLI diff is updated as my tuning is evolving.</p> <p>I started off UAV Tech's freestyle presets which I'm modestly trying to improve over time.</p> <pre><code># \n# Building AutoComplete Cache ... Done!\n# \n# diff all\n\n# version\n# Betaflight / STM32F7X2 (S7X2) 4.2.6 Jan  5 2021 / 19:08:42 (a4b6db1e7) MSP API: 1.43\n# config: manufacturer_id: TMTR, board_name: TMOTORF7, version: e02dd6f2, date: 2020-11-04T11:31:26Z\n\n# start the command batch\nbatch start\n\n# reset configuration to default settings\ndefaults nosave\n\nboard_name TMOTORF7\nmanufacturer_id TMTR\nmcu_id 002f00263338510639393832\nsignature \n\n# resources\nresource MOTOR 1 B01\nresource MOTOR 2 B00\nresource MOTOR 3 C07\nresource MOTOR 4 C06\n\n# feature\nfeature -RX_PARALLEL_PWM\nfeature RX_SERIAL\nfeature TELEMETRY\n\n# serial\nserial 1 1 115200 57600 0 115200\nserial 4 64 115200 57600 0 115200\n\n# aux\naux 0 0 0 1800 2100 0 0\naux 1 1 2 1300 1700 0 0\naux 2 13 1 1300 2100 0 0\naux 3 35 2 1800 2100 0 0\n\n# rxfail\nrxfail 7 s 750\n\n# master\nset gyro_lowpass2_hz = 375\nset dyn_notch_width_percent = 0\nset dyn_notch_q = 250\nset dyn_notch_max_hz = 350\nset dyn_lpf_gyro_min_hz = 300\nset dyn_lpf_gyro_max_hz = 750\nset acc_calibration = 26,3,238,1\nset min_check = 1000\nset rssi_channel = 8\nset serialrx_provider = CRSF\nset sbus_baud_fast = ON\nset dshot_bidir = ON\nset motor_pwm_protocol = DSHOT600\nset small_angle = 180\nset osd_warn_rssi = ON\nset osd_rssi_alarm = 40\nset osd_vbat_pos = 257\nset osd_rssi_pos = 2486\nset osd_rssi_dbm_pos = 161\nset osd_tim_1_pos = 353\nset osd_tim_2_pos = 321\nset osd_flymode_pos = 2241\nset osd_throttle_pos = 313\nset osd_vtx_channel_pos = 193\nset osd_craft_name_pos = 33\nset osd_gps_speed_pos = 161\nset osd_gps_lon_pos = 33\nset osd_gps_lat_pos = 1\nset osd_gps_sats_pos = 65\nset osd_home_dir_pos = 227\nset osd_home_dist_pos = 97\nset osd_flight_dist_pos = 184\nset osd_altitude_pos = 129\nset osd_warnings_pos = 2441\nset osd_avg_cell_voltage_pos = 2516\nset osd_disarmed_pos = 2411\nset osd_flip_arrow_pos = 65\nset osd_core_temp_pos = 248\nset osd_log_status_pos = 97\nset osd_gps_sats_show_hdop = OFF\nset gyro_rpm_notch_harmonics = 2\n\nprofile 0\n\n# profile 0\nset dyn_lpf_dterm_min_hz = 105\nset dyn_lpf_dterm_max_hz = 255\nset dterm_lowpass2_hz = 225\nset pidsum_limit = 1000\nset pidsum_limit_yaw = 1000\nset p_pitch = 69\nset d_pitch = 44\nset f_pitch = 171\nset p_roll = 63\nset d_roll = 40\nset f_roll = 162\nset p_yaw = 68\nset f_yaw = 162\nset d_min_roll = 0\nset d_min_pitch = 0\n\nprofile 1\n\nprofile 2\n\n# restore original profile selection\nprofile 0\n\nrateprofile 0\n\nrateprofile 1\n\nrateprofile 2\n\n# rateprofile 2\nset rates_type = ACTUAL\nset roll_rc_rate = 1\nset pitch_rc_rate = 1\nset yaw_rc_rate = 1\nset roll_expo = 54\nset pitch_expo = 54\nset yaw_expo = 54\nset roll_srate = 100\nset pitch_srate = 100\nset tpa_breakpoint = 1750\n\nrateprofile 3\n\nrateprofile 4\n\nrateprofile 5\n\n# restore original rateprofile selection\nrateprofile 2\n\n# save configuration\nsave\n</code></pre>"},{"location":"FPV%20Handbook/","title":"Introduction","text":"<p> Hello fellow FPV pilot!</p> <p>This handbook serves as a way for me to keep and reference all the links and assets that have been helpful to me in this hobby. I share it publicly in case it could help someone in the community!</p> <p>This is an Open Source project so feel free to contribute by opening a discussion in a Github issue or proposing a change using a Pull Request.</p> <p>Happy flying!</p>"},{"location":"FPV%20Handbook/Batteries/","title":"Batteries notes","text":"<p>Danger</p> <p>LiPo batteries are dangerous and can catch fire when they are damaged or badly maintained. What is written here is meant as a quick reminder and does not replace a fully comprehensive guide on battery management.</p>"},{"location":"FPV%20Handbook/Batteries/#calculating-charging-amperage","title":"Calculating charging amperage","text":"<p>The safest amperage to charge a battery pack is <code>1C</code> that we calculate by taking its mAh rating divided by 1000.</p> <p>Quick examples of <code>1C</code> calculation:</p> <ul> <li>1500 mAh battery = 1500 / 1000 = 1.5A charge</li> <li>850 mAh battery = 850 / 1000 = 0.8A or 0.9A charge</li> </ul> <p>If you're in a hurry, you can charge at <code>2C</code> which is basically twice the amperage you calculated above but do not do it too often.</p>"},{"location":"FPV%20Handbook/Batteries/#parallel-charging-reminder","title":"Parallel charging reminder","text":"<p>Warning</p> <p>All batteries charged in parallel should have the same cell count!</p> <ul> <li>Preferably parallel charge batteries of the same brand and model.</li> <li>Batteries cells voltage should not diverge from more than 0.1V per cell.   This means that the sum of the cell voltage difference should not exceed   0.1 x (cell count).</li> <li>Plug in the XT60 plug first on the parallel charging board, then the   balance lead.</li> <li>After you plug your batteries in the parallel charging board, allow a few   minutes for them to settle as some current will naturally distribute over   them.</li> <li>The calculation of charging amperage should be multiplied by the number of   batteries being charged in parallel but should not exceed 1C per battery.</li> </ul>"},{"location":"FPV%20Handbook/Batteries/#batteries-operation-voltages","title":"Batteries operation voltages","text":"<p>The voltages below are listed per cell.</p> <ul> <li>LiPo: safe voltage range is 3.0V to 4.2V (drone use keep above 3.5V)</li> <li>LiPo HV: safe voltage range is 3.0V to 4.35V (drone use keep above 3.5V)</li> <li>LiIon: safe voltage range is 2.5V to 4.2V</li> </ul>"},{"location":"FPV%20Handbook/Batteries/lipo_performance_database/","title":"LiPo database","text":"<ul> <li>Joshua Bardwell's Lipo Performance Database</li> </ul>"},{"location":"FPV%20Handbook/Batteries/skyrc-q200/","title":"SkyRC Q200 Charger","text":"<p>My version is the SK-100104.</p>"},{"location":"FPV%20Handbook/Batteries/skyrc-q200/#website","title":"Website","text":"<ul> <li>SkyRC Q200 Charger Website</li> </ul>"},{"location":"FPV%20Handbook/Batteries/skyrc-q200/#manual","title":"Manual","text":"<ul> <li>SkyRC Q200 Charger Manual</li> </ul>"},{"location":"FPV%20Handbook/Betaflight/osd-dji-fpv/","title":"DJI FPV Goggles OSD setup","text":"<p>CLI dump credits to Joshua Bardwell:</p> <pre><code>set osd_units = METRIC\nset osd_warn_arming_disable = ON\nset osd_warn_batt_not_full = ON\nset osd_warn_batt_warning = ON\nset osd_warn_batt_critical = ON\nset osd_warn_visual_beeper = ON\nset osd_warn_crash_flip = ON\nset osd_warn_esc_fail = ON\nset osd_warn_core_temp = ON\nset osd_warn_rc_smoothing = ON\nset osd_warn_fail_safe = ON\nset osd_warn_launch_control = ON\nset osd_warn_no_gps_rescue = ON\nset osd_warn_gps_rescue_disabled = ON\nset osd_warn_rssi = OFF\nset osd_warn_link_quality = OFF\nset osd_warn_over_cap = OFF\nset osd_rssi_alarm = 20\nset osd_link_quality_alarm = 80\nset osd_rssi_dbm_alarm = -60\nset osd_cap_alarm = 2200\nset osd_alt_alarm = 100\nset osd_distance_alarm = 0\nset osd_esc_temp_alarm = -128\nset osd_esc_rpm_alarm = -1\nset osd_esc_current_alarm = -1\nset osd_core_temp_alarm = 70\nset osd_ah_max_pit = 20\nset osd_ah_max_rol = 40\nset osd_ah_invert = OFF\nset osd_logo_on_arming = OFF\nset osd_logo_on_arming_duration = 5\nset osd_tim1 = 2560\nset osd_tim2 = 2561\nset osd_vbat_pos = 257\nset osd_rssi_pos = 2486\nset osd_link_quality_pos = 234\nset osd_rssi_dbm_pos = 161\nset osd_tim_1_pos = 353\nset osd_tim_2_pos = 321\nset osd_remaining_time_estimate_pos = 234\nset osd_flymode_pos = 2241\nset osd_anti_gravity_pos = 234\nset osd_g_force_pos = 234\nset osd_throttle_pos = 313\nset osd_vtx_channel_pos = 193\nset osd_crosshairs_pos = 205\nset osd_ah_sbar_pos = 206\nset osd_ah_pos = 78\nset osd_current_pos = 234\nset osd_mah_drawn_pos = 234\nset osd_motor_diag_pos = 234\nset osd_craft_name_pos = 33\nset osd_display_name_pos = 234\nset osd_gps_speed_pos = 2209\nset osd_gps_lon_pos = 2081\nset osd_gps_lat_pos = 2049\nset osd_gps_sats_pos = 2113\nset osd_home_dir_pos = 2275\nset osd_home_dist_pos = 2145\nset osd_flight_dist_pos = 184\nset osd_compass_bar_pos = 234\nset osd_altitude_pos = 2177\nset osd_pid_roll_pos = 234\nset osd_pid_pitch_pos = 234\nset osd_pid_yaw_pos = 234\nset osd_debug_pos = 234\nset osd_power_pos = 234\nset osd_pidrate_profile_pos = 234\nset osd_warnings_pos = 2441\nset osd_avg_cell_voltage_pos = 2516\nset osd_pit_ang_pos = 234\nset osd_rol_ang_pos = 234\nset osd_battery_usage_pos = 234\nset osd_disarmed_pos = 2411\nset osd_nheading_pos = 234\nset osd_nvario_pos = 234\nset osd_esc_tmp_pos = 234\nset osd_esc_rpm_pos = 234\nset osd_esc_rpm_freq_pos = 234\nset osd_rtc_date_time_pos = 234\nset osd_adjustment_range_pos = 234\nset osd_flip_arrow_pos = 65\nset osd_core_temp_pos = 248\nset osd_log_status_pos = 97\nset osd_stick_overlay_left_pos = 234\nset osd_stick_overlay_right_pos = 234\nset osd_stick_overlay_radio_mode = 2\nset osd_rate_profile_name_pos = 234\nset osd_pid_profile_name_pos = 234\nset osd_profile_name_pos = 234\nset osd_rcchannels_pos = 234\nset osd_camera_frame_pos = 35\nset osd_efficiency_pos = 234\nset osd_stat_rtc_date_time = OFF\nset osd_stat_tim_1 = OFF\nset osd_stat_tim_2 = ON\nset osd_stat_max_spd = ON\nset osd_stat_max_dist = OFF\nset osd_stat_min_batt = ON\nset osd_stat_endbatt = OFF\nset osd_stat_battery = OFF\nset osd_stat_min_rssi = ON\nset osd_stat_max_curr = ON\nset osd_stat_used_mah = ON\nset osd_stat_max_alt = OFF\nset osd_stat_bbox = ON\nset osd_stat_bb_no = ON\nset osd_stat_max_g_force = OFF\nset osd_stat_max_esc_temp = OFF\nset osd_stat_max_esc_rpm = OFF\nset osd_stat_min_link_quality = OFF\nset osd_stat_flight_dist = OFF\nset osd_stat_max_fft = OFF\nset osd_stat_total_flights = OFF\nset osd_stat_total_time = OFF\nset osd_stat_total_dist = OFF\nset osd_stat_min_rssi_dbm = OFF\nset osd_profile = 1\nset osd_profile_1_name = -\nset osd_profile_2_name = -\nset osd_profile_3_name = -\nset osd_gps_sats_show_hdop = OFF\nset osd_displayport_device = AUTO\nset osd_rcchannels = -1,-1,-1,-1\nset osd_camera_frame_width = 24\nset osd_camera_frame_height = 11\n</code></pre>"},{"location":"FPV%20Handbook/Betaflight/uav-tech-presets/","title":"UAV Tech presets","text":"<ul> <li>Link to the UAV Tech presets</li> </ul>"},{"location":"FPV%20Handbook/Build%20Tips/antennas/","title":"Antenna mounting tips","text":"<p>I found Joshua Bardwell's tips on antenna mounting quite useful:</p>"},{"location":"FPV%20Handbook/Build%20Tips/equipment/","title":"Equipment","text":""},{"location":"FPV%20Handbook/Build%20Tips/equipment/#screws","title":"Screws","text":"<ul> <li>Alloy of steel for durable and high quality screws: 10/9 and 12/9</li> </ul>"},{"location":"FPV%20Handbook/FC%20%26%20ESC/blheli32/","title":"BLHeliSuite32","text":"<ul> <li>BLHeliSuite32 download Google Drive</li> </ul>"},{"location":"FPV%20Handbook/FC%20%26%20ESC/mamba-f405-mini-3/","title":"Mamba F405+F35 Mini Mk3 DJI","text":""},{"location":"FPV%20Handbook/FC%20%26%20ESC/mamba-f405-mini-3/#website","title":"Website","text":"<ul> <li>Diatone Mamba F405+F35 Mini Mk3 DJI Website</li> </ul>"},{"location":"FPV%20Handbook/FC%20%26%20ESC/mamba-f405-mini-3/#general-wiring","title":"General wiring","text":""},{"location":"FPV%20Handbook/FC%20%26%20ESC/speedybee-f7/","title":"Speedybee F7 V2 Stack","text":""},{"location":"FPV%20Handbook/FC%20%26%20ESC/speedybee-f7/#general-wiring","title":"General wiring","text":""},{"location":"FPV%20Handbook/FC%20%26%20ESC/t-motor-f7/","title":"T-Motor F7+F55A PROII HD","text":""},{"location":"FPV%20Handbook/FC%20%26%20ESC/t-motor-f7/#website","title":"Website","text":"<ul> <li>T-Motor F7+F55A PROII HD Website</li> </ul>"},{"location":"FPV%20Handbook/FC%20%26%20ESC/t-motor-f7/#general-wiring","title":"General wiring","text":""},{"location":"FPV%20Handbook/Frames/","title":"ImpulseRC Apex","text":"<ul> <li>ImpulseRC Apex Build Guide</li> </ul>"},{"location":"FPV%20Handbook/Legal%20documents/","title":"DGAC","text":"<p> These are links to the French and Europeans regulation guides (in French).</p> <p> Liens utiles vers les guides de r\u00e9glementation drones.</p> <ul> <li>Guide de l'a\u00e9romod\u00e9lisme</li> <li>Guide Cat\u00e9gorie Ouverte: drones op\u00e9rants avec un faible risque (principalement de loisirs)</li> <li>Guide Cat\u00e9gorie Sp\u00e9cifique</li> </ul>"},{"location":"FPV%20Handbook/Prints/","title":"3D printing useful list","text":"<ul> <li>Crossfire Nano TPU holder</li> <li>Mini Immortal T holder</li> <li>Vertical Tracer Antenna Mount</li> <li>DJI FPV goggles cable lock</li> <li>AOS 5 v2 Arm Bumpers</li> <li>Antenna cable strain relief</li> </ul>"},{"location":"FPV%20Handbook/RX/expresslrs/","title":"ExpressLRS","text":""},{"location":"FPV%20Handbook/RX/expresslrs/#wiring","title":"Wiring","text":""},{"location":"FPV%20Handbook/RX/expresslrs/#flashing","title":"Flashing","text":"<p>Link to the ExpressLRS HappyModel EP documentation</p>"},{"location":"FPV%20Handbook/RX/expresslrs/#my-usual-settings","title":"My usual settings","text":"<ul> <li>250Hz packet rate</li> <li>100mW power</li> <li>ADC filtering off in Radio (Sys / Hardware)</li> </ul>"},{"location":"FPV%20Handbook/RX/expresslrs/#rssi-in-dji-osd-setup","title":"RSSI in DJI OSD setup","text":"<p>You need Betaflight 4.2+ to format the new RFMD:LQ% in the LQ OSD element.</p> <ul> <li>On Betaflight configuration tab, disable RSSI_ADC</li> <li>On Betaflight receiver tab, set RSSI Channel to AUX 12</li> </ul> <p>Note</p> <p>AUX12/ch16 is set as the \"RSSI Channel\" - Displays the RSSI dBm scaled as a percentage from the current Sensitivity Limit to -50dBm and is a decent indicator of how much range is left before the LQI cliff (0 here = Sensitivity Limit).</p> <ul> <li>On Betaflight OSD, activate the RSSI element, you can setup a warning to something like 30 since the AUX12 is a percentage trying to tell you how much range is left on your link.</li> </ul>"},{"location":"FPV%20Handbook/RX/expresslrs/#lq-warning-thresholds-setup","title":"LQ warning thresholds setup","text":"<ul> <li>On the radio, get to the ELRS Lua script</li> <li>Check your packet rate setting, you'll see a dbm threshold in parenthesis (say 108dBm for 250Hz for example)</li> <li>On Betaflight OSD, activate RSSI dBm warning</li> <li>On Betaflight CLI, set the RSS dBm alarm to the dBm threshold from your package rate + 10 (-108 + 10 = -98 in our example)</li> </ul> <pre><code>set osd_rssi_dbm_alarm = -98\nsave\n</code></pre> <p>Note</p> <p>There is no real low threshold for LQ since it greatly depends on what you fly and how acceptable it is for you that some of your packets do not get through.</p>"},{"location":"FPV%20Handbook/RX/expresslrs/#happymodel-ep2","title":"HappyModel EP2","text":""},{"location":"FPV%20Handbook/RX/expresslrs/#is-your-ceramic-antenna-defective","title":"Is your ceramic antenna defective?","text":"<ul> <li>Switch your power to 10mW on the radio</li> <li>Get your radio and receiver about 1 meter away</li> <li>On your radio, get to your Telemetry page, the 1RSS sensor value should be no worse than 60-65 dB</li> <li>Of course, the lower the dB the better</li> </ul>"},{"location":"FPV%20Handbook/RX/tracer/","title":"TBS Tracer","text":""},{"location":"FPV%20Handbook/RX/tracer/#manual","title":"Manual","text":"<ul> <li> <p>TBS Tracer Manual</p> </li> <li> <p>Joshua Bardwell's complete setup guide:</p> </li> </ul>"},{"location":"FPV%20Handbook/RX/tracer/#nano-receiver","title":"Nano receiver","text":""},{"location":"FPV%20Handbook/RX/tracer/#nano-receiver-general-wiring","title":"Nano receiver general wiring","text":""},{"location":"FPV%20Handbook/RX/tracer/#tbs-cloud-xf-wifi-firmware-update","title":"TBS Cloud XF WiFi firmware update","text":"<p>Updating the TBS Cloud WiFi firmware is done by using the TBS Agent X software and the TBS Cloud WiFi:</p> <ul> <li>You need to be logged into TBS Agent X</li> <li>Download the TBS Cloud activation ZIP file</li> <li>Unzip the file, you should get something like:</li> </ul> <pre><code>\u251c\u2500\u2500 V1.15\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 CROSSFIRE\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 firmware.bin\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 FUSION\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 firmware.bin\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 TANGO2\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 firmware.bin\n\u251c\u2500\u2500 V1.17\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 CROSSFIRE\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 firmware.bin\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 FUSION\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 firmware.bin\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 TANGO2\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 firmware.bin\n</code></pre> <ul> <li>Connect your TBS RX module using USB</li> <li>Make sure the Cloud XF WiFi module is green</li> <li>On your PC, look for and connect to a SSID named tbs_crossfire_xxxx</li> <li>Once connected to your TBS XF WiFi, go to http://192.168.4.1</li> <li>Select Upgrade on the XF WiFi interface, choose the latest CROSSFIRE/firmware.bin</li> </ul> <p></p> <ul> <li>Push the Upgrade button and wait for the upgrade to be completed.</li> </ul> <p>Note</p> <p>After the upgrade is done, your module will reboot but the version displayed in TBS Agent X will remain the same. To refresh it, just unplug/replug your RX module.</p>"},{"location":"FPV%20Handbook/TX/tbs_agent_lite/","title":"TBS Agent Lite","text":"<p>The TBS Agent Lite is a modern replacement for the default Crossfire LUA scripts.</p> <ul> <li>TBS Agent Lite download link</li> </ul>"},{"location":"FPV%20Handbook/TX/tx16s/","title":"TX16S","text":""},{"location":"FPV%20Handbook/TX/tx16s/#user-manual","title":"User manual","text":"<ul> <li>Radiomaster TX16S English User Guide</li> <li>Radiomaster TX16S French User Guide</li> </ul>"},{"location":"FPV%20Handbook/TX/tx16s/#initial-setup","title":"Initial setup","text":"<p>The initial setup is important especially the battery and gimbals calibration parts.</p> <ul> <li>Oscar Liang's Radiomaster TX16S setup</li> </ul>"},{"location":"FPV%20Handbook/VTX/","title":"Caddx Vista","text":""},{"location":"FPV%20Handbook/VTX/#setup-msp-for-dji-fpv-vista-air-unit","title":"Setup MSP for DJI FPV Vista / Air Unit","text":"<p>Take away:</p> <ul> <li>When MSP is working, you get the quad voltage instead of <code>N/A</code> and the <code>low power mode</code> message when you plugin, then as soon as you arm the quad the <code>unlocked</code> message and <code>frame rate</code> message appear</li> <li>Temperature protection set to Off will make the unit output its full power as soon as you plug in the quad (requested to get the video out via USB)</li> <li>Vista and Air Unit have protections against over heating, they will not allow themselves to be damaged because they over heat</li> </ul> <p>Joshua Bardwell made a good video about DJI FPV short range because of MSP misconfiguration.</p>"},{"location":"FPV%20Handbook/VTX/#how-to-tell-if-a-vista-or-air-unit-is-damaged","title":"How to tell if a Vista or Air Unit is damaged","text":"<ul> <li>Take a multimeter</li> <li>Check the continuity between Ground and TX, then RX : there should be none</li> <li>Check the resistance between Ground and TX, then RX : it should be mega ohms worth</li> </ul>"},{"location":"FPV%20Handbook/VTX/#how-to-switch-to-fcc-700mw-output","title":"How to switch to FCC 700mW output","text":"<ul> <li>Oscar Liang's guide on FCC mode</li> </ul>"},{"location":"FPV%20Handbook/VTX/#how-to-switch-to-1200mw-output","title":"How to switch to 1200mW output","text":"<ul> <li>Oscar Liang's guide on 1200mW output</li> </ul>"},{"location":"FPV%20Handbook/VTX/#sbus-baud-fast-low-latency-performance-with-dji-fpv","title":"SBUS Baud Fast low latency performance with DJI FPV","text":"<ul> <li>On the DJI FPV goggles go to <code>Settings &gt; Device &gt; Protocol</code> and make sure to select <code>Sbus Baud Fast</code></li> <li>On Betaflight CLI, type <code>set sbus_baud_fast=ON</code></li> </ul>"},{"location":"Tech%20Blog/","title":"Technical articles","text":"<p> I like to share some of my findings and experience in blog posts so that they can be found and referred to easily.</p> <p>Feel free to contact me if you feel like something needs clarification!</p>"},{"location":"Tech%20Blog/2012/2012-01-11-portage-basics/","title":"Portage basics","text":"<p>Les distributions Linux disposent toutes de ce qu'on appelle un gestionnaire de paquet dit package management system ou plus simplement package manager.</p> <ul> <li>Un package manager est un ensemble de programmes et d'outils permettant l'automatisation de l'installation / mise \u00e0 jour / configuration / d\u00e9sinstallation de logiciels sur un syst\u00e8me.</li> </ul> <p>Sous Gentoo Linux, le gestionnaire de paquet s'appelle portage. Il permet de manipuler les packages disponible sur notre syst\u00e8me Gentoo.</p> <ul> <li>Un package repr\u00e9sente un logiciel disponible \u00e0 travers le package manager. Selon les distributions il peut prendre diff\u00e9rentes formes comme par exemple une archive compress\u00e9e.</li> </ul>","tags":["gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-01-11-portage-basics/#portage","title":"Portage","text":"<p>Portage est \u00e9crit en python et en bash. C'est sans conteste l'un des package manager les plus flexibles et performants car il offre des possibilit\u00e9s de personnalisation tr\u00e8s fines des packages que l'on souhaite installer sur son syst\u00e8me.</p> <p>La liste des packages disponibles \u00e0 l'installation est organis\u00e9e dans une arborescence de dossiers, c'est ce qu'on appelle le\u00a0portage tree. Le nom \"arbre portage\" fait r\u00e9f\u00e9rence \u00e0 l'arborescence organis\u00e9e par cat\u00e9gorie des packages. Cette arborescence est stock\u00e9e par d\u00e9faut dans le dossier /usr/portage/ dont voici un exemple :</p> <p>/usr/portage/www-apache /usr/portage/www-apps /usr/portage/www-client /usr/portage/www-misc</p> <p>Dans chaque cat\u00e9gorie, on retrouve un dossier par package disponible :</p> <p>/usr/portage/www-client/chromium /usr/portage/www-client/firefox /usr/portage/www-client/opera</p> <p>On voit que firefox et opera font partie de la cat\u00e9gorie www-client, leur nom complet de package sous Gentoo est :</p> <ul> <li>www-client/firefox</li> <li>www-client/opera</li> </ul> <p>Bien s\u00fbr, on pourra aussi les appeler par leur petit nom mais il est important de noter qu'il est possible que deux packages aient le m\u00eame nom s'ils font partie d'une arborescence diff\u00e9rente. Mieux vaut donc toujours les appeler par leur nom complet.</p>","tags":["gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-01-11-portage-basics/#les-commandes","title":"Les commandes","text":"<p>Toutes les commandes suivantes font partie de portage et permettent de le manipuler. La plus connue est sans aucun doute emerge.</p> <p>/usr/bin/ebuild /usr/bin/egencache /usr/bin/emerge /usr/bin/portageq /usr/bin/quickpkg /usr/bin/repoman</p> <p>/usr/sbin/archive-conf /usr/sbin/dispatch-conf /usr/sbin/emaint /usr/sbin/emerge-webrsync /usr/sbin/env-update /usr/sbin/etc-update /usr/sbin/fixpackages /usr/sbin/regenworld /usr/sbin/update-env /usr/sbin/update-etc</p> <p>Dans un prochain post, je parlerai de l'utilisation des principales commandes de portage et de leur configuration.</p>","tags":["gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-01-13-portage-internals/","title":"Portage internals","text":"<p>Maintenant que nous savons ce qu'est Portage, comprenons simplement comment il fonctionne. Que se passe-t'il lorsque l'on veut installer un package, et d'ailleurs \u00e7a ressemble \u00e0 quoi un package sous Gentoo ?</p>","tags":["ebuild","gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-01-13-portage-internals/#les-ebuilds","title":"Les ebuilds","text":"<p>Les packages disponibles dans l'arbre portage sont repr\u00e9sent\u00e9s par des fichiers appel\u00e9s ebuilds. Les ebuilds contiennent toutes les informations n\u00e9cessaires \u00e0 la manipulation du package en question par portage (o\u00f9 t\u00e9l\u00e9charger les sources, quelle licence prot\u00e8ge le logiciel, quelle est l'URL du projet, etc...).</p> <p>Pour toute action vis \u00e0 vis d'un package, portage se base sur les informations des ebuilds\u00a0correspondants. Je dis des ebuilds car un ebuild contient aussi la version du package qu'il repr\u00e9sente. Il y a donc autant de fichiers ebuild que de versions disponibles d'un \u00a0package. Prenons l'exemple du package www-client/firefox :</p> <p>$ ls /usr/portage/www-client/firefox</p> <p>firefox-3.6.20.ebuild firefox-3.6.22.ebuild firefox-8.0.ebuild firefox-9.0.ebuild</p> <p>Les versions 3.6.20, 3.6.22, 8.0 et 9.0 sont donc disponibles sur portage. Si nous voulions des informations suppl\u00e9mentaires ou installer une de ces versions de firefox, portage n'aurait qu'\u00e0 ex\u00e9cuter les instructions contenues dans le fichier ebuild correspondant, et voil\u00e0 !</p> <p>Quand Mozilla sortira firefox 10, un d\u00e9veloppeur ou contributeur Gentoo devra cr\u00e9er l'ebuild pour cette version afin qu'il soit disponible dans portage, il est donc crucial de tenir sa liste d'ebuilds \u00e0 jour sur son syst\u00e8me.</p>","tags":["ebuild","gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-01-13-portage-internals/#synchroniser-portage","title":"Synchroniser portage","text":"<p>Mettre \u00e0 jour portage, c'est donc mettre \u00e0 jour la liste des ebuilds disponibles sur son syst\u00e8me !</p> <p># emerge --sync</p> <p>Le fameux sync t\u00e9l\u00e9charge les nouveaux ebuilds et supprime les obsol\u00e8tes pour nous, c'est gr\u00e2ce \u00e0 cela que nous disposerons du nouveau firefox quand il sortira, et il en va bien s\u00fbr de m\u00eame pour tous les packages.</p> <p>Les d\u00e9veloppeurs et contributeurs Gentoo tiennent ensemble \u00e0 jour un arbre portage commun qui est t\u00e9l\u00e9charg\u00e9 et r\u00e9pliqu\u00e9 par des serveurs qu'on appelle mirrors\u00a0(le terme mirroir signifie qu'ils contiennent une copie exacte de l'arbre de d\u00e9veloppement). Tous les utilisateurs r\u00e9pliquent \u00e0 leur tour leur arbre portage local (par d\u00e9faut dans /usr/portage/) en se connectant sur un de ces serveurs mirrors lors du sync.</p> <p>A l'heure o\u00f9 j'\u00e9cris ces lignes, le portage tree contient 15459 packages repr\u00e9sentant 29931 ebuilds !</p>","tags":["ebuild","gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-02-27-nginx-conditional-uwsgi-error-handling/","title":"nginx : conditional uWSGI error handling","text":"<p>To ensure the best possible quality of service we want to make sure that we catch our uWSGI application failures on the nginx\u00a0side and react accordingly. Our goal is to never serve a HTTP 500 error to visitors. I'll show you how you can adapt nginx error handling behavior based on the URI called by the visitor.</p>","tags":["conditional","error","nginx","uwsgi"]},{"location":"Tech%20Blog/2012/2012-02-27-nginx-conditional-uwsgi-error-handling/#nginx-uwsgi-base-configuration-nginxconf","title":"nginx + uWSGI base configuration (nginx.conf)","text":"<p>Suppose we have the following configuration to handle our uWSGI apps. We have set our gateway timeouts to 10 seconds to make sure no request will take more than that time to be answered, no matter what our application do.</p> <p>upstream uwsgi_app1  {     server 127.0.0.1:1000; }</p> <p>location / {     uwsgi_pass uwsgi_app1;     include uwsgi_params;     uwsgi_ignore_client_abort on;     uwsgi_connect_timeout 10;     uwsgi_read_timeout 10;     uwsgi_send_timeout 10; }</p>","tags":["conditional","error","nginx","uwsgi"]},{"location":"Tech%20Blog/2012/2012-02-27-nginx-conditional-uwsgi-error-handling/#static-uwsgi-error-handling","title":"Static uWSGI error handling","text":"<p>Now \u00a0we don't want nginx to reply to clients with errors such as 500 (our app crashed) and 504 (timeout has been triggered). At first, we'll serve a simple 1x1 GIF pixel instead.</p> <p>location = /px.gif {     empty_gif; }</p> <p>upstream uwsgid  {     server 127.0.0.1:1000; }</p> <p>location / {     uwsgi_pass uwsgi_app1;     include uwsgi_params;     uwsgi_ignore_client_abort on;     uwsgi_connect_timeout 10;     uwsgi_read_timeout 10;     uwsgi_send_timeout 10;</p> <pre><code>uwsgi_intercept_errors on;\nerror_page 500 504 /px.gif;\n</code></pre> <p>}</p> <p>The uwsgi_intercept_errors\u00a0directive tells nginx to handle errors from uWSGI. Then we just have to use the usual nginx error handling using the\u00a0error_page directive which in our case calls for /px.gif, returning our 1x1 GIF pixel using the empty_gif nginx module.</p>","tags":["conditional","error","nginx","uwsgi"]},{"location":"Tech%20Blog/2012/2012-02-27-nginx-conditional-uwsgi-error-handling/#dynamic-uwsgi-error-handling","title":"Dynamic uWSGI error handling","text":"<p>Let's go conditional, suppose we have two types or URLS :</p> <ol> <li>http://www.mysite.com/APP1?query=bar</li> <li>http://www.mysite.com/APP1?query=foo&amp;redir=http://www.ultrabug.fr&amp;word=bar</li> </ol> <p>For URL #1, we want to serve the 1x1 pixel whereas for URL #2, when we receive the redir\u00a0parameter, we want to redirect the visitor to exactly that URI.</p> <p>It's standard error_page handling remember ? Let's use the named location\u00a0feature to process the request.</p> <p>location @uwsgi_errors {     rewrite_log on;     if ($arg_redir ~* (.+)) {         set $redir $1;         rewrite ^ $redir? redirect;     }     rewrite ^ /px.gif? redirect; }</p> <p>location / {     uwsgi_pass uwsgi_app1;     include uwsgi_params;     uwsgi_ignore_client_abort on;     uwsgi_connect_timeout 10;     uwsgi_read_timeout 10;     uwsgi_send_timeout 10;</p> <pre><code>uwsgi_intercept_errors on;\nerror_page 500 504 @uwsgi_errors;\n</code></pre> <p>}</p> <p>Upon HTTP 500/504 error, the @uwsgi_errors\u00a0location is called by nginx internals. Let's detail its processing :</p> <ul> <li>rewrite_log on : turn the rewriting logging on for debugging / monitoring reasons</li> <li>$arg_redir ~* (.+)\u00a0: $arg_PARAMETER is a neat way to get the value of the given GET parameter (redir in our case). The condition here means that if the parameter is present, we'll use it and enter the condition.</li> <li>rewrite ^ $redir? redirect\u00a0: we call the rewrite module using the redirect method to send a HTTP 302 to the client with the value of the previously defined $redir variable which contains the URI of the redir parameter. The important part here is the question mark after the $redir variable which makes sure that the original URI parameters are stripped from the redirection URI.</li> <li>rewrite ^ /px.gif? redirect : if no redir parameter was received, we redirect to the the px.gif as usual. The question mark has the same meaning as above.</li> </ul> <p>That's it, we managed to handle our uWSGI errors based on certain conditions. Of course we could go further and use more named locations for different types of HTTP errors and use more nginx variables and conditions but that's up to you now !</p>","tags":["conditional","error","nginx","uwsgi"]},{"location":"Tech%20Blog/2012/2012-03-19-uwsgi-network-spooling-of-messages-between-applications/","title":"uWSGI : network spooling of messages between applications","text":"<p>One of the great new uWSGI v1.1 features is network spooling of messages between applications. This short article demonstrates how to use it between a front end django app and a back end python app.</p> <ul> <li>Download the fully packaged demo</li> </ul> <p>I advise you to use a uWSGI emperor and simply drop the provided ini files in its folder. The example is simple enough but here is an explanation of how it works.</p> <ol> <li>The sender is a django app which you call via your browser, the front end.</li> <li>The sender app uses the mashal module which permits to pass a type rich message (dictionary) through a string only spooling mechanism (yes, it's very handy).</li> <li>The sender sends a type 17 message (spool request message) over the network providing the message.</li> <li>The receiver app is a standalone spooling application written in standard python, this would be the back end.</li> <li>The receiver just prints out what it received via the network spooling mechanism.</li> </ol> <p>As I said, this is just an illustration of what can be done. You could look into uwsgidecorators and mix this with other stuff that suits your needs. Enjoy !</p>","tags":["django","network-spooling","python","uwsgi"]},{"location":"Tech%20Blog/2012/2012-05-11-mongodb-v2-0-5-released/","title":"mongoDB : v2.0.5 released","text":"<p>This is a bug fix release of mongoDB, it is now live in portage as well.</p> <p>+*mongodb-2.0.5 (11 May 2012) + +  11 May 2012; Ultrabug ultrabug@gentoo.org -mongodb-2.0.3.ebuild, +  -files/mongodb-2.0.3-fix-scons.patch, +mongodb-2.0.5.ebuild: +  Version bump, generic mms-agent URL, drop old. +</p>","tags":["mongodb","nosql","release"]},{"location":"Tech%20Blog/2012/2012-05-11-mongodb-v2-0-5-released/#bug-fix-highlight","title":"Bug fix highlight :","text":"<ul> <li>Inconsistent query results on large data and result sets</li> <li>Race during static destruction of CommitJob object</li> </ul> <p>See the complete changelog.</p>","tags":["mongodb","nosql","release"]},{"location":"Tech%20Blog/2012/2012-05-11-pymongo-v2-2-released/","title":"pymongo : v2.2 released","text":"<p>The mongoDB python driver pymongo was bumped to v2.2 and is now in portage.</p>","tags":["mongodb","nosql","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-05-11-pymongo-v2-2-released/#changelog-highlights","title":"Changelog highlights :","text":"<ul> <li>Support for Python 3</li> <li>Support for Gevent</li> <li>Improved connection pooling</li> </ul> <p>See the complete changelog.</p>","tags":["mongodb","nosql","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-05-15-uwsgi-new-ebuild-in-portage/","title":"uWSGI : new ebuild in portage","text":"<p>I started to rework the uwsgi ebuild on March 7th because I was not satisfied with the one available in portage. The current version was out of date and the package itself was not really suited for production deployment.</p> <p>Luckily my fellow Gentoo Linux developer Tiziano M\u00fcller (dev-zero) was also in the same kind of process for his own needs so we teamed up to achieve this goal. Our main focuses were :</p> <ul> <li>Bring the emperor mode support</li> <li>Ease and clarify the overall configuration</li> <li>Code a more versatile init script and\u00a0conf.d file</li> <li>Add a better support of the available plugins and python versions</li> <li>Support PHP</li> </ul> <p>I'm glad to announce that our reworked ebuild is now available in portage for all users, we hope that it will come handy to everyone who needs it.</p> <p>Thanks again Tiziano, it's always a pleasure to work with you !</p>","tags":["ebuild","portage","uwsgi"]},{"location":"Tech%20Blog/2012/2012-05-16-clustering-corosync-v1-4-3-pacemaker-v1-1-7-released/","title":"Clustering : corosync v1.4.3 & pacemaker v1.1.7 released","text":"<p>I've finally taken the time to take care of the corosync and pacemaker ebuilds. The new versions are now available in portage.</p>","tags":["cluster","corosync","gentoo","pacemaker"]},{"location":"Tech%20Blog/2012/2012-05-16-clustering-corosync-v1-4-3-pacemaker-v1-1-7-released/#corosync-143-10042012","title":"Corosync 1.4.3 (10/04/2012)","text":"<p>This is one of the last supported old stable release of the Corosync Cluster Engine. FYI, I've also bumped the new corosync-2.0.0 version but it needs more testing before I hard-unmask it.</p>","tags":["cluster","corosync","gentoo","pacemaker"]},{"location":"Tech%20Blog/2012/2012-05-16-clustering-corosync-v1-4-3-pacemaker-v1-1-7-released/#pacemaker-117-280312","title":"Pacemaker 1.1.7 (28/03/12)","text":"<p>This is a bug fix release of Pacemaker. See the changelog for details.</p> <p>Special thanks to my fellow Gentoo Linux developer Kacper Kowalik (xarthisius) for his help on these bumps.</p>","tags":["cluster","corosync","gentoo","pacemaker"]},{"location":"Tech%20Blog/2012/2012-05-25-state-of-the-event-log-architecture-enhancements/","title":"State of the event log architecture enhancements","text":"<p>Interesting stuff is happening on the event log (syslog) community and more precisely on the topic of syslog format extension and structuring syslog data.</p> <p>As of today there's no real standard on how to format and structure data on a syslog message. Every project has its own log message structure and syntax (qmail and postfix don't log a mail delivery failure the same way for example), so we rely on parsers to extract any given data from a log message because the syslog software has no way to do it for us. I for one have coded a postfix log parser and believe me it's not a pleasant thing to do and maintain !</p> <p>The main idea about structuring syslog messages is to represent them using\u00a0JSON\u00a0along with the current free form strings to prevent backward compatibility breakage. To achieve this, we need to normalize and extend this format so that syslog software such as\u00a0rsyslog and syslog-ng can directly understand them.\u00a0That's where CEE-enhanced messages and Lumberjack kick in.</p>","tags":["cee","json","lumberjack","rsyslog","syslog","syslog-ng"]},{"location":"Tech%20Blog/2012/2012-05-25-state-of-the-event-log-architecture-enhancements/#cee-enhanced-messages","title":"CEE-enhanced messages","text":"<p>The CEE project aims at defining a syntax which extends\u00a0the current log message format while being compatible with all the currently and widely used log frameworks or the well known glibc's syslog() call. To achieve this the main idea is to use what is called a cookie before the JSON representation of the data we want to pass to the syslog software.</p> <p>To make it simple, let's pretend we see this postfix log meaning that a queued mail has been removed from the queue (I removed the date etc to only focus on the message part) :</p> <p>CAA3B607DA: removed</p> <p>The equivalent CEE-enhanced message could\u00a0(this would be up to postfix) be represented as :</p> <p>@cee: {\"id\":\"CAA3B607DA\", \"removed\":\"true\"}</p> <ul> <li>@cee: is what is called the cookie which tells the syslog software that this message is using the CEE-enhanced syntax</li> </ul> <p>I guess you already see how handy this would be and how we could then rely on the syslog software to automagically use our favorite storage backend to store this structured data (think mongoDB).</p> <p>More information on the handy and quick video presentation by Rainer Gerhards and his article about it.</p>","tags":["cee","json","lumberjack","rsyslog","syslog","syslog-ng"]},{"location":"Tech%20Blog/2012/2012-05-25-state-of-the-event-log-architecture-enhancements/#the-lumberjack-project","title":"The Lumberjack project","text":"<p>So now how do we format the JSON part ? Could we have other types such as booleans and integers directly interpreted by the syslog software ? Well this needs definitions and standardization proposals, that's what project Lumberjack is for.</p> <p>Have a nice read on Lumberjack origins on\u00a0Rainer Gerhards's blog.</p>","tags":["cee","json","lumberjack","rsyslog","syslog","syslog-ng"]},{"location":"Tech%20Blog/2012/2012-06-01-rsyslog-new-v6-branch-in-portage/","title":"rsyslog : new v6 branch in portage","text":"<p>The first ebuild of the v6.2 stable branch of rsyslog is finally available in portage. This branch provides functional and performance enhancements of rsyslog.</p>","tags":["ebuild","gentoo","release","rsyslog"]},{"location":"Tech%20Blog/2012/2012-06-01-rsyslog-new-v6-branch-in-portage/#quick-highlights","title":"Quick highlights :","text":"<ul> <li>Hadoop (HDFS) support has been considerably speeded up by supporting batched insert mode.</li> <li>TCP transmission overhead for TLS has been dramatically improved.</li> <li>TCP supports input worker thread pools.</li> <li>Support of log normalization via liblognorm rule bases. This permits very high performance normalization of semantically equal messages from different devices (and thus in different syntaxes).</li> </ul> <p>Interesting upcoming features such as mongoDB support and the enhanced config language are on the way with v6.4. Stay tuned !</p>","tags":["ebuild","gentoo","release","rsyslog"]},{"location":"Tech%20Blog/2012/2012-06-06-mongodb-v2-0-6-release/","title":"mongoDB : v2.0.6 released","text":"<p>Bug fix release, it is now available in portage. Starting from this package version\u00a0I introduced a logrotate script which compresses daily the mongodb logs and keeps them for a year.</p> <p>Release highlights :</p> <ul> <li>mongos does not send reads to secondaries after replica restart when using keyFiles</li> <li>If only slaveDelay'd nodes are available, use them</li> <li>OplogReader has no socket timeout</li> </ul> <p>See the the complete changelog.</p>","tags":["logrotate","mongodb","nosql","release"]},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/","title":"mongoDB : export based on objectIDs' timestamp","text":"<p>I needed to export a set of data from a mongoDB\u00a0collection based on their objectIDs' (_id) timestamp using mongoexport. The mongoexport documentation is everything but helpful on the subject so I had to find a workaround to answer this simple question : \"export \u00a0all documents inserted yesterday on this collection in a CSV format\".</p>","tags":["mongodb","mongoexport","nosql"]},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/#relevant-mongoexport-options","title":"Relevant mongoexport options","text":"<ul> <li>--host : specify the mongoDB host</li> <li>--username / --pasword : if you're using authentication on your server</li> <li>-d : database to use</li> <li>-c : collection to use</li> <li>--fields : fields you want to export (omit for all)</li> <li>--query : the actual query selecting the result set you want to export</li> <li>--csv : export in a CSV format</li> </ul> <p>The date range query workaround</p> <p>So the hard part is to actually ask mongoexport to only return the documents in the desired time frame using an objectID compliant query. I overcame this problem using a simple but efficient python script generating the query for me.</p>","tags":["mongodb","mongoexport","nosql"]},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/#usrbinpython","title":"!/usr/bin/python","text":"","tags":["mongodb","mongoexport","nosql"]},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/#using-pymongo-22","title":"using pymongo-2.2","text":"<p>from bson.objectid import ObjectId import datetime</p> <p>now = datetime.datetime.now() yesterday = now - datetime.timedelta(days=1) start_date = datetime.datetime(yesterday.year, yesterday.month, yesterday.day, 0, 0, 0) end_date = datetime.datetime(now.year, now.month, now.day, 0, 0, 0) oid_start = ObjectId.from_datetime(start_date) oid_stop = ObjectId.from_datetime(end_date)</p> <p>print '{ \"_id\" : { \"$gte\" : { \"$oid\": \"%s\" }, \"$lt\" : { \"$oid\": \"%s\" } } }' % ( str(oid_start), str(oid_stop) )</p> <p>This script just prints out a command line compliant representation of the objectIDs for yesterday and today. So this query will select exactly what I wanted : all yesterday's objectIDs. Example :</p> <p>{ \"_id\" : { \"$gte\" : { \"$oid\": \"4fd535000000000000000000\" } , \"$lt\" : { \"$oid\": \"4fd686800000000000000000\" } } }</p>","tags":["mongodb","mongoexport","nosql"]},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/#using-mongoexport","title":"Using mongoexport","text":"<p>We then can simply use mongoexport from the shell by issuing (I left the optional parameters out) :</p> <p>$ mongoexport -h localhost -d myDatabase -c theCollection --query \"$(python oid.py)\" --csv</p> <p>Et voil\u00e0 !</p> <p>I guess there must be a cleaner way to do it out there, but I was unable to find it in my limited search time frame, so comment this post if you have a better solution please !</p>","tags":["mongodb","mongoexport","nosql"]},{"location":"Tech%20Blog/2012/2012-06-15-rsyslog-v6-2-2-released/","title":"rsyslog : v6.2.2 released","text":"<p>This is a bug fix release of rsyslog, it is now available in portage.</p>","tags":["release","rsyslog"]},{"location":"Tech%20Blog/2012/2012-06-15-rsyslog-v6-2-2-released/#bug-fix-highlights","title":"Bug fix highlights :","text":"<ul> <li>disk queue was not persisted on shutdown</li> <li>--enable-smcustbindcdr configure directive did not work (my fix, yay!)</li> <li>potential hang due to mutex deadlock</li> <li>\u201clast message repeated n times\u201d message was missing hostname</li> </ul> <p>See the complete changelog.</p>","tags":["release","rsyslog"]},{"location":"Tech%20Blog/2012/2012-06-18-flask-pymongo-new-ebuild-in-portage/","title":"Flask-PyMongo : new ebuild in portage","text":"<p>I just baked a new ebuild available now in portage :\u00a0dev-python/flask-pymongo !</p> <p>This is an extension for the Flask python microframework which simplifies the use of the mighty PyMongo for all your mongoDB\u00a0usages.</p> <p>Enjoy !</p>","tags":["ebuild","mongodb","nosql","portage","pymongo"]},{"location":"Tech%20Blog/2012/2012-07-09-pymongo-v2-2-1-released/","title":"pymongo : v2.2.1 released","text":"<p>The mongoDB python driver pymongo was bumped to v2.2.1 in portage.</p>","tags":["mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-07-09-pymongo-v2-2-1-released/#changelog-highlights","title":"Changelog highlights :","text":"<ul> <li>fixes an incompatibility with mod_wsgi 2.x that could cause connections to leak</li> </ul> <p>See the complete changelog.</p>","tags":["mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-07-09-uwsgi-v1-2-4-released/","title":"uWSGI : v1.2.4 released","text":"<p>This is a maintenance release of uWSGI whicih contains two new features and lot of bug fixes, it is now available in portage. Two of those fixes I was longing for :)</p>","tags":["ebuild","gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-07-09-uwsgi-v1-2-4-released/#bug-fix-highlights","title":"Bug fix highlights :","text":"<ul> <li>fixed python atexit usage in the spooler</li> <li>fixed a threading issue with uwsgi.send()</li> <li>fixed a leak in python uwsgi.workers()</li> <li>fixed spooler with chdir</li> <li>fixed async+threading</li> <li>fixed the\u00a0spooler-max-tasks respawn</li> <li>allow gevent's greenlets to send jobs to the spooler</li> </ul> <p>See the complete changelog.</p>","tags":["ebuild","gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-07-23-clustering-glue-v1-0-10-released/","title":"Clustering : glue v1.0.10 released","text":"<p>The newly released cluster glue libraries for Pacemaker / Heartbeat are available in portage.</p> <p>From my perspective, the major enhancement is that the clplumbing (the code responsible for passing along cib messages between nodes) now include compression. This was something I reported upstream\u00a0a long time ago and I was handling with the large-cluster USE flag on the ebuild (I thus dropped it from IUSE).</p>","tags":["cluster","gentoo","portage","release"]},{"location":"Tech%20Blog/2012/2012-07-23-clustering-glue-v1-0-10-released/#highlights","title":"Highlights :","text":"<ul> <li>Compression modules included and compression handling improved in clplumbing</li> <li>one memory leak fixed in clplumbing</li> <li>support for asynchronous I/O in sbd</li> </ul>","tags":["cluster","gentoo","portage","release"]},{"location":"Tech%20Blog/2012/2012-08-03-httping/","title":"Httping","text":"<p>Quick post about a great tool I came across to test the response time of \u00a0a web service :\u00a0httping</p> <p>As its name suggests, it's like ping but for http requests ! Its options are vast enough to fit with any of your dreams so just try it out, it's a must-have.</p> <p>Sample usage :</p> <p>$ httping -g 'http://google.com'</p> <p>PING google.com:80 (http://google.com): connected to 74.125.230.195:80 (321 bytes), seq=0 time=17.70 ms  connected to 74.125.230.197:80 (321 bytes), seq=1 time=13.97 ms  connected to 74.125.230.199:80 (321 bytes), seq=2 time=20.49 ms</p> <p>PS: yes, you can emerge it.</p>","tags":["http","ping","tools"]},{"location":"Tech%20Blog/2012/2012-08-07-rabbitmq-v2-8-5-released/","title":"rabbitMQ : v2.8.5 released","text":"<p>As the new maintainer of net-misc/rabbitmq-server , I'm pleased to announce the availability of the bug fix release 2.8.5 of our beloved Rabbit Message Queuing server.</p> <p>A special care have been taken on HA queues, so read the Changelog for more details.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2012/2012-08-14-uwsgi-v1-2-5-released/","title":"uWSGI : v1.2.5 released","text":"<p>This is a maintenance release of uWSGI with one new feature supporting pam authentication mechanism. The ebuild has been bumped with a new\u00a0pam USE flag.</p>","tags":["gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-08-14-uwsgi-v1-2-5-released/#bug-fix-highlights","title":"Bug fix highlights :","text":"<ul> <li>allows ugreen with threads</li> <li>added the pam plugin (sponsored by PythonAnywhere.com)</li> <li>added --so-keepalive option to enable TCP keepalives on sockets</li> </ul> <p>See the complete changelog.</p>","tags":["gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-08-20-mongodb-v2-0-7-released/","title":"mongoDB : v2.0.7 released","text":"<p>This is yet a new bug fix release, nothing too fancy here in my pov. I've bumped it in portage along with v1.8.5 of the previous branch.</p> <p>Release highlights :</p> <ul> <li>option to turn off splitVectors on particular mongoses</li> </ul> <p>See the the complete changelog.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2012/2012-08-27-les-vosges/","title":"Les Vosges","text":"<p>Premier roadtrip en Harley : les Vosges.</p> <p>Nous les avons atteint apr\u00e8s un passage sur Nancy pour profiter des \u00e9v\u00e9nements organis\u00e9s pour rendre hommage au designer et architecte Jean Prouv\u00e9. Au programme : des paysages magnifiques, des routes qui sentent le sapin mais dans le vrai sens du terme, de l'urbex dans un manoir incroyable et du vert, beaucoup de vert.</p> <p></p> <p></p> <p></p> <p>5 jours, 1350 km parcourus et une t\u00eate de pare-brise \u00e0 l'arriv\u00e9e : on va recommencer, c'est s\u00fbr !</p> <p></p> <p></p> <p>Disclamer : photos de mauvaise qualit\u00e9 prises avec le t\u00e9l\u00e9phone portable.</p>","tags":["harley","roadtrip","vosges"]},{"location":"Tech%20Blog/2012/2012-08-30-the-mongodb-2-2-era/","title":"The mongoDB 2.2 era","text":"<p>Yesterday MongoDB entered the version 2.2 era with all its long time awaited new features. For the strong MongoDB fans across the world, those features were known and expected because some of them were requested for quite long now. They often were tested and debugged with 10gen before being released also, that's the way it goes.</p> <p>In mongoParis 2012, I took the chance to discuss a few matters with a 10gen engineer about some of those new features and he predicted that the release would take place in September. He was not far from the truth and I'm glad that for once a release is launched before what I was expecting (even if mongo jira was even more optimistic). My main points in this discussions and his answers were :</p> <ul> <li>Will they take the chance to drop the spidermonkey 1.7 requirement and finally move their code to use v1.8 ?</li> </ul> <p>No, they'd rather focus on switching over to the v8 javascript engine, even if it's a long time goal. They're still not satisfied with the current v8 stability regarding locks so it's not suited for production yet.</p> <ul> <li>Will the new aggregation framework be capable of handling \"join\" like requests ?</li> </ul> <p>No, that's not what it was designed for yet.</p> <ul> <li>What about the output of the new aggregation framework, can we write it into a collection or is it RAM only (thus limiting the result set size) ?</li> </ul> <p>The result set is RAM only so we have the same limitations as a distinct command :\u00a016MB. We plan on being able to store the output on a collection or something later.</p> <p>I must admit, for my planned use cases the last answer was a head shot on the aggregation framework but I still think it's a remarkable achievement and I can only say 10gen did a good job and thank them for releasing it.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2012/2012-08-30-the-mongodb-2-2-era/#mongodb-220","title":"mongodb-2.2.0","text":"<p>My thoughts on this new release are clouded with both my intensive administrator point of view (our main DB is over 2,5 billions documents large) and my packager point of view (Gentoo Linux). I'll start with my packager pov, which is bad unfortunately :</p> <ul> <li>I am annoyed by their stubbornness with sticking to spidermonkey-1.7. Hell, it's not like the community didn't do the work for them already : Fedora do provide a working spidermonkey-1.8 patch for mongoDB. I may reconsider using it for the mongodb-2.2.0 series if they update their patch again.</li> <li>I am not happy with them not being strict in their compile-from-source procedure. Scons sucks but everything is arguable, what is not is that they don't seem to make some real efforts and testing on proper compilation. I feel like they neglect the people caring for the sources and not some pre-baked binary and it's not good imo because this is also a great field for optimizing your software.</li> <li>They gave me a headache debugging the 2.2.x release to make it compile fine. I won't go into too much details (I'll fill bugs for them) but hell, they say they provide a scons option --use-system-all which they don't even honor by hard sourcing spidermonkey-1.7 libraries from their own sources !</li> <li>They now use boost-1.49, I won't blame them here.</li> </ul> <p>Now come the good news, at last, from my user perspective :</p> <ul> <li>No strong upgrade plan needed. Just upgrade the clients (mongos) first then roll on the servers, perfect.</li> <li>The concurrency improvements are just so awesome on paper : locks are now per database, not for the whole mongod process. This is good news if you have multiple databases, which I don't, but they also implemented a new subsystem which avoids locks on most page faulting operations so even I can benefit from this.</li> <li>The aggregation framework will simplify some queries we could only achieve using mapReduce and offer interesting possibilities for calculations such as statistics.</li> <li>TTL collections : those I have been waiting since mongoParis 2011. I have tons of ideas and use cases.</li> <li>mongotop and mongostat now support authentication while mongodump can read from secondaries.</li> <li>We can now log to syslog instead of fixed log files using the --syslog command line argument to mongod.</li> <li>Lower migration thresholds ensure a better distribution of data for small collections on clusters.</li> </ul> <p>See the full changelog for all the details.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2012/2012-08-30-the-mongodb-2-2-era/#pymongo-23","title":"pymongo-2.3","text":"<p>All drivers also had to adapt to be able to offer those new features and benefit from them. Apart from supporting the aggregation framework, I will highlight this change as it affects me potentially :</p> <ul> <li>Users of authentication must upgrade to PyMongo 2.3 (or newer) for \u201csafe\u201d write operations to function correctly.</li> </ul> <p>Better be aware and safe than sorry mates, remember to update your drivers and read the changelog.</p> <p>Those versions are already available in portage, you can stop reading and go compiling now.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2012/2012-09-03-rabbitmq-2-8-6-released/","title":"rabbitMQ 2.8.6 released","text":"<p>I was in a bug killing spree today and I'm pleased to announce that I closed the 5 open bugs open for net-misc/rabbitmq-server while bumping this package to its new v2.8.6 version.</p> <p>+*rabbitmq-server-2.8.6 (03 Sep 2012) + +  03 Sep 2012; Ultrabug ultrabug@gentoo.org -rabbitmq-server-2.8.1-r1.ebuild, +  rabbitmq-server-2.8.4.ebuild, rabbitmq-server-2.8.5.ebuild, +  +rabbitmq-server-2.8.6.ebuild, files/rabbitmq.service, +  files/rabbitmq-script-wrapper: +  Drop old. Add GPL-2 LICENSE fix #426092. Enhanced systemd service file fix +  #419531 and init script fix #416345 thx to Maksim Melnikau. Fix #430510 VCS +  fetching in compilation. Fix #430508 parallel building. Version bump. +</p> <p>Bug fix version mates, the changelog\u00a0is here for more details as always.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2012/2012-09-11-gentoo-make-conf-et-make-profile-demenagent/","title":"Gentoo : make.conf et make.profile d\u00e9m\u00e9nagent","text":"<p>Bien que cette nouvelle soit diffus\u00e9e par le syst\u00e8me de news interne \u00e0 Portage, je sais d'exp\u00e9rience qu'elles ne sont pas tr\u00e8s lues.</p> <p>Sachez simplement que prochainement, les fichiers make.conf et make.profile seront pr\u00e9sents dans les stages d'installation dans /etc/portage et non plus dans /etc comme actuellement.</p> <p>Il est important de noter que ce changement ne concerne que les nouvelles installations\u00a0et n'a aucun impact sur les syst\u00e8mes actuels, les deux r\u00e9pertoires sont donc support\u00e9s mais ne vous \u00e9tonnez pas si vous ne les trouvez pas \u00e0 leur place habituelle lors de vos prochaines installs. Pour ceux qui utilisent des scripts automatis\u00e9s d'installation ou des d\u00e9mons comme Puppet ou Chef, pensez \u00e0 modifier vos configurations !</p> <p>La news en question, en anglais :</p> <p>Title                     make.conf and make.profile move   Author                    Jorge Manuel B. S. Vicetto jmbsvicetto@gentoo.org   Posted                    2012-09-09   Revision                  1</p> <p>Starting next week, new stages will have make.conf and make.profile moved from /etc to /etc/portage. This is a change in the installation defaults, that will only affect new installs so it doesn't affect current systems.</p> <p>Current users don't need to do anything. But if you want to follow the preferred location, you may want to take the chance to move the files in your system(s) to the new location.</p>","tags":["gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-10-29-uwsgi-v1-2-6-and-v1-3-released/","title":"uWSGI : v1.2.6 and v1.3 released","text":"<p>The v1.2.6 is a maintenance release with some interesting backports while the v1.3 series start with a couple of new interesting features and plugins such as a mongodb logger. On my async python usage side, I'm glad to see this new version coming for its enhanced --lazy-apps option\u00a0and gevent grace reloading.</p>","tags":["gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-10-29-uwsgi-v1-2-6-and-v1-3-released/#v126-highlights","title":"v1.2.6 highlights :","text":"<ul> <li>fixed idle mode on busy workers</li> <li>backported subscription round robin weight handling from 1.3</li> </ul> <p>See the complete changelog.</p>","tags":["gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-10-29-uwsgi-v1-2-6-and-v1-3-released/#v13-highlights","title":"v1.3 highlights :","text":"<ul> <li>New plugin : router_http (compiled-in by default)</li> <li>New feature : --if-env (and --if-opt) can compare values</li> <li>http/https non-blocking writes</li> <li>Busyness cheaper algorithm by\u00a0\u0141ukasz Mierzwa</li> <li>MongoDB integration for logging</li> </ul>","tags":["gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/","title":"My views on Python","text":"<p>We're having a pretty hot debate in my company about the core development language we would like to embrace in order to enhance our work flow and unlock both our innovation and development iteration.</p> <p>I thought of doing a blog post instead of writing an email summarizing my point of view about why we should choose python in case it could help other people to make their own mind or at least understand mine. I don't want to be dragged into the typical \"language x VS language y\" type of post as there are a lot of those already but instead focus on specific use cases. Anyway, to be totally transparent with you, dear reader, I have to tell you that the main \"opponent\" we're considering is Javascript / node.js.</p>","tags":["development","python"]},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#python-is-easy-to-learn-and-write","title":"Python is easy to learn and write","text":"<p>I work for an online marketing/advertising company and we have four different IT/development teams with specific work to do. \u00a0That means that we all have our own constraints to take into account and we use different technologies / languages to achieve them. Opening a debate on a development language rationalization thus means that people will have to be able to learn the one we will choose, the quicker the better.</p> <p>Well let's be honest, python is not widely taught on IT schools so our guys definitely WILL have to learn it. But python is one of the most simple language to learn as you can quickly test and make progress. This mainly comes from its coding syntax which makes the code cleaner and easy to read, also the language's keywords are simple to remember, no boring brackets or semicolons to drive you crazy. Python is about coding clean and instinctively.</p> <p>A simple and straightforward syntax is very important to me because :</p> <ul> <li>it is easier to read</li> <li>it is easier to maintain</li> <li>your code is lighter</li> <li>you spend less time coding so you spend more time thinking</li> </ul>","tags":["development","python"]},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#python-everywhere","title":"Python everywhere","text":"<p>When choosing your core language, you must make sure you can rely on it today and tomorrow for a wide range of use cases. I find it very interesting to have a look at a list of its application domains as it does really show that with this language at our core, we would be able to achieve anything.</p> <p>The versatility of python is my main argument in favor of its adoption. This is a mature and proven language with a lot of libraries and frameworks\u00a0to suit all our present and future needs. It is widely supported on an extensive set of platforms and I can't think of an open-source project not supporting python right from the start.</p> <p>At this point, I will quote the folks at AppNexus from their conference in the recent PyData NYC 2012 :</p> <p>\"Python's versatility allows us to use it both for offline analytical tasks as well as production system development. Doing so allows us to bridge the gap between prototypes and production by relying on the same code libraries and frameworks for both, thereby tightening our innovation loop\". </p> <p>They say python helped them grow very rapidly and efficiently by permitting them to focus on innovation, needless to say that I share their point of view and would love to see this happening in my company.</p>","tags":["development","python"]},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#python-use-cases-in-my-company","title":"Python use cases in my company","text":"<p>Now let's talk about our present and projected use cases. This is not an exhaustive list as I want to keep it simple and demonstrate the versatility I just talked about.</p>","tags":["development","python"]},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#scripting-and-automation","title":"Scripting and automation","text":"<p>I am a sysadmin and I am lazy. Nothing new here okay, but that's how I met python a bunch of years ago. At that time my company's infrastructure became more complex everyday as was growing very rapidly. New servers were arriving and provided new functionalities and new technologies which lead more and more to heterogeneity in the things we had to monitor, automate and configure.</p> <p>Python is a sysadmin's heaven with all its libraries capable of handling complex tasks easily, even in our cluster environments where you have to deal with parallel and high availability computing. This is a big relief to know that whatever the task you're asked to carry you can safely say : \"python can do it\". The keyword here is efficiency.</p>","tags":["development","python"]},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#complete-and-complex-applications","title":"Complete and complex applications","text":"<p>A lot of modern and cross-platform applications are written in python or based on it. I for one wrote an email parsing software a few years ago and it's still kicking in production, its maintenance is easy and it has evolved smoothly with our growth and needs.</p> <p>Another thing I like about this language is that it's fast and can benefit from a semi-compiled \"byte-code\" which speeds up your application. No, python is not C++ and speed is not it's biggest advantage of course but it's really fast enough to compete with others easily.</p> <p>Let's sample some famous software written in python :</p> <ul> <li>BitTorrent, original client, along with several derivatives</li> <li>Dropbox, a web-based file hosting service</li> <li>OpenStack, a cloud computing IaaS platform</li> <li>Portage, the heart of Gentoo Linux</li> <li>Ubuntu Software Center, a graphical package manager</li> </ul>","tags":["development","python"]},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#web-applications","title":"Web applications","text":"<p>Python also have some solid\u00a0and very powerful libraries able to manage asynchronous, real-time and scalable web applications and services. We already do have some of those robust web apps running in production and python demonstrates everyday all of the strengths I already talked about here. We use libraries such as gevent along with web frameworks like flask\u00a0and message queuing with zeromq. Someday I may write a post about our python web stack, it may be interesting to share about it.</p> <p>I have been able to recode a web app written in .NET very quickly while enhancing it in every way possible. It is way faster, reliable, fault tolerant and maintainable that it was before. Thanks to python we have a short development iteration which proves itself everyday as the application grows and is capable to meet and achieve any new challenge we're asked to take care of. I'm convinced that no other language could have been so powerful and versatile than python to do so.</p> <p>We're not the only ones thinking and experiencing this of course, still in the list we can see :</p> <ul> <li>Google uses Python for many tasks including the backends of web apps such as Google Groups, Gmail, and Google Maps, as well as for some of its search-engine internals</li> <li>AppNexus uses Python for some of their web apps backend</li> <li>YouTube uses Python \"to produce maintainable features in record times, with a minimum of developers\"</li> <li>Yahoo! Groups uses Python \"to maintain its discussion groups\"</li> </ul> <p>That's some big players indeed and it's interesting to see they use python for their web app backends.</p>","tags":["development","python"]},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#conclusion","title":"Conclusion","text":"<p>I am not a software developer as I never took strong development courses at school. I am a sysadmin of complex, clustered and heterogeneous environments so this affects my development standards and point of view in a way that my expectations will surely be different from a pure developer.</p> <p>My main concerns can be defined with words like proven, easy, clean, versatile, maintainable, fast (to code and execute), scalable, fault-tolerant and cross-platform. All of my choices have been based on those standards and concerns and I think they apply well in our debate because I chose Python to meet them all and I've never been disappointed or limited by it.</p> <p>I hope this post reflects my thoughts and helped you understand them. I will tell you about the result and the decision my company made on this debate.</p>","tags":["development","python"]},{"location":"Tech%20Blog/2012/2012-11-04-mongodb-ebuilds-cleanup-and-v2-2-1-released/","title":"mongoDB : ebuilds cleanup and v2.2.1 released","text":"<p>Another bug killing spree has happened :) I'm glad to have closed quite a bunch of bugs related to mongoDB today.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2012/2012-11-04-mongodb-ebuilds-cleanup-and-v2-2-1-released/#no-more-spidermonkey-17-dependency","title":"No more spidermonkey-1.7 dependency","text":"<p>Thanks to the help from Ian Stakenvicius, we managed to drop the spidermonkey-1.7 dependency and use the embedded version shipped in the sources. I extended this fix to all 2.0.x and 2.2.x ebuilds, the only package remaining is the one for v1.8.5 but it will be dropped soon.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2012/2012-11-04-mongodb-ebuilds-cleanup-and-v2-2-1-released/#newer-boost-compatibility","title":"Newer boost compatibility","text":"<p>Boost-1.50 introduced a new filesystem v3 version while breaking compatibility with older v2 filesystems. This broke mongoDB compilation\u00a0for the guys running unstable version of boost. As I didn't want to force stable users to keyword their boost versions, I kept a version of each 2.x series compatible with v2 &lt;boost-1.50 filesystem.</p> <ul> <li>&lt;dev-libs/boost-1.50 users should use the ebuilds revisions 1 (-r1)</li> <li>&gt;=dev-libs/boost-1.50 users should use the ebuilds revisions 2 (-r2)</li> </ul>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2012/2012-11-04-mongodb-ebuilds-cleanup-and-v2-2-1-released/#v221-version-bump","title":"v2.2.1 version bump","text":"<p>Last but not least, the new v2.2.1 is also available but only for &gt;=boost-1.50 users. This is a nice bugfix release which you should consider to apply since it's the first of the 2.2 series.</p> <p>See the full changelog.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2012/2012-11-05-portugal-tavira/","title":"Portugal, Tavira","text":"<p>So long since I didn't post a photo I've taken here. Since I got back from 3 weeks in Portugal, I guess it's a good time to share some of my favorite shots with you.</p> <p></p> <p>Nikon FM2 with Kodak Ektachrome 100</p>","tags":["portugal","tavira"]},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/","title":"Clustering : corosync v2.1.0 & pacemaker v1.1.8","text":"<p>I recently bumped quite a bunch of the clustering suite packages such as :</p> <ul> <li>sys-cluster/cluster-glue-1.0.11</li> <li>sys-cluster/libqb-0.14.3</li> <li>sys-cluster/corosync-1.4.4</li> <li>sys-cluster/corosync-2.1.0</li> <li>sys-cluster/crmsh-1.2.1 (new package)</li> </ul>","tags":["cluster","gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/#corosync-210","title":"corosync-2.1.0","text":"<p>You should be aware that the corosync-2.x packages are still hard masked because the 2.0.x ones didn't compile properly and we didn't have a suitable pacemaker version for it to work with.</p> <p>There is another thing for us to consider and handle in the ebuilds when we'll be willing to release corosync-2 : it is not backward compatible ! So yes, you will either have to start with a fresh cluster or break your existing ones to migrate to corosync-2. The reason is that upstream decided to drop the plugins support from their software. So a non-plugin cluster cannot work with a plugin-enabled one.</p>","tags":["cluster","gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/#pacemaker-118","title":"pacemaker-1.1.8","text":"<p>As for pacemaker-1.1.8, the bump request took quite some time. The reason is that it was released without backward compatibility support as well so you couldn't join your existing pacemaker-1.1.x cluster even when using corosync-1.x ! I found it unacceptable because this meant I would have to force for corosync-2 usage starting from pacemaker-1.1.8 for no real reason. Pacemaker upstream, namely Andrew Beekhof, is very responsive and kind so he offered a solution which I'm happy to provide to Gentoo users.</p>","tags":["cluster","gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/#crmsh-121","title":"crmsh-1.2.1","text":"<p>The\u00a0crm\u00a0command is not included in the pacemaker sources anymore. It is now an independent project lead by Dejan Muhamedagic to allow more versatility in its development and a better iteration of releases. I packaged and released it along with pacemaker-1.1.8 as well.</p>","tags":["cluster","gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/#todo","title":"Todo","text":"<p>I'm still slacking on the sys-cluster/resource-agents package unfortunately. I already discussed with upstream about it as it's not a straightforward bump because they merged two different resource-agents developments. I think to have a pretty good idea of what needs to be done and will do my best to fix this gap as soon as I can.</p>","tags":["cluster","gentoo","portage"]},{"location":"Tech%20Blog/2012/2012-11-19-uwsgi-v1-4-1-lts-released/","title":"uWSGI : v1.4.1 LTS released","text":"<p>Ok that's a neat uwsgi release here and stamped for Long Time Support by unbit. I've tested it before bumping it to portage and it's behaving nicely with my gevent apps as there have been some nice improvements on this part as well. Quite a bunch of code have been refactored and optimized for enhanced performances. Have a look at the highlights, they're to be read with care.</p>","tags":["gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-11-19-uwsgi-v1-4-1-lts-released/#highlights","title":"highlights :","text":"<ul> <li>Support for the Go language</li> <li>Enhanced gevent reloading and handling\u00a0(truly non-blocking wsgi.input, truly non-blocking writes...)</li> <li>Improved http/https router and fastrouter (better event-based engineering, reduced syscall usage)</li> <li>Improved systemd support</li> <li>Log filtering and routing</li> <li>Smart attach daemon to start a daemon along with your app</li> <li>A big work is being done about the documentation</li> </ul> <p>Yep that's some heavy stuff, there is more so you should definitely read the different changelogs on the mailing list !</p>","tags":["gentoo","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-11-20-rsyslog-new-v7-branch-released/","title":"rsyslog : new v7 branch released","text":"<p>It's been a long time since I took care of the rsyslog package so bear with me for these quite huge version releases. The main thing to note is that I finally packaged the\u00a0new v7 branch which is stamped \"stable for production\" by upstream. The v5 branch is not supported anymore unless you have a professional contract with Adiscon so I encourage you to read Rainer's blog post.</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2012/2012-11-20-rsyslog-new-v7-branch-released/#v722","title":"v7.2.2","text":"<p>Optimizations, code refactoring for way improved performances and a lot of new features are there. The v7 series bring a lot of changes and neat stuff to rsyslog as you can see on this v5 vs v7 link. Here are some hints :</p> <ul> <li>Improved configuration language</li> <li>Improved execution engine</li> <li>Full support for structured logging and JSON-based log messages (I discussed this matter on a previous post)</li> <li>Ability to normalize legacy text log messages to JSON</li> </ul> <p>The changelog is very long, so you can browse this to find out more.</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2012/2012-11-20-rsyslog-new-v7-branch-released/#v5101-and-v660","title":"v5.10.1 and v6.6.0","text":"<p>Those releases contain backports bugfixes (v5) and enhancements (v6) fixed on the v7 branch so if you're not ready to jump straight to the new version, you might consider updating your branch for a last time :)</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/","title":"mongoDB v2.2.2 and pymongo v2.4 released","text":"","tags":["gentoo","mongodb","portage","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#mongodb-222","title":"mongodb-2.2.2","text":"<p>This is a bugfix release of mongoDB, there is nothing major to note about it so just have a look at the changelog.</p>","tags":["gentoo","mongodb","portage","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#pymongo-24","title":"pymongo-2.4","text":"<p>Ok here's a bigger cake we'll have to focus on as you will have to adapt your code to use it properly. Don't be scared, upgrading will not instantly break your current apps but...</p>","tags":["gentoo","mongodb","portage","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#connection-replicasetconnection-deprecation","title":"Connection / ReplicaSetConnection deprecation","text":"<p>Those classes are still available and provide the old\u00a0safe=False behavior\u00a0meaning that by default, operations are not acknowledged.\u00a0They are being replaced by MongoClient and MongoReplicaSetClient classes which on the contrary do acknowledge operations by default. So yes, now your operations will run with a safe=True by default !</p>","tags":["gentoo","mongodb","portage","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#write-concern","title":"Write concern","text":"<p>In the mongoDB talks we never hear about safe writes but about write concerns. A new API now handles these operations' behavior such as fsync / journal committing / write acknowledgment which is in line with the internals of mongoDB. I think it's more clear and straightforward to handle this that way so it's a good job done by upstream even if it means we have to adapt our code for it. They come in the number of four options which are applied on a database or collection level :</p> <ul> <li>w \\= integer\u00a0: A value of 0 means we don't care so it's the fire-and-forget behavior we knew as safe=False. A value &gt; 0 is the equivalent of the safe=True but with a more fine tuning on how many servers should confirm the operation.</li> <li>wtimeout = integer : Adds a timeout on the w parameter.</li> <li>j = bool : Wait until the operation has been committed to the journal.</li> <li>fsync = bool : Wait until the database to fsync all files to disk.</li> </ul>","tags":["gentoo","mongodb","portage","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#highlights","title":"Highlights","text":"<ul> <li>Cursor can be copied with functions from the copy module</li> <li>The set_profiling_level() method now supports a slow_ms option</li> </ul> <p>See the rest in the full changelog.</p>","tags":["gentoo","mongodb","portage","pymongo","release"]},{"location":"Tech%20Blog/2012/2012-11-28-uwsgi-v1-4-2-released/","title":"uWSGI : v1.4.2 released","text":"<p>Quick post for a bugfix release with some added flavor features for PHP users.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-11-28-uwsgi-v1-4-2-released/#highlights","title":"highlights","text":"<ul> <li>improved perl mules support</li> <li>fixed pending data management in https router</li> <li>fixed thread-offloading of really big static files</li> <li>added --php-app-qs as micro-optimization (in-place of rewrite rules) for php apps</li> <li>added --php-var to inject custom (fixed) vars in the request</li> </ul> <p>Full changelog here, as usual.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-12-05-python-appnexus/","title":"Python @ Appnexus","text":"<p>On my earlier blog post about python I talked about Appnexus' motivation and use cases of this language. The PyData 2012 video of the talk is available now and I strongly encourage you to listen to it as it's not only focused on python but covers also teamwork management and organization.</p> <p>[ylwm_vimeo height='360' width='640']53053331[/ylwm_vimeo]</p>","tags":["python"]},{"location":"Tech%20Blog/2012/2012-12-13-uwsgi-v1-4-3-released/","title":"uWSGI : v1.4.3 released","text":"<p>Bugfix release with a few features.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-12-13-uwsgi-v1-4-3-released/#highlights","title":"highlights","text":"<ul> <li>close useless file descriptors when spawning daemons or external commands</li> <li>fixed log format in cheaper busyness algo</li> <li>allow to override the number of cpus detected by the build system using the CPUCOUNT env var</li> </ul> <p>See the full changelog.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2012/2012-12-16-portugal-algarve/","title":"Portugal, Algarve","text":"<p>Rolleiflex</p>","tags":["portugal"]},{"location":"Tech%20Blog/2013/2013-01-10-uwsgi-v1-4-4-released/","title":"uWSGI : v1.4.4 released","text":"<p>Bug fix release only,\u00a0I've just bumped it on portage.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-01-10-uwsgi-v1-4-4-released/#highlights","title":"highlights","text":"<ul> <li>backported a couple of fixes for the https router</li> <li>fixed wrong typecasting in yaml and fixed subscription system on 32 bit</li> <li>added and additional error report for the gevent plugin if a read() fails</li> <li>improved apache2 mod_proxy_uwsgi</li> </ul> <p>See the full changelog as usual.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-01-11-rabbitmq-2-8-7-3-0-1-released/","title":"rabbitMQ 2.8.7 & 3.0.1 released","text":"<p>It took me quite a while to bump this package as v3.0.0 and v3.0.1 were released respectively on Nov. 19 and Dec. 11 but here they are. This bump is dedicated to Jasper @darkroom bar in NZ :)</p> <p>Anyway, that's quite a huge release for the rabbitMQ team which they described themselves very well :</p> <p>This release introduces dynamic, policy-based control of mirroring and federation, improves the user friendliness of clustering, adds support for per-message TTL, introduces plugins for web-STOMP and MQTT, and adds many smaller new features and bug fixes.</p> <p>In addition, performance is improved in several cases. Most notably, mirrored queues are substantially faster.</p> <p>So my highlights will be very modest compared to the real thing and I strongly encourage you to spend 5 minutes to read the changelogs.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-01-11-rabbitmq-2-8-7-3-0-1-released/#highlights","title":"highlights","text":"<ul> <li>allow queue mirroring to be defined by broker-wide policy, not queue declaration, and add \"exactly\" mode</li> <li>support per-message TTL</li> <li>enable heartbeats by default</li> <li>remove support for AMQP's \"immediate\" publish mode</li> <li>greatly improve performance of mirrored queues</li> <li>improve performance of SSL when using HiPE compilation</li> <li>improve performance of bulk dead-lettering</li> <li>new plugin: implement Message Queue Telemetry Transport version 3.1</li> <li>allow mixed patch versions of RabbitMQ in a cluster</li> </ul> <p>Read the 3.0.0 changelog and 3.0.1 changelog for more juicy stuff.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-01-15-coming-soon-on-mongodb/","title":"Coming soon on mongoDB","text":"<p>Some interesting stuff is cooking on the 2.3.x development branch of mongoDB, let's take a look at what we can expect to see in the future 2.4.x releases.</p>","tags":["mongodb","nosql"]},{"location":"Tech%20Blog/2013/2013-01-15-coming-soon-on-mongodb/#switch-to-v8-javascript-engine","title":"Switch to v8 Javascript engine","text":"<p>It's finally real since 2.3.1 as the folks at 10gen switched to v8 as the default JS engine powering mongoDB. This is a huge and a long time craved move which will primary improve performance and allow concurrent queries to be executed (aka collection level locking).</p>","tags":["mongodb","nosql"]},{"location":"Tech%20Blog/2013/2013-01-15-coming-soon-on-mongodb/#full-text-search","title":"Full text search","text":"<p>This one is around since 2.3.2 and will be available as a new type of index you'll have to create using the textSearchEnabled=true parameter. It's still a new feature under development so don't expect something able to compete with solr of course but still, it's a very nice feature ! You'll find more information about this on A. Jesse Jiryu Davis'\u00a0blog.</p>","tags":["mongodb","nosql"]},{"location":"Tech%20Blog/2013/2013-01-15-coming-soon-on-mongodb/#other-highlights","title":"Other highlights","text":"<ul> <li>Aggregation framework performance improvements</li> <li>New circular geospatial index type. Support for line, polygon, and point intersection queries as well as GeoJSON parsing.</li> <li>Better server stats framework</li> <li>Storage engine improvements to reduce fragmentation</li> <li>New operators : $push to sorted and fixed size arrays, $setOnInsert modifier for upserts, $geoNear and $within operators in aggregation framework</li> <li>_secondaryThrottle is now on by default\u00a0: this adds a write concern support for chunk migration reducing the replication lag caused by chunk moves</li> <li>--objcheck is now on by default\u00a0: the server validates the requests' objects before inserting the data. This used to have a slight performance impact but should be countered by v8 fairly well</li> </ul>","tags":["mongodb","nosql"]},{"location":"Tech%20Blog/2013/2013-01-18-clustering-corosync-v2-3-0-resource-agents-3-9-4/","title":"Clustering : corosync v2.3.0 & resource-agents-3.9.4","text":"<p>Some new stuff related to clustering are now available on portage ! Here are the highlights.</p>","tags":["cluster","corosync","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-01-18-clustering-corosync-v2-3-0-resource-agents-3-9-4/#crmsh-124","title":"crmsh-1.2.4","text":"<ul> <li>better pacemaker-1.1.8 compatibility</li> <li>fine tuning history and regression fixes</li> </ul>","tags":["cluster","corosync","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-01-18-clustering-corosync-v2-3-0-resource-agents-3-9-4/#resource-agents-394","title":"resource-agents-3.9.4","text":"<p>Oh yes I've been slacking on that one but it's finally here ! As a reminder this huge bump (1.0.4 -&gt; 3.9.4) is the result of the upstream merge from the pacemaker and rgmanager resource agents developments. This is reflected by a new rgmanager USE flag for those who want to install those resources.</p> <ul> <li>zabbixserver : new resource agent</li> <li>IPaddr2: partial rewrite and support for IPv6</li> <li>iscsi: support for auto recovery and performance improvements</li> <li>tools: replace the findif binary by findif.sh</li> </ul> <p>See the full changelog.</p>","tags":["cluster","corosync","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-01-18-clustering-corosync-v2-3-0-resource-agents-3-9-4/#corosync-145-and-corosync-230","title":"corosync-1.4.5 and corosync-2.3.0","text":"<p>The next releases of the flatiron and needle branches of corosync are rich of bug fixes, man updated and performance improvements. I wasn't able to find the proper changelog pages but you can have a look\u00a0here\u00a0where a bunch of the fixes are listed.</p>","tags":["cluster","corosync","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-01-20-portugal-villa-extramuros/","title":"Portugal, Villa Extramuros","text":"<p>Rolleiflex</p>","tags":["portugal","villa-extramuros"]},{"location":"Tech%20Blog/2013/2013-01-25-python-new-zeromq-and-mongodb-drivers/","title":"Python : new zeroMQ and mongoDB drivers","text":"","tags":["gentoo","portage","pymongo","python","release"]},{"location":"Tech%20Blog/2013/2013-01-25-python-new-zeromq-and-mongodb-drivers/#pyzmq-2201","title":"pyzmq-2.2.0.1","text":"<p>This one is very interesting to me because the code from the mighty gevent-zeromq library which brought gevent support to pyzmq has been merged into it ! I find it very humble and positive for the Open Source community to see such merges and want to express my gratitude to Travis Cline and the zeroMQ team for that.</p> <p>Migrating is as easy as :</p> <p># gevent-zeromq previous way from gevent_zeromq import zmq</p>","tags":["gentoo","portage","pymongo","python","release"]},{"location":"Tech%20Blog/2013/2013-01-25-python-new-zeromq-and-mongodb-drivers/#pyzmq-new-way","title":"pyzmq new way","text":"<p>from pyzmq.green import zmq</p> <p>I strongly encourage you to read the changelog.</p>","tags":["gentoo","portage","pymongo","python","release"]},{"location":"Tech%20Blog/2013/2013-01-25-python-new-zeromq-and-mongodb-drivers/#pymongo-242","title":"pymongo-2.4.2","text":"<p>Bugfix release, the main point is that\u00a0PyMongo will no longer select a replica set member for read operations that is not in primary or secondary state. Here is the changelog.</p>","tags":["gentoo","portage","pymongo","python","release"]},{"location":"Tech%20Blog/2013/2013-01-27-turkey-istanbul/","title":"Turkey, Istanbul","text":"<p>I used to go there quite a lot when I was young so I had a strange feeling to go back there after all those years. I feared not to be able to find the old Istanbul streets I knew but thankfully they do still exist.</p> <p></p> <p>Rolleiflex, dec. 2012</p>","tags":["istanbul","turkey"]},{"location":"Tech%20Blog/2013/2013-02-02-mongodb-v2-2-3-uwsgi-v1-4-5/","title":"mongoDB v2.2.3 & uwsgi v1.4.5","text":"<p>Today's bumps are interesting, kudos to both upstreams for their work.</p>","tags":["gentoo","mongodb","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-02-02-mongodb-v2-2-3-uwsgi-v1-4-5/#mongodb-223","title":"mongodb-2.2.3","text":"<p>The main benefit of this version is the performance improvements on replicaSets. You can see the changelog here.</p>","tags":["gentoo","mongodb","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-02-02-mongodb-v2-2-3-uwsgi-v1-4-5/#uwsgi-145","title":"uwsgi-1.4.5","text":"<p>This one is quite a bump. I was particularly interested to know that the memory leak discovered on the gevent loop got fixed. Have a look at all the nice backports too, they're worth it.</p> <ul> <li>added --python-version and --perl-version</li> <li>fixed a gevent memory leak</li> <li>backported --extract option</li> <li>added HTTP_Authorization support in mod_proxy_uwsgi</li> <li>backported --php-fallback</li> <li>backported perl uwsgi::postfork and uwsgi::atexit</li> <li>fixed a memory leak with --http-socket during uploads</li> <li>fixed unix signals usage in mules and spoolers</li> <li>reset cores/requests on worker startup</li> <li>fixed a refcnt bug in python uwsgi.workers()</li> <li>fixed async mode when multiple fds are in place</li> <li>fixed a cache collision bug</li> <li>backported --emperor-procname</li> <li>backported --touch-reload refactoring from 1.5</li> <li>backported master+emperor fix from 1.5</li> <li>new rbtree implementation (based on nginx) backported from 1.5</li> <li>backported new logvars from 1.5</li> </ul>","tags":["gentoo","mongodb","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-02-12-rabbitmq-v3-0-2-released/","title":"rabbitMQ : v3.0.2 released","text":"<p>Quick post for a bugfix release.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-02-12-rabbitmq-v3-0-2-released/#highlights","title":"highlights","text":"<ul> <li>fix broken error reporting for rabbitmqctl</li> <li>fix race causing queues to crash when stopping mirroring</li> <li>prevent rabbitmqctl status from killing web-STOMP connections</li> <li>fix hang of rabbitmqctl status when JSON-RPC plugin enabled</li> </ul> <p>Read the full changelog.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-02-13-moosefs-v1-6-26-released/","title":"MooseFS : v1.6.26 released","text":"<p>This was one of my first package creation in portage and although it doesn't have a quick iteration I still find this software very interesting. I was glad to see upstream release a new stable version but I have to admit I slacked quite a lot on actually seeing this bump (2012-08-16) :) Anyway, it's now live for some time in portage and I hope some folks were happy to update their platform with it.</p>","tags":["gentoo","moosefs","portage","release"]},{"location":"Tech%20Blog/2013/2013-02-13-moosefs-v1-6-26-released/#changelog","title":"changelog","text":"<ul> <li>(all) fixed signal handling in multithreaded modules</li> <li>(master) added goal and trashtime limits to mfsexport.cfg</li> <li>(metalogger) added simple check for downloaded metadata file (inspired by Davies Liu)</li> <li>(master) better handle disk full (inspired by Davies Liu)</li> <li>(master+metalogger) added keeping previous copies of metadata (inspired by Davies Liu)</li> <li>(all) reload all settings on \"reload\" (SIGHUP)</li> <li>(cs) disk scanning in background</li> <li>(cs) fixed long termination issue (found by Davies Liu)</li> </ul>","tags":["gentoo","moosefs","portage","release"]},{"location":"Tech%20Blog/2013/2013-02-15-clustering-resource-agents-v3-9-5-crmsh-v1-2-5/","title":"Clustering : resource-agents v3.9.5 & crmsh v1.2.5","text":"<p>Quick post about two bumps I made yesterday. The important one is sys-cluster/resource-agents-3.9.5 because the previous release contained a regression on the IPaddr2 resource. IPaddr2 didn't send unsolicited ARPs on start, depending on the ARP cache timeout time of the hosts on your topology, this could cause some serious delay when a failover takes place ! Also note the nice additions on crmsh which will make our lives easier.</p>","tags":["cluster","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-02-15-clustering-resource-agents-v3-9-5-crmsh-v1-2-5/#resource-agents-395","title":"resource-agents-3.9.5","text":"<ul> <li>fix IPaddr2 ARP regression</li> <li>pgsql: support starting as Hot Standby</li> <li>support for RA tracing</li> </ul>","tags":["cluster","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-02-15-clustering-resource-agents-v3-9-5-crmsh-v1-2-5/#crmsh-125","title":"crmsh-1.2.5","text":"<ul> <li>cibconfig: modgroup command</li> <li>cibconfig: directed graph support</li> <li>history: diff command (between PE inputs)</li> <li>history: show command (show configuration of PE inputs)</li> <li>history: graph command</li> </ul>","tags":["cluster","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-02-16-postfix-2-10-0/","title":"Postfix 2.10.0","text":"<p>Earlier this week, Wietse Venema announced the latest stable release of postfix, the famous Mail Transfer Agent. As I'm a long time user of this MTA, I thought I'd give it an echo on my blog with the usual highlights for you lazy readers.</p>","tags":["postfix","release"]},{"location":"Tech%20Blog/2013/2013-02-16-postfix-2-10-0/#highlights","title":"highlights","text":"<ul> <li>Separation of relay policy (with smtpd_relay_restrictions) from spam policy (with smtpd_{client, helo, sender, recipient}_restrictions), which makes accidental open relay configuration less likely. The default is backwards compatible.</li> <li>HAproxy load-balancer support for postscreen(8) and smtpd(8). The nginx proxy was already supported by Postfix 2.9 smtpd(8), using XCLIENT commands.</li> <li>Support for the TLSv1 and TLSv2 protocols, as well as support to turn them off if needed for interoperability.</li> <li>Laptop-friendly configuration. By default, Postfix now uses UNIX-domain sockets instead of FIFOs, and thus avoids MTIME file system updates on an idle mail system.</li> <li>Revised postconf(1) command. The \"-x\" option expands $name in a parameter value (both main.cf and master.cf); the \"-o name=value\" option overrides a main.cf parameter setting; and postconf(1) now warns about a $name that has no name=value setting.</li> </ul>","tags":["postfix","release"]},{"location":"Tech%20Blog/2013/2013-02-18-mongodb-2-4-0-rc/","title":"mongoDB 2.4.0 RC","text":"<p>Last month I talked about what was coming on the next major stable release of mongoDB and I'm glad to point you to the recent 2.4.0 release notes which were announced today.</p> <p>Undoubtedly, the \"new\" cool stuff is about the hashed index feature\u00a0which will help those who don't have a really even shard key to distribute their data on. Promising future as always !</p>","tags":["mongodb","nosql"]},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/","title":"Meet py3status","text":"<p>This is the first public release of one of my open-source projects, don't hesitate to share some feedback and/or thoughts with me.</p>","tags":["i3wm","py3status","python"]},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#background","title":"Background","text":"<p>As a sysadmin, I have\u00a0a lot of consoles open on multiples desktops and my 30\" screen was still not enough to cover my needs. To make things short, I needed to spare every pixel I could and KDE was really frustrating me as it was wasting a lot of space and ran quite a bunch of useless stuff in the background (akonadi/nepomuk anyone ?).</p> <p>Then came my cyclic rage about it and I finally found my precious\u00a0: i3wm. I just love it as it is what I ever needed : a lightweight yet very functional and handy WM.</p> <ul> <li>No more resizing my consoles to fit next to each other and I can still use floating windows for the needed applications.</li> <li>No more huge and pixel-hungry task bar, just a simple and very efficient one.</li> </ul>","tags":["i3wm","py3status","python"]},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#customization","title":"customization","text":"<p>The problem when you start using something new and awesome is that you get a lot of ideas on what you could do with it and how you'd love to customize it. I mean, when using KDE or Gnome, your ideas are quickly shaped by the fact that you'd have to learn some exotic framework or language to implement them.</p> <ul> <li> <p>Did you ever ask yourself how to add your own stuff in your task bar on KDE or Gnome ?</p> </li> <li> <p>What if the customization options you want are not available in your WM menus ?</p> </li> </ul> <p>Well, my answer was \"never mind\" tbh and I slowly even lost the idea of implementing anything on my task bar.</p>","tags":["i3wm","py3status","python"]},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#i3bar-i3status","title":"i3bar &amp; i3status","text":"<p>After switching to i3wm, my first customization was to name my workspaces and setup my own colors to adjust the look &amp; feel of my desktop. Then I started to tune the program responsible for displaying useful information on my bar : i3status. As you may know, you have some limited modules which can take care of displaying some useful information on your bar such as the free disk space on a disk partition or your wired/wireless network status.</p> <p>But then I asked myself the same questions as I used to on my KDE days : what if I want more ? my own stuff on my task bar ?</p>","tags":["i3wm","py3status","python"]},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#introducing-py3status","title":"Introducing py3status","text":"<p>Thanks to the i3bar open and simple protocol and the robust (even if somewhat limited) i3status\u00a0program, I could finally hack into my bar. Naturally, I had to do it myself and there was a few examples available on the net but nothing really handy and extensible enough. That's how I had the idea of developping py3status !</p>","tags":["i3wm","py3status","python"]},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#philosophy-goals","title":"philosophy &amp; goals","text":"<ul> <li>no extra configuration file needed</li> <li>rely on i3status and its existing configuration as much as possible</li> <li>be extensible, it must be easy for users to add their own stuff/output by writing a simple python class which will be loaded and executed dynamically</li> <li>add some built-in enhancement/transformation of basic i3status modules output</li> </ul>","tags":["i3wm","py3status","python"]},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#available-now-on-github","title":"available now on github","text":"<p>I'm glad to announce that I pushed it today on github ! You can start using py3status now and give your feedback. I hope this project will help users get more of their i3wm environment and encourage their hacking power !</p>","tags":["i3wm","py3status","python"]},{"location":"Tech%20Blog/2013/2013-02-23-packaging-py3status/","title":"Packaging py3status","text":"<p>So I exchanged some mails with Michael Stapelberg of i3wm who rightly pointed out that my initial installation method of py3status was un-pythonic. I was not satisfied of using a bash setup either and I couldn't imagine a better opportunity to learn how to write a proper setup.py for my project.</p> <p>Thanks to my Gentoo Linux packager experience, I knew what I had to do, so a few searches and tests later I'm glad to announce that py3status installation is standard ! I of course also packaged py3status for Gentoo Linux users : meet x11-misc/py3status on my overlay.</p> <p>py3status being a real command and not a simple python module, I had to find the way to have setuptools taking care of this for me. I was happy to find out that this is pretty easy and that it works on both Linux &amp; Windows, it's awesome !</p> <p>I will explain all this in one of my next blog post as I'm sure it can be of interest.</p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-02-26-uwsgi-mono-asp-net-support/","title":"uWSGI : Mono ASP.NET support","text":"<p>Quick post to point out that the upcoming 1.9 release of uWSGI will support running ASP.NET applications using Mono.</p> <p>Not to mention all the benefits of the uWSGI application deployment and monitoring mechanisms, this is a very interesting point as you can have a unique platform running your Python and ASP.NET applications ! I'll certainly keep an eye on it.</p>","tags":["development","uwsgi"]},{"location":"Tech%20Blog/2013/2013-02-26-uwsgi-v1-4-6-released/","title":"uWSGI : v1.4.6 released","text":"<p>This is quite a big release for its backports and improvements. But there are two more interesting things to note here : uWSGI supports Heroku and Dreamhost deployments !</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-02-26-uwsgi-v1-4-6-released/#tutorials","title":"tutorials","text":"<ul> <li>uWSGI + Heroku + Python tutorial</li> <li>uWSGI + Dreamhost tutorial</li> </ul>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-02-26-uwsgi-v1-4-6-released/#highlights","title":"highlights","text":"<ul> <li>fix SERVER_PORT value in corerouters when using shared sockets</li> <li>backported --thunder-lock option to reduce thundering herd problem (use with caution)</li> <li>fixed pthread robust mutexes in newer glibc</li> <li>backported improvements for the alarm_xmpp plugin</li> <li>fixed suspend when harakiri is in place</li> <li>reset sigmask on startup</li> <li>fixed master+emperor configurations</li> <li>backported more logvars (check here: https://uwsgi-docs.readthedocs.org/en/latest/LogFormat.html)</li> <li>fixed muleloop in uwsgidecorators</li> <li>fixed a refcnt bug in the psgi plugin spotted by Nick Gregory (issue #158)</li> <li>backported new python build system (Heroku friendly)</li> <li>backported --perl-arg and --perl-args options (to add items in @ARGV)</li> <li>backported perl async fixes</li> <li>allows --attach-daemon without workers</li> </ul>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-03-04-uwsgi-v1-4-8-released/","title":"uWSGI : v1.4.8 released & v2.0 sneak peek","text":"<p>Yet another bump for uWSGI as upstream is working hard on the 1.9 branch which will lead to the 2.0 LTS version. I guess it's time I take a few moments to give you some hints about what's coming for the v2.0 of uWSGI, be aware that this is some heavy stuff.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-03-04-uwsgi-v1-4-8-released/#future-v20-highlights","title":"future v2.0 highlights","text":"<ul> <li>new fully non-blocking API which applies to all plugins, this will benefit the perl/PSGI plugin as well</li> <li>faster uwsgi/HTTP/FastCGI/SCGI native sockets thanks to better parsers</li> <li>split error logging from request logging for enhanced debugging</li> <li>more offloading\u00a0improvements such as a new function to write files on disk and non-blocking workers for static files service</li> <li>better static files handling thanks to the new caching system</li> <li>totally rewritten web cache system allows you to have multiple caches per instance and tune them finely</li> <li>replaced the old clustering system with a new Legion subsystem providing resources management (yeah you wouldn't need stuff like pacemaker to handle your uWSGI cluster)</li> <li>advanced exception subsystem</li> <li>SPDY v3 support</li> <li>SNI support</li> <li>support for\u00a0HTTP router keepalive, auto-chunking, auto-gzip and transparent websockets</li> <li>a SSL router will be available</li> <li>websockets API sponsored by\u00a020Tab S.r.l. (a company working on HTML5 browsers game, thanks guys)</li> <li>programmable internal router</li> <li>and of course, the Mono/ASP.NET plugin I talked about in my previous post</li> </ul> <p>See the full and detailed list here</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-03-04-uwsgi-v1-4-8-released/#v148-highlights","title":"v1.4.8 highlights","text":"<ul> <li>added support for ruby 2.0</li> <li>removed the mono/asp.net plugin (a new, working one, is in 1.9)</li> <li>backported the improved carbon plugin</li> <li>fixed a corner-case bug with the caching subsystem (Laurent Luce)</li> <li>fixed ipcsem on Linux</li> <li>backported --not-log-alarm (negative version of --log-alarm)</li> <li>backported add_timer and add_rb_timer api functions for the perl/psgi plugin</li> <li>backported --for-glob, this is like --for but with glob expansion (Guido Berhoerster)</li> <li>avoid gateways crash on master shutdown</li> <li>backported https re-handshake management</li> <li>improved gevent timeout management</li> <li>uWSGI can now be installed as a ruby gem</li> <li>backported --http-socket-modifier1/2</li> </ul>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-03-05-switching-from-xchat-to-hexchat/","title":"Switching from Xchat to HexChat","text":"<p>As polynomial-c announced last year, xchat upstream is dead for some time now. Fortunately for us the fork hexchat is taking over and is compatible with your current xchat configuration so it's really easy to migrate.</p>","tags":["hexchat","irc","xchat"]},{"location":"Tech%20Blog/2013/2013-03-05-switching-from-xchat-to-hexchat/#migrating-to-hexchat","title":"migrating to hexchat","text":"<p>$ mv .xchat2/ .config/hexchat/ $ mv .config/hexchat/xchat.conf .config/hexchat/hexchat.conf</p> <p>As mentioned in the comments by Louis Tim Larsen, if you have some servers configured with more than one channel make sure to run this command as well :</p> <p>sed -i 's/,#/\\nJ=#/g' ~/.config/hexchat/servlist.conf</p> <p>Then run hexchat as you would xchat.</p>","tags":["hexchat","irc","xchat"]},{"location":"Tech%20Blog/2013/2013-03-05-switching-from-xchat-to-hexchat/#theme-your-irc","title":"theme your IRC","text":"<p>If like me you're a fan of Solarized, you can get hexchat to use these colors thanks to some contributed\u00a0themes. Here's the quick hack to have it done, for the Solarized Light theme.</p> <p>$ cd .config/hexchat $ wget http://dl.hexchat.org/themes/Solarized%20Light.hct $ unzip -o Solarized\\ Light.hct</p> <p>Restart hexchat and enjoy your up to date and colorized IRC client.</p>","tags":["hexchat","irc","xchat"]},{"location":"Tech%20Blog/2013/2013-03-11-uwsgi-v1-4-9-released/","title":"uWSGI : v1.4.9 released","text":"<p>Yet another version bump for this very active package.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-03-11-uwsgi-v1-4-9-released/#highlights","title":"highlights","text":"<ul> <li>avoid crashing carbon on master shutdown</li> <li>call ERR_clear_error after each https session close</li> <li>fixed broodlord mode</li> <li>removed broken JVM and JWSGI plugins (stable versions are in 1.9)</li> <li>backported cache_update for lua and fixed its lock handling</li> </ul> <p>Full changelog is here.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2013/2013-03-13-pacemaker-vulnerability-and-v1-1-9-release/","title":"Pacemaker vulnerability and v1.1.9 release","text":"<p>A security vulnerability (CVE-2013-0281)\u00a0was found on pacemaker which permitted attackers to prevent your cluster from serving more CIB requests. Although this issue was quickly fixed by upstream, they didn't add a new tag to pacemaker so I did ask Andrew Beekhof\u00a0for one so I could take care of bug #457572. Gentoo users, here comes pacemaker-1.1.9 !</p>","tags":["cluster","gentoo","pacemaker","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-13-pacemaker-vulnerability-and-v1-1-9-release/#important","title":"important","text":"<p>While packaging and testing pacemaker-1.1.9, I ran into some weird permission issues which I debugged with @beekhof and @asalkeld\u00a0(thx again guys). Turns out that when enabling ACL support on pacemaker, you now need to add root to the haclient group ! The reason is that pacemaker now uses shared memory IPC sockets from libqb to communicate with corosync (on /dev/shm/).</p>","tags":["cluster","gentoo","pacemaker","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-13-pacemaker-vulnerability-and-v1-1-9-release/#v119-changelog","title":"v1.1.9 changelog","text":"<ul> <li>corosync: Allow cman and corosync 2.0 nodes to use a name other than uname()</li> <li>corosync: Use queues to avoid blocking when sending CPG messages</li> <li>Drop per-user core directories</li> <li>ipc: Compress messages that exceed the configured IPC message limit</li> <li>ipc: Use queues to prevent slow clients from blocking the server</li> <li>ipc: Use shared memory by default</li> <li>lrmd: Support nagios\u00a0remote monitoring</li> <li>lrmd: Pacemaker Remote Daemon for extending pacemaker functionality outside corosync cluster.</li> <li>pengine: Check for master/slave resources that are not OCF agents</li> <li>pengine: Support a 'requires' resource meta-attribute for controlling whether it needs quorum, fencing or nothing</li> <li>pengine: Support for resource container</li> <li>pengine: Support resources that require unfencing before start</li> </ul> <p>Since the main focus of the bump was to fix a security issue, I didn't add the new nagios feature to the ebuild. If you're interested in it, just say so and I'll do my best to add it asap.</p>","tags":["cluster","gentoo","pacemaker","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-15-follow-up-on-pacemaker-v1-1-9-and-updated-pacemaker-gui/","title":"Follow-up on pacemaker v1.1.9 and updated pacemaker-gui","text":"<p>In my previous post\u00a0I talked about a permission problem introduced in pacemaker-1.1.9 which requires root to be a member of the haclient group. I've been helping @beekhof to investigate on this and I'm glad he found and fixed both the problem and a memory leak ! We're still investigating on another issue but we should be seeing a new version bump pretty soon, thank you Andrew !</p>","tags":["cluster","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-15-follow-up-on-pacemaker-v1-1-9-and-updated-pacemaker-gui/#pacemaker-gui-v212","title":"pacemaker-gui v2.1.2","text":"<p>One of my colleagues recently complained that pacemaker-gui-2.1.1 was not compatible with newer pacemaker releases (&gt;=1.1.8) so he had to install pacemaker-1.1.7 if he wanted to benefit from the GUI. I contacted @gao-yan\u00a0from SUSE who's the main upstream for this package and asked him for a tag bump. Here comes pacemaker-gui-2.1.2 which is compatible with all newer pacemaker releases ! Thanks again mate.</p>","tags":["cluster","gentoo","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-16-rabbitmq-v3-0-4-released/","title":"rabbitMQ : v3.0.4 released","text":"<p>Within a week, rabbitMQ got bumped twice. I'm happy to quickly post about those recent bumps so here is a highlight of 3.0.3 and 3.0.4 changelogs.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-03-16-rabbitmq-v3-0-4-released/#highlights","title":"highlights","text":"<ul> <li>fix connection failure to start reading again in rare circumstances when coming out of flow control</li> <li>ensure invocation of \"rabbitmqctl stop_app\" during server startup on a fresh node does not leave a corrupted Mnesia schema</li> <li>ensure messages expire immediately when reaching the head of a queue after basic.get</li> <li>ensure parameters and policies for a vhost are removed with that vhost</li> <li>do not log spurious errors for connections that close very early</li> <li>ensure \"rabbitmqctl forget_cluster_node\" removes durable queue records for unmirrored queues on the forgotten node</li> <li>clean up connection and channel records from nodes that have crashed</li> <li>do not show 404 errors when rabbitmq_federation_management is installed and rabbitmq_federation is not</li> <li>ensure the reader process hibernates when idle</li> <li>prevent x-received-from header from leaking upstream credentials</li> </ul>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-03-19-py3status-v0-5/","title":"py3status v0.5","text":"<p>Since the first release of py3status, quite a bunch of bugfixes and features came such as python3 support and SIGUSR1 signal handling to force an update of the bar.</p>","tags":["i3wm","py3status"]},{"location":"Tech%20Blog/2013/2013-03-19-py3status-v0-5/#changelog","title":"changelog","text":"<ul> <li>bugfix : fix delta variable declaration</li> <li>examples : add GLPI open tickets counter module example</li> <li>python3 compatibility inspired by waaaaargh (Johannes Firlefanz)</li> <li>improvement : iterate over user classes in a sorted manner to allow a predictive ordering of outputs</li> <li>bugfix : dont fail if i3status output comes slower than py3status message polling interval</li> <li>feature : signal SIGUSR1 forces i3status and i3bar refresh, feature request by Michael Schaefer</li> </ul>","tags":["i3wm","py3status"]},{"location":"Tech%20Blog/2013/2013-03-20-mongodb-v2-4-0-released/","title":"mongoDB : v2.4.0 released","text":"<p>A few months ago, I pointed out what was coming with this release and did an update\u00a0of this cooking 2.4.0 later. Yesterday, 10gen announced the release of the new stable branch of mongoDB v2.4.0. Instead of talking about it again, I'll focus on what this release brings to Gentoo users as I'm glad to announce that it's already available in portage.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-20-mongodb-v2-4-0-released/#ssl-support","title":"SSL support","text":"<p>First of all, I think it was a good time to close bug #421289 and finally enable the SSL support via the ssl\u00a0USE flag. I'll support it as much as upstream does, so don't expect some big magic about it.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-20-mongodb-v2-4-0-released/#shared-client-library","title":"Shared client library","text":"<p>Since this has always been a mess, I also added the sharedclient USE flag so that users who really need the client shared library can toggle its installation easily. This also permits me to isolate possible problems from the main ebuild.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-20-mongodb-v2-4-0-released/#upgrading-to-24","title":"Upgrading to 2.4","text":"<p>This is seamless unless you're running a sharded cluster\u00a0! In this case, take great care of what you do and note that the upgrade is only possible if your cluster is running v2.2\u00a0! Please read with care the upgrade plan.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-24-python-writing-a-proper-setup-for-your-project/","title":"Python : writing a proper setup for your project","text":"<p>For an open-source project to be adopted, it must be easy to install and test it. Before publicly releasing py3status I thus had to figure out how to get my python project installed properly for every user who would be interested in it.</p> <p>The common and most efficient way of writing a setup process in python is by using the setuptools package and writing your own setup.py file. Doing so is not hard at all as there are quite a bunch of examples you can start from but my challenge was that py3status is not a library you install and then import on your python code, instead it must be seen and used as an executable available for all users (something like /usr/bin/py3status). I'll cover the steps I used to achieve this kind of installation.</p>","tags":["python","setup"]},{"location":"Tech%20Blog/2013/2013-03-24-python-writing-a-proper-setup-for-your-project/#setuppy-basics","title":"setup.py basics","text":"<p>You can see the setup.py file just like any other python program you write where you import the functions you need from setuptools.</p> <p>import os from setuptools import find_packages, setup</p> <p>Basically, a setup.py file is just a call to setup\u00a0with a fair amount of parameters depending on the size and complexity of your project. Let's see a basic usage with no real magic.</p> <p>setup(     name='py3status',     version='0.5',     url='https://github.com/ultrabug/py3status/wiki',     download_url='https://github.com/ultrabug/py3status',     license='BSD',     author='Ultrabug',     author_email='ultrabug@sikritdomain.com',     description='py3status is an extensible i3status wrapper written in python',     long_description='this is a very long description which im writing for example',     platforms='any',     )</p> <p>As you can see those parameters are just fields describing your project but there are of course more parameters you can use to become more specific about it such as which other packages it depends from or special operations to be made upon installation.</p>","tags":["python","setup"]},{"location":"Tech%20Blog/2013/2013-03-24-python-writing-a-proper-setup-for-your-project/#classifiers","title":"classifiers","text":"<p>As with a literal description, you must categorize your project so that it will be correctly understood by automatic classifiers for example. The classifiers parameter is a list of those categories which you can find a list here.</p> <pre><code>classifiers=[\n    'License :: OSI Approved :: BSD License',\n    'Operating System :: POSIX :: Linux',\n    'Programming Language :: Python',\n    'Programming Language :: Python :: 2.5',\n    'Programming Language :: Python :: 2.6',\n    'Programming Language :: Python :: 2.7',\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: 3.2',\n    'Topic :: Software Development :: Libraries :: Python Modules',\n    'Topic :: Desktop Environment :: Window Managers :: i3wm',\n    ],\n</code></pre>","tags":["python","setup"]},{"location":"Tech%20Blog/2013/2013-03-24-python-writing-a-proper-setup-for-your-project/#getting-an-executable-from-your-python-program","title":"getting an executable from your python program","text":"<p>As I explained earlier, py3status must be used as an executable available in the users' PATH just like any other binary or commands on the system. I was thrilled to discover that achieving this is a piece of cake using setuptools, you just have to use the entry_points parameter and it will be taken care of for you.</p> <p>entry_points={         'console_scripts': [             'py3status = py3status:main',             ]         },</p> <p>So here I'm asking setuptools to create a script which will execute py3status' main\u00a0function. It will generate a python program that just does that, call it py3status, place it in /usr/bin and make it executable. Et voil\u00e0 ! An important thing to note is that it also works in Windows and that's how you'll get a .exe from your python code !</p> <p>Learn more on this subject by reading the excellent documentation on how to get started with setuptools.</p>","tags":["python","setup"]},{"location":"Tech%20Blog/2013/2013-03-25-mongodb-v2-4-1-and-pymongo-2-5-released/","title":"mongoDB v2.4.1 and pymongo 2.5 released","text":"<p>10gen released a critical update for mongoDB 2.4.0 which affected queries\u00a0on secondaries, you should upgrade asap. The python mongo driver followed the 2.4.x releases and got bumped to 2.5 this week-end. I am pleased to announce that I took the chance to add the kerberos authentication support to both ebuilds while bumping them.</p>","tags":["gentoo","mongodb","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-25-mongodb-v2-4-1-and-pymongo-2-5-released/#pymongo-25","title":"pymongo-2.5","text":"<ul> <li>GSSAPI (Kerberos) authentication</li> <li>SSL certificate validation with hostname matching</li> <li>Delegated and role based authentication</li> </ul>","tags":["gentoo","mongodb","portage","release"]},{"location":"Tech%20Blog/2013/2013-03-30-tokyo-mont-fuji/","title":"Tokyo, mont Fuji","text":"<p>Photos prises du haut de la mairie de Bunkyo-ku, il y avait beaucoup de Japonais car le coucher de soleil laissait voir le mont Fuji entre les tours. Moment magique !</p> <p></p> <p></p> <p>Rolleiflex</p>","tags":["rolleiflex","tokyo"]},{"location":"Tech%20Blog/2013/2013-04-06-py3status-v0-7/","title":"py3status v0.7","text":"<p>Some cool bugfixes happened since\u00a0v0.5 and py3status broke the 20 github stars, I hope people are enjoying it.</p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-04-06-py3status-v0-7/#changelog","title":"changelog","text":"<ul> <li>clear the user class cache when receiving SIGUSR1</li> <li>specify default folder for user defined classes</li> <li>fix time transformation thx to @Lujeni</li> <li>add Pingdom checks latency example module</li> <li>fix issue #2 reported by @Detegr which caused the clock to drift on some use cases</li> </ul>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-04-14-py3status-v0-8/","title":"py3status v0.8","text":"<p>I went on a coding frenzy to implement most of the stuff I was not happy with py3status so far. Here comes py3status\u00a0code name : San Francisco (more photos to come). </p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-04-14-py3status-v0-8/#pep8","title":"PEP8","text":"<p>I always had the habit of using tabulators to indent my code. @Lujeni pointed out that this is not a PEP8 recommended\u00a0method and that we should start respecting more of it in the near future. Well, he's right and I guess it was time to move on so I switched to using spaces and corrected a lot of other coding style stuff which got my code a score going from around -1/10 to around 9.5/10 on pylint !</p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-04-14-py3status-v0-8/#threaded-modules-execution","title":"Threaded modules' execution","text":"<p>This was the major thing I was not happy with : when a user-written module was executed for injection, the time it took to get its response would cause py3status to stop updating the bar. This means that if you had a database call to make to get some stuff you need displayed on the bar and it took 10 seconds, py3status was sleeping for those 10 seconds to update the bar ! This behavior could cause some delays in the clock ticking for example.</p> <p>I decided to offload all of the modules' detection and execution to a thread to solve this problem. To be frank, this also helped to rationalize the code better as well. No more delays and a cleaner handling is what you get, stuff will start appending themselves whatever the time they take to execute !</p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-04-14-py3status-v0-8/#python3","title":"Python3","text":"<p>It was about time the examples available on py3status would also work using python3.</p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-04-16-san-francisco-chinatown/","title":"San Francisco : chinatown","text":"","tags":["san-francisco"]},{"location":"Tech%20Blog/2013/2013-04-18-mongodb-v2-4-2-released/","title":"mongoDB v2.4.2 released","text":"<p>After the security issue related bumps of the previous releases which happened last weeks it was about time 10gen released a 2.4.x fixing the following issues:</p> <ul> <li>Fix for upgrading sharded clusters</li> <li>TTL assertion on replica set secondaries</li> <li>Several V8 memory leak and performance fixes</li> <li>High volume connection crash</li> </ul> <p>I guess everything listed above would have affected our cluster at work so I'm glad we've been patient on following-up this release :) See the\u00a0changelog for details.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2013/2013-04-18-py3status-v0-9/","title":"py3status v0.9","text":"<p>First of all\u00a0py3status is on pypi\u00a0! You can now install it with the simple and usual :</p> <p>$ pip install py3status</p> <p>This new version features my first pull request from @Fandekasp\u00a0who kindly wrote a pomodoro module which helps this\u00a0technique's\u00a0adepts by having a counter on their bar. I also fixed a few glitches on module injection and some documentation.</p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-04-18-san-francisco-streets/","title":"San Francisco : streets","text":"","tags":["san-francisco"]},{"location":"Tech%20Blog/2013/2013-04-24-hello-gentoo-planet/","title":"Hello Gentoo Planet","text":"<p>Hey Gentoo folks !</p> <p>I finally followed a friend's advice and stepped into the Gentoo Planet and Universe feeds. I hope my modest contributions will help and be of interest to some of you readers.</p> <p>As you'll see, I don't talk only about Gentoo but also about photography and technology more generally. I also often post about the packages I maintain or I have an interest in to highlight their key features or bug fixes.</p>","tags":["gentoo"]},{"location":"Tech%20Blog/2013/2013-04-26-mongodb-and-pacemaker-recent-bumps/","title":"mongoDB and Pacemaker recent bumps","text":"","tags":["cluster","gentoo","mongodb","nosql","pacemaker","portage","release"]},{"location":"Tech%20Blog/2013/2013-04-26-mongodb-and-pacemaker-recent-bumps/#mongodb-243","title":"mongoDB 2.4.3","text":"<p>Yet another bugfix release, this new stable branch is surely one of the most quickly iterated I've ever seen. I guess we'll wait a bit longer at work before migrating to 2.4.x.</p>","tags":["cluster","gentoo","mongodb","nosql","pacemaker","portage","release"]},{"location":"Tech%20Blog/2013/2013-04-26-mongodb-and-pacemaker-recent-bumps/#pacemaker-1110_rc1","title":"pacemaker 1.1.10_rc1","text":"<p>This is the release of pacemaker we've been waiting\u00a0for, fixing among other things, the ACL problem which was introduced in 1.1.9. Andrew and others are working hard to get a proper 1.1.10 out soon, thanks guys.</p> <p>Meanwhile, we (gentoo cluster herd) have been contacted by @Psi-Jack who has offered his help to follow and keep some of our precious clustering packages up to date, I wish our work together will benefit everyone !</p> <p>All of this is live on portage, enjoy.</p>","tags":["cluster","gentoo","mongodb","nosql","pacemaker","portage","release"]},{"location":"Tech%20Blog/2013/2013-05-16-fujifilm-gf670w/","title":"Fujifilm GF670W","text":"<p>It's been so long since I switched to film-only photography that I decided a few months ago to sell all my digital equipment. I already own a Nikon FM2 camera which I love but I've to admit that I was and still am totally amazed by the pictures taken by my girlfriend's Rolleiflex 3.5F. The medium format is the kind of rendering I was craving to get and that sooner or later I'd step into the medium format world. Well, I didn't have to wait as when we were in Tokyo to celebrate new year 2013 I fell in love with what was the perfect match between my love for wide angles and medium format film photography : the Fujifilm GF670W !</p> <p>For my soon to come birthday, I got myself my new toy in advance so I could use it in my upcoming roadtrip around France (I'll talk about it soon, it was awesome). Oddly, the only places in the world where you can get this camera is in the UK and in Japan so I bought it from the very nice guys at Dale photographic. Here is the beast (literally) :</p> <p></p> <p>Yes, this is a big camera and it comes with a very nice leather case and a lens hood. This is a telemetric camera with a comfortable visor, it accepts 120 and 220 films and is capable of shooting in standard 6x6 and 6x7 !</p> <p>In the medium format world, the 55mm lens is actually a wide angle one as it is comparable to a 28mm in the usual 24x36 world. Its performances are not crazy on paper with a 4.5 aperture and a shutter speed going from 4s to 1/500s (as fast as a 1956 Rolleiflex) but the quality is just stunning as it's sharp and offers a somewhat inexistent chromatic aberration.</p> <p>Want proof ? These are some of my first roll's shoots uploaded at full resolution :</p> <p></p> <p></p>","tags":["gf670w"]},{"location":"Tech%20Blog/2013/2013-05-17-squid-proxy-blocking-download-of-some-file-extensions/","title":"Squid proxy : blocking download of some file extensions","text":"<p>It is a common request in squid to have it block downloading certain files based on their extension in the url path. A quick look at google's results on the subject apparently gives us the solution to get this done easily by squid.</p> <p>The common solution is to create an ACL file listing regular expressions of the extensions you want to block and then apply this to your http_access rules.</p>","tags":["gentoo","proxy","squid"]},{"location":"Tech%20Blog/2013/2013-05-17-squid-proxy-blocking-download-of-some-file-extensions/#blockextensionsacl","title":"blockExtensions.acl","text":"<p>\\.exe$</p>","tags":["gentoo","proxy","squid"]},{"location":"Tech%20Blog/2013/2013-05-17-squid-proxy-blocking-download-of-some-file-extensions/#squidconf","title":"squid.conf","text":"<p>acl blockExtensions urlpath_regex -i \"/etc/squid/blockExtensions.acl\"</p> <p>[...]</p> <p>http_access allow localnet !blockExtensions</p> <p>Unfortunately this is not enough to prevent users from downloading .exe files.\u00a0The mistake here is that we assume that the URL will strictly finish by the extension we want to block, consider the two examples below :</p> <p>http://download.com/badass.exe     // will be DENIED as expected</p> <p>http://download.com/badass.exe?    // WON'T be denied as it does not match the regex !</p> <p>Squid uses the extended regex processor which is the same as egrep. So we need to change our blockExtensions.acl file to handle the possible ?whatever\u00a0string which may be trailing our url_path. Here's the solution to handle all the cases :</p>","tags":["gentoo","proxy","squid"]},{"location":"Tech%20Blog/2013/2013-05-17-squid-proxy-blocking-download-of-some-file-extensions/#blockextensionsacl_1","title":"blockExtensions.acl","text":"<p>\\.exe(\\?.*)?$ \\.msi(\\?.*)?$ \\.msu(\\?.*)?$ \\.torrent(\\?.*)?$</p> <p>You will still be hated for limiting people's need to download and install shit on their Windows but you implemented it the right way and no script kiddie can brag about bypassing you ;)</p>","tags":["gentoo","proxy","squid"]},{"location":"Tech%20Blog/2013/2013-05-21-rabbitmq-v3-1-0-released/","title":"rabbitMQ : v3.1.1 released","text":"<p>EDIT: okay, they just released v3.1.1 so here it goes on portage as well !</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-05-21-rabbitmq-v3-1-0-released/#highlights","title":"highlights","text":"<ul> <li>relax validation of x-match binding to headers exchange for compatibility with brokers &lt; 3.1.0</li> <li>fix bug in ack handling for transactional channels that could cause queues to crash</li> <li>fix race condition in cluster autoheal that could lead to nodes failing to re-join the cluster</li> </ul> <p>3.1.1 changelog is here.</p> <p>I've bumped the rabbitMQ message queuing server on portage. This new version comes with quite a nice bunch of bugfixes and features.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-05-21-rabbitmq-v3-1-0-released/#highlights_1","title":"highlights","text":"<ul> <li>eager synchronisation of slaves by policy (manual &amp; automatic)</li> <li>cluster \"autoheal\" mode to automatically choose nodes to restart when a partition has occurred</li> <li>cluster \"pause minority\" mode to prefer partition tolerance over availability</li> <li>improved statistics (including charts) in the management plugin</li> <li>quite a bunch of performance improvements</li> <li>some nice memory leaks fixes</li> </ul> <p>Read the full changelog.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-05-25-san-francisco-street-art/","title":"San Francisco : street art","text":"","tags":["san-francisco"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/","title":"Using keepalived for a self-balancing cluster","text":"<p>Load balancing traffic between servers can sometimes lead to headaches depending on your topology and budget. Here I'll discuss how to create a self load balanced cluster of web servers distributing HTTP requests between themselves and serving them at the same time. Yes, this means that you don't need dedicated load balancers !</p> <p>I will not go into the details on how to configure your kernel for ipvsadm etc since it's already covered enough on the web but instead focus on the challenges and subtleties of achieving a load balancing based only on the realservers themselves. I expect you reader have a minimal knowledge of the terms and usage of ipvsadm and keepalived.</p>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#the-setup","title":"The setup","text":"<p>Let's start with a scheme and some principles explaining our topology.</p> <ul> <li>3 web servers / realservers (you can do the same using 2)</li> <li>Local subnet : 192.168.0.0/24</li> <li>LVS forwarding method : DR (direct routing)</li> <li>LVS scheduler : WRR (you can choose your own)</li> <li>VIP : 192.168.0.254</li> <li>Main interface for VIP : bond0</li> </ul> <p></p> <p>Let's take a look at what happens as this will explain a lot of why we should configure the servers in a quite special way.</p> <p>black arrow / serving</p> <ol> <li>the master server (the one who has the VIP) receives a HTTP port connection request</li> <li>the load balancing scheduler decides he's the one who'll serve this request</li> <li>the local web server handles the request and replies to the client</li> </ol> <p>blue arrow / direct routing / serving</p> <ol> <li>the master server receives a HTTP port connection request</li> <li>the load balancing scheduler decides the blue server should handle this request</li> <li>the HTTP packet is given to the blue server as-this (no modification is made on the packet)</li> <li>the blue server receives a packet whose destination IP is the VIP but he doesn't hold the VIP (tricky part)</li> <li>the blue server's web server handles the request and replies to the client</li> </ol>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#ip-configuration","title":"IP configuration","text":"<p>Almost all the tricky part lies in what needs to be done in order to solve the point #4 of the blue server example. Since we're using direct routing, we need to configure all our servers so they accept packets directed to the VIP even if they don't have it configured on their receiving interface.</p> <p>The solution is to have the VIP configured on the loopback interface (lo) with a host scope on the keepalived BACKUP servers while it is configured on the main interface (bond0) on the keepalived MASTER server. This is what is usually done when you use pacemaker and ldirectord with IPAddr2 but keepalived does not handle this kind of configuration natively.</p> <p>We'll use the notify_master and notify_backup directives of keepalived.conf to handle this :</p> <p>notify_master /etc/keepalived/to_master.sh notify_backup /etc/keepalived/to_backup.sh</p> <p>We'll discuss a few problems to fix before detailing those scripts.</p>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#the-arp-problem","title":"The ARP problem","text":"<p>Now some of you wise readers will wonder about the ARP cache corruptions which will happen when multiple hosts claim to own the same IP address on the same subnet. Let's fix this problem now then as the kernel does have a way of handling this properly. Basically we'll ask the kernel not to advert the server's MAC address for the VIP on certain conditions using the arp_ignore and arp_announce sysctl.</p> <p>Add those lines on the sysctl.conf of your servers :</p> <p>net.ipv4.conf.all.arp_ignore = 3 net.ipv4.conf.all.arp_announce = 2</p> <p>Read more about those parameters for the detailed explanation of those values.</p>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#the-ipvs-synchronization-problem","title":"The IPVS synchronization problem","text":"<p>This is another problem arising from the fact that the load balancers are also acting as realservers. When keepalived starts, it spawns a synchronization process on the master and backup nodes so you load balancers' IPVS tables stay in sync. This is needed for a fully transparent fail over as it keeps track of the sessions' persistence so the clients don't get rebalanced when the master goes down. Well, this is the limitation of our setup : clients' HTTP sessions served by the master node will fail if he goes down. But note that the same will happen to the other nodes because we have to get rid of this synchronization to get our setup working. The reason is simple : IPVS table sync conflicts with the actual acceptance of the packet by our loopback set up VIP. Both mechanisms can't coexist together, so you'd better use this setup for stateless (API?) HTTP servers or if you're okay with this eventuality.</p>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#final-configuration","title":"Final configuration","text":"<p>to_master.sh</p>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#binbash","title":"!/bin/bash","text":"<p>ip addr del 192.168.0.254/32 dev lo ipvsadm --restore &lt; /tmp/keepalived.ipvs</p> <ol> <li>drop the VIP from the loopback interface (it will be setup by keepalived on the master interface)</li> <li>restore the IPVS configuration</li> </ol> <p>to_backup.sh</p>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#binbash_1","title":"!/bin/bash","text":"<p>ip addr add 192.168.0.254/32 scope host dev lo ipvsadm --save &gt; /tmp/keepalived.ipvs ipvsadm --clear</p> <ol> <li>add the VIP to the loopback interface, scope host</li> <li>keep a copy of the IPVS configuration, if we get to be master, we'll need it back</li> <li>drop the IPVS local config so it doesn't conflict with our own web serving</li> </ol>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#conclusion","title":"Conclusion","text":"<p>Even if it offers some serious benefits, remember the main limitation of this setup : if the master fails, all sessions of your web servers will be lost. So use it mostly for stateless stuff or if you're okay with this. My setup and explanations may have some glitches, feel free to correct me if I'm wrong somewhere.</p>","tags":["cluster","gentoo","ipvsadm","keepalived","load-balancing"]},{"location":"Tech%20Blog/2013/2013-06-02-roadtrip-3600/","title":"Roadtrip 3600","text":"<p>Deuxi\u00e8me roadtrip en Harley : la c\u00f4te d'Azur et Biarritz.</p> <p>Un vrai tour de France en un peu moins de deux semaines. 3600 kilom\u00e8tres de libert\u00e9 : Paris - Luberon - Gorges du Verdon - Grimaud (30 ans du HOG) - Marseille - Biarritz + les premi\u00e8res photos de vacances avec le GF670W, magique </p> <p> </p>","tags":["biarritz","harley","marseille","roadtrip"]},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/","title":"mongoDB : latest releases","text":"","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#mongodb-244","title":"mongodb-2.4.4","text":"<p>Just bumped it to portage and fixed an open bug along. This is yet another bugfix release which backports the switch to the Cyrus SASL2 library for sasl authentication (kerberos). Dependencies were adjusted so you no longer need libgsasl on your systems (remember to depclean).</p>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#highlights","title":"highlights","text":"<ul> <li>config upgrade fails if collection missing \"key\" field</li> <li>migrate to Cyrus SASL2 library for sasl authentication</li> <li>rollback files missing after rollback</li> </ul>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#pymongo-252","title":"pymongo-2.5.2","text":"<p>This one is important to note and I strongly encourage you to upgrade asap as it fixes an important security bug (CVE-2013-2132). I've almost dropped all other versions from tree anyway...</p>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#highlights-25x","title":"highlights 2.5.x","text":"<ul> <li>support GSSAPI (kerberos) authentication</li> <li>support for SSL certificate validation with hostname matching</li> <li>support for delegated and role based authentication</li> </ul>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#mongodb-25x-dev","title":"mongodb-2.5.x dev","text":"<p>What's cooking for the next 2.6 releases ? Let's take a quick look as of today.</p> <ul> <li>background indexing on secondaries (hell yes!)</li> <li>new implementation of external sort</li> <li>add support for building from source with particular C++11 compilers (will fix a gentoo bug reported quite a long time ago)</li> <li>mongod automatically continues in progress index builds following restart</li> </ul>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-06-13-vertigo/","title":"Vertigo","text":"<p>GF670W</p>","tags":["gf670w","urbex-2"]},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/","title":"HP Moonshot","text":"<p>A few months ago I had the chance to get to know about one of the upcoming HP innovation in the server architecture : the Moonshot project. Now that it is public, I thought I'd take some time to talk about it as I'm convinced this is something big enough to change the way we see datacenter infrastructures and servers in general. I'll do my best to keep it short and understandable so if you want deeper technical insights, fell free to ask or search around.</p>","tags":["hp","moonshot"]},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/#the-nowadays-servers","title":"The nowadays servers","text":"<p>As a reminder, that's what a standard server looks like today :</p> <p></p> <p>We call them pizza boxes for their flat and tasty aspect.</p> <p>Then in datacenters we put them in enclosures we call racks which look like this :</p> <p></p> <p>Now the basic stuff to understand and keep in mind is that those racks can typically hold 42 standard servers like the one above. Datacenters are just big hangars where you store and cool hundreds/thousands of those racks.</p>","tags":["hp","moonshot"]},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/#the-moonshot-project","title":"The Moonshot project","text":"<p>The more processing power you need, the more racks you need, the more datacenters you need. Think of facebook or google and their enormous amount of servers/racks/datacenters around the world. Every new device (PC/tablet/smartphone) activated is a new client to an always-growing infrastructure of those powerhouses.</p> <p>Basically, there's a limit in the number of full datacenters you can build and operate eventually (not to mention powering them up) but there's worse : the new devices/clients growth is higher than our datacenter building/powering capabilities.</p> <p>The Moonshot project is one of HP's response to this challenge : permit businesses to accommodate and serve this rapidly growing demand of devices/clients without the datacenter model collapsing. Their method ? Invent a new server architecture from scratch.</p>","tags":["hp","moonshot"]},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/#the-cartridges-are-back","title":"The cartridges are back !","text":"<p>No your Master System is still out of date... But HP's approach to getting more servers in less space while consuming way less power resides in turning the pizza-box above into a cartridge which looks like this :</p> <p></p> <p>No black magic involved : you can now store 45 servers in 4,3 units of space. Based on their calculation, if you want the same computing power as you would have with standard pizza-boxes you'd need only one full rack of those new servers versus 4 to 6 racks filled with standard ones (depending on their config). Overall gain factors are huge :</p> <ul> <li>space divided by 4 to 6</li> <li>energy divided by 6 to 2</li> <li>cabling divided by 26 to 18</li> <li>not to mention the time saved by technicians to put everything up</li> </ul> <p>That's what the beast looks like :</p> <p></p> <p>Of course, they have integrated redundant switches and all the flavors of modern enclosures.</p>","tags":["hp","moonshot"]},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/#the-right-cartridge-at-the-right-place","title":"The right cartridge at the right place","text":"<p>Over the year, HP will launch a series of cartridges with their own specifications (RAM/HDD/CPU) which should be used to meet specific needs and designs. They can also accommodate your needs by designing your own server-cartridges if you're in a hurry of course.</p> <p>As a DevOps, I love this idea of the hardware being designed and used upon your software's architecture because that's closer to real efficiency and will lead both developers and IT architects to distributed and massively scaling designs.</p> <p>I'll conclude with the last thing you need to understand about this technology :\u00a0it does not fit everyone's need.\u00a0Moonshot will not take over the world and replace every server around, instead it should be used as a hardware matching a real software design.</p>","tags":["hp","moonshot"]},{"location":"Tech%20Blog/2013/2013-06-20-py3status-v0-12/","title":"py3status v0.12","text":"<p>I'm glad to announce a new release of py3status\u00a0! I would like to thank @drahier\u00a0for reporting an issue he found after suspending his computer. I took the opportunity to add a feature which will be helpful at work since we now have a local package installing some modules we share between colleagues (thx to @lujeni).</p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-06-20-py3status-v0-12/#changelog","title":"changelog","text":"<ul> <li>bugfix : don't hang horribly when resuming from a suspend (was caused by an IOError exception which could occur when reading/writing to a suspending system).</li> <li>feature : allow multiple -i include_path options to be passed\u00a0and handle all the modules thus found.</li> <li>feature : do not try to execute private and special methods on user-written Py3status' classes.</li> </ul>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-06-24-py3status-v0-13/","title":"py3status v0.13","text":"<p>Yep, a quick bump of py3status to fix a bug reported by @lathan using python3. The private and special methods detection didn't work on python3 because the class methods are reported differently from python2.</p> <p>A special thanks to @bloodred and @drahier too for debugging, testing and proposing some solutions to this problem. First time I see multiple members of what I could humbly call the py3status community working together, it's very nice of you guys !</p>","tags":["py3status","python"]},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/","title":"mongoDB v2.4.5 & rabbitMQ v3.1.3","text":"<p>Quick catch-up on\u00a0recent\u00a0bumps I've made for rabbitMQ and mongoDB before posting about EuroPython2013 !</p>","tags":["gentoo","mongodb","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#mongodb-v245","title":"mongoDB v2.4.5","text":"<p>The 2.4 branch is definitely the most bugged one I've seen so far. We waited until 2.4.4 at work before migrating but this was not enough and we got hit for a few weeks by a bug which finally got fixed in 2.4.5. Also this release fixes two security bugs so you're strongly advised to upgrade.</p>","tags":["gentoo","mongodb","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#highlights","title":"highlights","text":"<ul> <li>CVE-2013-4650 - Improperly grant user system privileges on databases other than \u201clocal\u201d.</li> <li>CVE-2013-4650 - Remotely triggered segmentation fault in Javascript engine.</li> <li>config server performance improvements</li> <li>improve initial sync resilience to network failure (that's the one)</li> </ul>","tags":["gentoo","mongodb","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#flask-pymongo-v030","title":"flask-pymongo v0.3.0","text":"<p>@dcrosta finally had the time to take care of my pull request for\u00a0flask-pymongo. We now rely on pymongo's MongoClient parameters' validation instead of implementing them again on flask-pymongo and added connect and socket timeout options.</p>","tags":["gentoo","mongodb","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#rabbitmq-v313","title":"rabbitMQ v3.1.3","text":"<p>A roll-up bugfix release mainly.</p>","tags":["gentoo","mongodb","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#highlights_1","title":"highlights","text":"<ul> <li>fix startup failure when using SSL with Erlang/OTP R16B01</li> <li>fix queue crash requeuing in-memory messages (since 2.7.0)</li> <li>fix leak affecting HA/mirrored queues (since 3.0.0)</li> <li>fix bug that lead to incorrect reporting of accumulated stats (since 3.1.2)</li> </ul>","tags":["gentoo","mongodb","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-07-11-paris-siege-du-pcf/","title":"Paris : si\u00e8ge du PCF","text":"<p>GF670W</p>","tags":["gf670w"]},{"location":"Tech%20Blog/2013/2013-08-02-rsyslog-v7-4-3-released/","title":"rsyslog : v7.4.3 released","text":"<p>It's been more than 3 months since the last version bump of rsyslog, I'm sorry about that (special kudos to @Opportunist for his patience). Since June 6th, we have a new stable 7.4 branch of rsyslog containing all the bugfixes and improvements made in the 7.3 dev branch.</p> <p>So here comes the catch-up to current v7.4.3. Please note that as upstream do not support older branches, I will soon remove them from portage and close related bugs.</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2013/2013-08-02-rsyslog-v7-4-3-released/#highlights","title":"highlights","text":"<ul> <li>tons of bugfixes I won't bother to list</li> <li>imjournal: add ratelimiting capability</li> <li>max number of templates for plugin use has been increased to five</li> <li>added support for encrypting log files</li> <li>omhiredis: added support for redis pipeline support</li> </ul>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2013/2013-08-13-rabbitmq-v3-1-4-released/","title":"rabbitMQ : v3.1.4 released","text":"<p>Quick post for a quick bugfix release of the rabbitMQ server. Please note that I dropped the remaining 3.0.4 version in tree while doing this bump.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-08-13-rabbitmq-v3-1-4-released/#highlights","title":"highlights","text":"<ul> <li>security fix : ensure DLX declaration checks for publish permission (since 2.8.0)</li> <li>security fix :\u00a0update to a later version of Mochiweb that fixes a directory traversal vulnerability allowing arbitrary file access on Windows (since 2.1.0)</li> <li>fix resource leak with mirrored queues when whole clusters stop (since 3.0.0)</li> <li>fix queue crash in mirrored queue handling of messages during promotion (since 2.6.0)</li> <li>fix mirrored queue sync failure in the presence of un-acked messages not at the head of the queue (since 3.1.0)</li> <li>allow hipe compilation on Erlang R16B01</li> <li>make `rabbitmqctl join_cluster' idempotent (since 3.0.0)</li> <li>improve `rabbitmqctl cluster_status' handling of partition info when cluster nodes are in the process of stopping (since 3.1.0)</li> </ul>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-08-16-rabbitmq-v3-1-5-released/","title":"rabbitMQ : v3.1.5 released","text":"<p>Looks like rabbitMQ upstream likes to bump their stuff right after I catch-up with my bumps :) You are strongly advised to upgrade to this version since it fixes a quite important bug introduced by 3.1.4.</p>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-08-16-rabbitmq-v3-1-5-released/#highlights","title":"highlights","text":"<ul> <li>fix crash in the delegate mechanism leading to various crashes, and intra-cluster incompatibility between RabbitMQ 3.1.4 and other members of the 3.1.x series (since 3.1.4)</li> <li>prevent (harmless) errors being logged when pausing in pause_minority mode (since 3.1.0)</li> </ul>","tags":["gentoo","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/","title":"mongoDB v2.4.6 & pymongo v2.6","text":"","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/#mongodb-246","title":"mongodb-2.4.6","text":"<p>The folks at 10gen discovered a severe bug\u00a0in the mongoDB chunk migration process on sharded environments.</p> <p>Basically, depending of the size of your documents, there was a chance that some get lost during data migration ! Relax tho, this case affected only the chunks containing documents which size are in the range of 16,776,185 and 16,777,216 bytes (inclusive) so this means that if you don't have quite big documents in your cluster, you have not been affected by this bug.</p> <p>Still, as a maintainer and production user of mongoDB, this is not the kind of news I like to hear especially when thinking of you Gentoo users. On top of this, I had the bad surprise to experience again the stale replication bug that was supposed to be fixed on 2.4.5 on my production cluster.</p> <p>So I decided this was time for a major cleanup of the mongoDB ebuilds in portage to make sure we're not shipping known broken versions of mongoDB to you guys. I thus :</p> <ul> <li>dropped all obsolete &lt;mongodb-2.2 ebuilds (I warned about this quite some time ago now)</li> <li>dropped the known bugged versions of mongodb (&lt;2.2.6 and &lt;2.4.6)</li> <li>cleaned up all obsolete files in $FILESDIR</li> <li>added, on the v2.4.6 ebuild, the embedded-v8\u00a0USE flag as a user convenience\u00a0so you can now have packages requiring v8-3.19 and mongodb installed on your machine. I added an explicit warning about this as this is not the way to go on normal usage as this is against Gentoo policy.</li> </ul> <p>*mongodb-2.4.6 (21 Aug 2013) *mongodb-2.2.6 (21 Aug 2013)</p> <p>21 Aug 2013; Ultrabug ultrabug@gentoo.org -mongodb-2.0.7-r1.ebuild,   -mongodb-2.0.7-r2.ebuild, -mongodb-2.0.8-r1.ebuild, -mongodb-2.0.8-r2.ebuild,   -mongodb-2.2.0-r1.ebuild, -mongodb-2.2.0-r2.ebuild, -mongodb-2.2.4.ebuild,   +mongodb-2.2.6.ebuild, -mongodb-2.4.5.ebuild, -mongodb-2.4.6_rc1.ebuild,   +mongodb-2.4.6.ebuild, -files/mongodb-1.8.5-fix-smokepy.patch,   -files/mongodb-1.8-fix-scons.patch, -files/mongodb-2.2-fix-scons.patch,   -files/mongodb-2.2-fix-sconscript.patch,   -files/mongodb-2.4.4-fix-sharedclient.patch, -files/mongodb.initd,   -files/mongodb-linux3.patch, -files/mongos.initd, metadata.xml:   version bump, add embedded-v8 USE, drop critically bugged versions, drop   obsolete versions, filesdir cleanup</p> <p>I understand some people may still need some of those ebuilds so if that's the case, just shout and I'll gladly add them to my overlay so you can still use them easily.</p>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/#other-highlights","title":"other highlights","text":"<ul> <li>Improved replication robustness in presence of high network latency</li> <li>Resolved replica set initial sync issue on certain virtualized platforms</li> <li>Resolved sharding migration issue that produced excessive small chunks</li> <li>Resolved C++ client shutdown issues</li> </ul>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/#pymongo-26","title":"pymongo-2.6","text":"<p>This one is quite interesting as it brings both new and improved features as well as some bug fixes. Also note that they fixed some gevent compatibility stuff.</p>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/#highlights-explanations","title":"highlights / explanations","text":"<ul> <li>The max_pool_size\u00a0option actually means what it says now. Pymongo will open at most this number of sockets to your servers. Do remember that if you share a connection between threads, then your (max_pool_size+1) thread will wait for a socket to be freed before being able to process your command.</li> <li>waitQueueMultiple and\u00a0waitQueueTimeoutMS options will help you define how much and how long you want a process to wait for a socket to be available before raising an exception.</li> <li>Pymongo automatically splits batch inserts into 48MB chunks so you don't have to worry about pushing a huge list of documents for insertion.</li> <li>Support for aggregation cursors (for use with dev version 2.5.1, not used on production now)</li> <li>Support for exhaust cursors.\u00a0When you queried a large amount of data, the client had to ask the server for each batch of results. An exhaust cursor will instead stream batches to the client as quick as possible. This make pulling large sets of data faster and more reliable than before !</li> </ul> <p>You can see the full extend of this bump on the pymongo Jira.</p>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-08-30-py3status-v1-0/","title":"py3status v1.0","text":"<p>I'm glad to announce the release of py3status v1.0 !</p> <p>This version features a lot of stuff that I wanted to add to py3status for a long time and which I had in a small todo floating around. After the recent i3wm 4.6\u00a0release, I found it the perfect time to implement them while benefiting from the new click event support from i3bar.</p> <p>The idea of allowing py3status users to have their modules respond to clicks got my head fuzzing and I started slowly to implement my whole todo while adding some great new features thanks to the enhanced i3bar protocol.</p> <p>I ended up rewriting (again) completely (and slowly) py3status :) But it's for its own good, and yours hopefully. I strongly encourage you to have a look at the empty_class.py and pomodoro.py examples as they showcase the new on_click system.</p> <p>You can already benefit from this bump without modifying your modules thanks to the default middle click event which forces a refresh of the module's method you click on (handy isn't it?).</p>","tags":["gentoo","portage","py3status","python","release"]},{"location":"Tech%20Blog/2013/2013-08-30-py3status-v1-0/#changelog","title":"changelog","text":"<ul> <li>support for i3bar click_events, they're dispatched to user-written py3status classes based on their name/instance</li> <li>add support for on_click methods in user-written modules to handle i3bar click_events (see the pomodoro example)</li> <li>default is to clear the method's cache (force a refresh) if you middle click (button 2) on a method's output and the module does not support click_events</li> <li>rewrite pomodoro example to showcase the on_click usage</li> <li>use i3-nagbar to display warnings/errors to the user and also log them to syslog</li> <li>new user-written module output ordering mechanism is more intuitive as it uses strictly numeric then alphabetical sorting</li> <li>use select/poll() to implement a non-blocking I/O reading mechanism on threads</li> <li>new Events thread is responsible for reading i3bar JSONs and dispatching them to the correct user module (click_events)</li> <li>each user-written module is started and executed in its own thread</li> <li>remove the pointless -d option</li> <li>add a --debug option to be verbose in syslog (useful for debugging your modules)</li> <li>add a real CHANGELOG</li> <li>add a proper LICENSE file</li> <li>make sure all examples are PEP8 compatible</li> <li>update the empty_class example to explain on_click and kill usage</li> </ul> <p>Note for Gentoo users : starting with this release, py3status is now available in portage so you don't need my overlay anymore.</p>","tags":["gentoo","portage","py3status","python","release"]},{"location":"Tech%20Blog/2013/2013-09-03-pacemaker-v1-1-10-corosync-v2-3-1/","title":"pacemaker v1.1.10 & corosync v2.3.1","text":"<p>More than 5 months since the last bump of pacemaker. I'm glad that @beekhof did release the final pacemaker-1.1.10 and that the officially stable corosync got bumped to 2.3.1.</p> <p>The changelogs are quite heavy so I won't go into details about them but they both have quite a nice bunch of bugfixes and compatibility features. That's why I'm hoping we should soon be able to fix bug #429416 and drop corosync hard mask. Hopefully some users such as\u00a0@pvsa will give us some valuable feedback which will allow us to do it smoothly.</p>","tags":["cluster","gentoo","pacemaker","portage","release"]},{"location":"Tech%20Blog/2013/2013-09-03-pacemaker-v1-1-10-corosync-v2-3-1/#changelog","title":"changelog","text":"<ul> <li>pacemaker-1.1.10</li> <li>corosync-2.3.1</li> </ul>","tags":["cluster","gentoo","pacemaker","portage","release"]},{"location":"Tech%20Blog/2013/2013-09-10-rsyslog-v7-4-4-released/","title":"rsyslog : v7.4.4 released","text":"<p>Contribution. This must be what I love the most and one of the more rewarding thing in the Open Source community.</p> <p>I always found it more natural with Gentoo as you're really close to the source code so we (devs and users) can see and propose fixes natively to upstream. I known some devs of cool upstreams (mongoDB) do use Gentoo on their developing box because of this close-to-source philosophy.</p> <p>I don't know if Rainer does, but I'm glad our contribution (thx to @hwoarang) has been merged to the newer rsyslog. I'm glad to say that we now need no patch at all to build rsyslog on Gentoo !</p> <p>Long time warning had been issued, I took the opportunity of this bump to\u00a0clean and drop older and unsupported versions of rsyslog. The only stable branch remaining is v7 from now on. I also made a step towards systemd integration thanks to @slyfox.</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2013/2013-09-10-rsyslog-v7-4-4-released/#highlights","title":"highlights","text":"<ul> <li>make rsyslog use the new json-c pkgconfig file if available. Thanks to the Gentoo team for the patches.</li> <li>bugfix: imfile parameter \u201cpersistStateInterval\u201d was unusable due to a case typo in imfile; work-around was to use legacy config</li> <li>bugfix: slightly malformed SMTP handling in ommail</li> <li>bugfix: segfault in omprog if no template was provided (now dflt is used)</li> <li>bugfix: segfault in ompipe if no template was provided (now dflt is used)</li> <li>bugfix: segfault in omsnmp if no template was provided (now dflt is used)</li> <li>bugfix: some omsnmp optional config params were flagged as mandatory</li> </ul>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2013/2013-10-15-wonderlands-roadtrip/","title":"Wonderlands roadtrip","text":"<p>A part of me is still there, in the wild.</p> <p></p> <p>Our USA West Coast and National Parks roadtrip was amazing, surely the most stunning we ever had so far with such a strong feeling of freedom. I'll give more details about it along with the next photo posts.</p>","tags":["roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/","title":"Latest cluster releases","text":"<p>Now that I'm back I've bumped some of the sys-cluster packages. Users of keepalived will be interested in this since it was more than a year that upstream released a version.</p>","tags":["cluster","gentoo","keepalived","portage","release"]},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/#keepalived-128","title":"keepalived-1.2.8","text":"<p>This is a big and long awaited one. It features major enhancements, features and bug fixes. The changelog is pretty huge but here are some quick points which I particularly liked (biased view warning) :</p> <ul> <li>Revisited the whole code to use posix declaration style</li> <li>Boon Ang fixed comparison of primary IP addresses. If a router in the master state receives an advertisement with priority equal to the local priority, it must also compare the primary IP addresses (RFC 3768, section 6.4.3). The code to handle this was comparing two IP addresses with different byte-ordering, resulting in multiple routers in the master state. This patches resolves the problem by converting the local primary IP address to network byte order for the comparison.</li> <li>Henrique Mecking fixed memory leak in libipvs</li> <li>Willy Tarreau and Ryan O'Hara add the ability to use VRRP over unicast. Unicast IP addresses may be specified for each VRRP instance with the 'unicast_peer' configuration keyword. When a VRRP instance has one or more unicast IP address defined, VRRP advertisements will be sent to each of those addresses. Unicast IP addresses may be either IPv4 or IPv6. If you are planing to use this option, ensure every ip addresses present in unicast_peer configuration block do not belong to the same router/box. Otherwise it will generate duplicate packet at reception point.</li> </ul>","tags":["cluster","gentoo","keepalived","portage","release"]},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/#crmsh-126","title":"crmsh-1.2.6","text":"<p>Many bug fixes with better performances for this release. This is quite impressive, good work upstream !</p>","tags":["cluster","gentoo","keepalived","portage","release"]},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/#corosync-232","title":"corosync-2.3.2","text":"<p>This one is about supporting live config reloading and fix high CPU usage when idle. See the release notes.</p>","tags":["cluster","gentoo","keepalived","portage","release"]},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/#soon-to-come","title":"Soon to come","text":"<p>The resource-agents v3.9.6 and cluster-glue v1.0.12 should be released by their upstream pretty soon, stay tuned.</p>","tags":["cluster","gentoo","keepalived","portage","release"]},{"location":"Tech%20Blog/2013/2013-10-20-usa-roadtrip-facts-start/","title":"USA roadtrip facts & start","text":"","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-10-20-usa-roadtrip-facts-start/#the-plan","title":"The plan","text":"<p>We started our one month roadtrip in Los Angeles where we stayed 3 days to adjust to the -9h time zone shifting.\u00a0We had planned and rented a SUV for the first 15 days to tour the national parks (just before the shutdown, lucky us) then we rented a motorbike for the last 15 days of our trip to ride the West coast. Needless to say it was an amazing trip, the photos I'll post will show it by themselves.</p> <p>Still, it's always fun and quite stunning to look back at the odometers of the two vehicles we used to see how much miles/km we drove :</p> <ul> <li>In the SUV, touring the national parks : 4269 miles / 6870 km</li> <li>With the Harley-Davidson Street Glide, riding the West coast : 2175 miles / 3500 km</li> </ul> <p>That's a nice total of 6444 miles / 10 370 km ! It's more than the trip from Paris to LA ;)</p>","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-10-20-usa-roadtrip-facts-start/#the-start","title":"The start","text":"<p>We headed to the Death Valley but took a detour to \"visit\" an abandoned waterpark in the middle of the desert !</p> <p></p> <p></p>","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-10-22-mongodb-v2-4-7-pymongo-v2-6-3/","title":"mongoDB v2.4.7 & pymongo v2.6.3","text":"<p>First of all, I'd like to point out a quite big change in the Gentoo mongodb package. The Chromium team responsible for the v8 package decided to stop its maintenance as it was too much trouble to be used efficiently as a shared library (mainly due to upstream's breakage behavior). Even tho I don't like bundled libraries on sources, I understand my fellow developers point of view.</p> <p>I've thus been asked and did switch the mongodb ebuild to use the bundled v8 library. This means that mongodb has no more v8 packaging dependency now. The mongodb v2.2.x\u00a0users are advised that since upstream does not bundle the v8 lib in their source, I dropped the v8 USE flag and support altogether on this version (it's not officially supported anyway) !</p> <p>This being said, I'll drop the old ebuilds from tree on the next releases iterations.</p>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-10-22-mongodb-v2-4-7-pymongo-v2-6-3/#mongodb-247","title":"mongodb-2.4.7","text":"<p>Yet another bugfix release on this unfamous 2.4.x series :</p> <ul> <li>Fixed over-aggressive caching of V8 Isolates</li> <li>Removed extraneous initial count during mapReduce</li> <li>Cache results of dbhash command</li> <li>Fixed memory leak in aggregation</li> </ul>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-10-22-mongodb-v2-4-7-pymongo-v2-6-3/#pymongo-263","title":"pymongo-2.6.3","text":"<p>BSON parser hardening and fixes in the connection pool mechanism. More info here.</p>","tags":["gentoo","mongodb","nosql","portage","pymongo","release"]},{"location":"Tech%20Blog/2013/2013-10-25-drop-down-terminal-in-i3/","title":"Drop-down terminal in i3","text":"<p>One of the reasons I switched from KDE to i3wm is that I love and need terminals. In my field of work you happen to spawn dozens of them and you always end up running out of space / workspaces.</p> <p>Yakuake has been a real ally to me for years as I intensively use a drop-down terminal for sporadic usages. It is hard to match Yakuake's efficiency and ability to split terminals but I couldn't stand all those KDE dependencies anymore; I had to find a great drop-down terminal solution in i3.</p> <p>So I started looking at other drop-down terminals such as Terra and Guake but they didn't fit my low dependencies and features list requirements.</p>","tags":["drop-down","gentoo","i3","i3wm","terminal"]},{"location":"Tech%20Blog/2013/2013-10-25-drop-down-terminal-in-i3/#drop-down-terminator","title":"Drop-down Terminator","text":"<p>My current solution is to take advantage of the floating mode of i3, use it with my beloved terminator\u00a0et voil\u00e0 ! Nothing more to install, no extra dependency, using the same shortcuts of my main $TERMINAL and all of its features :)</p> <p>The idea is simple, we'll create a special profile in terminator and have it spawned in floating mode upon i3 start. This profile must cover the following drop-down behaviors :</p> <ul> <li>respond to a configurable show/hide key binding</li> <li>present a drop-down terminal at the center of the screen</li> <li>the interface should be dead simple and efficient and support splitting</li> </ul> <p>Just edit your terminator configuration in ~/.config/terminator/conf and add :</p> <p>[keybindings]   hide_window = F1</p> <p>Then add the dropdown profile under the [profiles] section :</p> <p>[profiles] [[dropdown]]   exit_action = close   scrollback_lines = 10000   background_image = None   scroll_on_output = False   show_titlebar = False</p> <p>That's my minimal config, you can add your own stuff to it as well. Now we only need to configure i3 to spawn this profile at login and have it in floating mode.</p> <p>Modify your i3 config file, usually ~/.i3/config :</p> <p>exec terminator -c dropdown -p dropdown -T \"Le Terminator\" -H --geometry=1550x800</p> <p>for_window [class=\"Terminator\" instance=\"dropdown\"] floating enable</p> <p>That's as simple as this.</p> <p>EDIT: as per Joe's comment, you can also configure i3 to place your floating Terminator window wherever you want (in his case, top off the screen). This still goes into your i3 config from above :</p> <p>for_window [class=\u201dTerminator\u201d instance=\u201ddropdown\u201d] floating enable move absolute position 0 0</p> <p>There's still one limitation which I didn't come across yet :</p> <ul> <li>Unlike Yakuake, our drop-down terminator has a fixed geometry which you must set in the i3 config above and does not support percentage values. So if you have multiple screens of different resolutions it won't adapt on them based on the screen you want to show your drop-down terminator.</li> </ul> <p>So long, Yakuake !</p>","tags":["drop-down","gentoo","i3","i3wm","terminal"]},{"location":"Tech%20Blog/2013/2013-10-27-death-valley/","title":"Death Valley","text":"<p>The Death Valley was our first shock of wilderness and immensity as we were welcomed by strong and sandy winds. We slept in our car at an almost empty campground inside the park. Spending the evening and waking up alone in the desert is something very special, especially there.</p> <p> </p>","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-10-29-bryce-canyon/","title":"Bryce Canyon","text":"<p>After the Death Valley, we headed North to reach Bryce Canyon. On the way up, we did take a quick detour to cruise Las Vegas by car but didn't stop there.</p> <p>We had a pleasant night in a very old fashioned motel near Bryce Canyon where the tenant asked me if I really was French because \"the French are usually blond and don't have such dark hair as you mister\".</p> <p>I'm willing to bet a great deal on the fact that the vast majority of us are brown/dark haired... Mister blond @Lujeni is NOT representative of us French people ! :)</p> <p> </p>","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-11-03-north-rim-grand-canyon-the-navajo-indians/","title":"North rim Grand Canyon & the Navajo Indians","text":"<p>We then headed South towards the North rim of Grand Canyon, unfortunately the nights were already cold out there so the rangers did advise against sleeping in the car. We thus\u00a0visited Point Imperial and stayed for the sunset. This was our first encounter with the Grand Canyon and it was amazing.</p> <p></p> <p>It was dark when we got out of there, we had to drive very carefully to avoid the numerous deers along the road. We had planned to sleep in Page but we hit a detour due to road work and had to take a huge detour which took us to Tuba City.</p> <p>I could not stand driving one more mile but the only three hotels of the whole area were full. One person from the front desk of a hotel advised us to go to the nearby Greyhills Inn which remains one of the most exotic experience we had in Indian territory. We slept in a room in the Tuba City High School as they do rent rooms to outsiders ! We were welcomed nicely in this old fashioned place operated by the local Navajo people.</p> <p>We visited Page and its (too) touristic area the next day along with the famous (and so crowded) Antelope Canyon and Horseshoe Bend.</p> <p> </p>","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-11-07-monument-valley/","title":"Monument Valley","text":"<p>Forget about Marlboro, this Navajo nation area is way more than that. To be honest, you can see the money made from the tax you have to pay to enter the actual park is invested back in the community but I guess there's more and some drawbacks to it.</p> <p>Meet the first 10 minutes of rain we experienced during a whole month ! Actually, it was a localized storm and it happened at the perfect moment where we had the most beautiful view. We felt this moment was quite unique and we were excited and glad to experience it. I guess you won't argue with me after looking at those pictures.</p> <p></p> <p></p> <p></p> <p></p>","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-11-09-mongodb-v2-4-8-rabbitmq-v3-2-1-rsyslog-v-7-4-6/","title":"mongoDB v2.4.8, rabbitMQ v3.2.1, rsyslog v7.4.6","text":"","tags":["gentoo","mongodb","nosql","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-11-09-mongodb-v2-4-8-rabbitmq-v3-2-1-rsyslog-v-7-4-6/#mongodb-248","title":"mongodb-2.4.8","text":"<p>You should consider this\u00a0important\u00a0update if you have a cluster running v2.4.7. It contains a fix for the config servers which can have them possibly disagree on chunks hashes and thus prevent mongos to start or balancing to happen. See this bug for more info.</p>","tags":["gentoo","mongodb","nosql","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-11-09-mongodb-v2-4-8-rabbitmq-v3-2-1-rsyslog-v-7-4-6/#rabbitmq-321","title":"rabbitMQ-3.2.1","text":"<p>The famous message queuing server got a nice bunch of bug fixes on a lot of its modules along with some interesting additions such as :</p> <ul> <li>support for federated queues</li> <li>report client authentication errors during connection establishment explicitly using connection.close</li> <li>inform clients when memory or disk alarms are set or cleared</li> <li>allow policies to target queues or exchanges or both</li> <li>offer greater control over threshold at which messages are paged to disk</li> <li>allow missing exchanges &amp; queues to be deleted and unbound without generating an AMQP error</li> <li>implement consumer priorities</li> </ul> <p>Full changelog here and here.</p>","tags":["gentoo","mongodb","nosql","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-11-09-mongodb-v2-4-8-rabbitmq-v3-2-1-rsyslog-v-7-4-6/#rsyslog-746","title":"rsyslog-7.4.6","text":"<p>This is a bug fix release, nothing too big about it as reported\u00a0by Thomas D (thanks again).</p> <p>Please note that\u00a0rsyslog-7.4.4 is being stabilized, mainly for security purposes.</p>","tags":["gentoo","mongodb","nosql","portage","rabbitmq","release"]},{"location":"Tech%20Blog/2013/2013-11-15-keepalived-v1-2-9/","title":"keepalived v1.2.9","text":"<p>Another release, 3 months after the mighty 1.2.8. It seems like upstream has awaken !</p>","tags":["gentoo","keepalived","portage","release"]},{"location":"Tech%20Blog/2013/2013-11-15-keepalived-v1-2-9/#highlights","title":"highlights","text":"<ul> <li>Jonas Johansson fixed VRRP sync group by sending prio 0 when entering FAULT state. This fix will send prio 0 (VRRP_PRIO_STOP) when the VRRP router transists from MASTER to FAULT state. This will make a sync group leave the MASTER state more quickly by notifying the backup router(s) instead of having them to wait for time out.</li> <li>Jonas Johansson fixed VRRP to honor preempt_delay setting on startup.</li> <li>Jonas Johansson extended VRRP code for faster sync group transition.</li> <li>Some nice bug fixes to unicast mode.</li> </ul> <p>Full changelog here !</p>","tags":["gentoo","keepalived","portage","release"]},{"location":"Tech%20Blog/2013/2013-11-27-rip-stabber/","title":"RIP stabber","text":"<p>Today we did shutdown the oldest Gentoo Linux server of our oldest production datacenter. It was running since April, 5th of year 2006 so that's a total of 2793 days of production level service as a stateful firewall. Its name was stabber, in reference of a vessel in the Eve Online MMORPG which I played a lot at the time.</p> <p>Our company has been running on Gentoo Linux since 2004 for its Linux platforms and I often hear and experience the astonishment of the other persons I speak to about this : \"Gentoo Linux in production, really ?\" or \"Wow you guys are a bunch of crazy hardcore Gurus\"...</p> <p>As if Gentoo Linux did not meet the production level requirements or the security level you expect from another major (usually not free) distribution and as if you had to master some major skills to have it done...</p> <p>7 years later, stabber is in my opinion a proof that all those assumptions are wrong.</p> <ul> <li>I was a junior sysadmin at the time I made this server, we didn't want to pay for having a proper firewall so we decided to make our own (that's what Gentoo is to me : simple things done right, no added sugar)</li> <li>The rolling updates of Gentoo did not brake our system and it evolved along our infrastructure</li> <li>The GLSA kept our server immune to security breaches over the years (thx to the Gentoo security team)</li> <li>This server/firewall passed the security tests of both Paypal and Ebay, this looks production level enough to me</li> </ul> <p>We did shutdown this server because it was a single point of failure on an old part of our architecture. Its role has been taken over by two fault tolerant servers/firewalls running... Gentoo Linux of course !</p> <p>First emerge.log entry</p> <p>Wed Apr  5 12:53:22 2006 &gt;&gt;&gt; sys-kernel/hardened-sources-2.6.14-r5</p> <p>Latest uname -a</p> <p>Linux stabber 2.6.16-hardened-r11 #1 SMP PREEMPT Wed Aug 30 15:51:49 CEST 2006 i686 Intel(R) Xeon(TM) CPU 3.20GHz GenuineIntel GNU/Linux</p> <p>Latest commands</p> <p>stabber ~ # echo \"je taime\" &gt;&gt; last.letter stabber ~ # shutdown now -h</p> <p>Dear fellow Gentoo Linux developers, your work makes all this possible, thank you !</p>","tags":["gentoo"]},{"location":"Tech%20Blog/2013/2013-12-05-arcosanti/","title":"Arcosanti","text":"<p>On the route down from Flagstaff to Phoenix, we stopped and stayed in\u00a0Arcosanti. This experimental city is a unique place as it was imagined by an Italian architect who began its construction in the 70s with volunteers from all around the world. Today its construction is still ongoing and people come there to contribute for a certain period of time before going back home (we met a French lady from Britany there, this was weird).</p> <p>The cool thing about this community is that you can book a room there to stay and live among them for some time. We had dinner and breakfast with them, everything was very cool and very tasty ! We didn't really know what to expect when arriving there and were amazed by the place and its incredible athmosphere.</p> <p>We had the feeling of being in another world, far from everything we're used to. This is the kind of place where you can feel far from everything but close from the earth, a unique experience really. I even had the chance to take a dive in the swimming pool overlooking the surrounding desert.</p> <p>I couldn't help but feel strange about how the architecture of this place made me feel like I was walking in the video game Riven !</p> <p></p> <p></p> <p></p> <p></p>","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/","title":"py3status v1.1","text":"<p>I'm glad to announce the v1.1 release of py3status, roughly 6 months after v1.0 which features some nice and often contributed bug fixes and some exciting new features !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#fixes-enhancements","title":"fixes / enhancements","text":"<ul> <li>allow float on refresh interval option</li> <li>fix respect user's locale for time transformation</li> <li>fix i3status time adjustment when format does not contain the necessary items to get an exact datetime</li> <li>fix delay on py3status start waiting for i3status, this caused a useless first refresh delay of py3status of i3status interval seconds</li> <li>fix first click event opening line detection</li> <li>redirect stdout and stderr to null to suppress modules outputs, this prevents i3bar from frezzing when a user module prints something to stdout or stderr, more info</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#features","title":"features","text":"<p>Thanks to py3status, you can now\u00a0take action on clicks made on your i3status modules !</p> <ul> <li>the new i3bar_click_events.py module allows you to implement some actions when clicking on your i3bar modules</li> <li>more info in the wiki and in the source code of the module</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#new-modules","title":"new modules","text":"<ul> <li>new generic click event handler using the special module file named i3bar_click_events.py\u00a0which will be forwarded any orphan click event for action</li> <li>new example module displaying Yahoo Weather forcast</li> <li>new example whoami displaying the currently logged in user, inspired by user request on i3 FAQ</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#contributors","title":"contributors","text":"<p>Thanks a lot for their issues ranging from #15 to #20 with feedback, proposals and pull requests !</p> <ul> <li>@alethiophile</li> <li>@Edholm</li> <li>@ifschleife</li> <li>@lathan</li> <li>@patrickshan</li> <li>@ShadowPrince</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#also","title":"also...","text":"<p>Feel free to join the #py3status IRC channel on FreeNode to get help or share your ideas !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2013/2013-12-24-rsyslog-v7-4-7/","title":"rsyslog v7.4.7","text":"<p>I've been slacking on this package for some months but slowly got the chance to catch-up with the multiple bugs open. So today I'm glad to say to make this blog post not only for a version bump release but also for some nice added features and ebuild improvements fixing 6 bugs in a row.</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2013/2013-12-24-rsyslog-v7-4-7/#ebuild","title":"ebuild","text":"<ul> <li>added support for the mongoDB output template module thanks to Vadim Kuznetsov</li> <li>added support for sub-slot operators\u00a0for json-c and libgcrypt dependencies using EAPI5 thanks to Thomas D.</li> <li>libgcrypt is now a required dependency of rsyslog as I didn't want to add another USE flag for such a widely spread dependency</li> <li>I've also bumped net-libs/czmq with approval of @jlec which fixes dependencies when trying to build rsyslog with zeromq support, thanks to\u00a0Allen Parker for his report and valuable debugging</li> </ul>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2013/2013-12-24-rsyslog-v7-4-7/#rsyslog-v747","title":"rsyslog v7.4.7","text":"<p>This is a bugfix release, see the full changelog here.</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2014/2014-01-03-aircraft-boneyard/","title":"Aircraft Boneyard","text":"<p>After Arcosanti, we kept on going south into Arizona where we headed to Phoenix and stayed in Tucson for a few days. It was hot and sunny of course and we loved this part of Arizona.</p> <p>We went to see the US Army Aircraft Boneyard which is located a bit South-East of Tucson, in the middle of the desert. Imagine hundreds of planes parked in the sand, waiting to be dismantled !</p> <p> </p>","tags":["gf670w","roadtrip","usa"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/","title":"Tuning pacemaker for large clusters","text":"<p>We've been running quite a lot of production clusters using pacemaker/corosync for a while. Some of them are large, handling more than 200 resources across multiple nodes and we've exceeded some limits on pacemaker's CIB size.</p> <p>I thought I'd share how to tune your cluster to handle such a bunch of resources since there are some default limits on the IPC buffer size which can lead to problems when your resources (and thus CIB) grows too much.</p>","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#hitting-the-ipc-limit","title":"Hitting the IPC limit","text":"<p>When running a large cluster you may hit the following problem :</p> <p>error: crm_ipc_prepare: Could not compress the message into less than the configured ipc limit (51200 bytes).Set PCMK_ipc_buffer to a higher value (2071644 bytes suggested)</p>","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#evaluating-the-buffer-size","title":"Evaluating the buffer size","text":"<p>Have a look at the size of your current CIB :</p> <p># cibadmin -Ql &gt; cib.xml</p>","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#ls-l-cibxml","title":"ls -l cib.xml","text":"","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#bzip2-cibxml","title":"bzip2 cib.xml","text":"","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#ls-l-cibxmlbz2","title":"ls -l cib.xml.bz2","text":"<p>The CIB is compressed on the wire using bzip2 so you have to compare the compressed cib.xml.bz2 with the IPC default buffer size of\u00a051200 and you'll find the sufficient\u00a0PCMK_ipc_buffer value for you (take more just to be safe).</p>","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#setting-the-environment-variables","title":"Setting the environment variables","text":"<p>On Gentoo Linux, you'll have to create the /etc/env.d/90pacemaker\u00a0file containing :</p> <p>PCMK_ipc_type=shared-mem PCMK_ipc_buffer=2071644</p> <ul> <li>PCMK_ipc_buffer : you may need to increase this depending on your cluster size and needs</li> <li>PCMK_ipc_type\u00a0: the shared-mem one is the default now, other values are\u00a0socket|posix|sysv</li> </ul> <p>You will also need to set these env. vars in your\u00a0.bashrc so that the crm CLI doesn't break\u00a0:</p> <p>export PCMK_ipc_type=shared-mem export PCMK_ipc_buffer=2071644</p>","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#future","title":"Future","text":"<p>Finally, I wanted to let you know that the upcoming Pacemaker v1.1.11 should come with a feature which will allow the IPC layer to adjust the\u00a0PCMK_ipc_buffer automagically !</p> <p>Hopefully you shouldn't need this blog post anymore pretty soon :)</p>","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#edit-jan-16-2014","title":"EDIT, Jan 16 2014","text":"<p>Following this blog post, I had a very interesting comment from @beekhof (lead dev of pacemaker)</p> <p>beekhof&gt; Ultrabug: regarding large clusters, the cib in 1.1.12 will be O(2) faster than 1.1.11. Ultrabug&gt; beekhof: that's great news mate ! when is it scheduled to be released ? beekhof&gt; 30th of Feb</p>","tags":["clus","cluster","gentoo","pacemaker"]},{"location":"Tech%20Blog/2014/2014-01-12-py3status-v1-2/","title":"py3status v1.2","text":"<p>I'm glad to announce a new release of py3status with an exciting main new feature giving the ability to modify any of i3status' module output from any of your modules !</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-01-12-py3status-v1-2/#feature","title":"feature","text":"<ul> <li>you can now change i3status modules' output by altering the\u00a0i3status_output_json parameter received in any of your module !</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-01-12-py3status-v1-2/#changelog","title":"changelog","text":"<ul> <li>new module dpms.py allowing activation and deactivation of DPMS thx to\u00a0Andr\u00e9 Doser</li> <li>order i3status output updates to prevent it from overwriting any modification made on i3status json list by a user module, this avoids a possible user filter flapping on i3status modules</li> <li>fix delay on first execution of each module which could be equal to py3status interval time before being executed : your modules get executed and displayed immediately no matter py3status' interval</li> <li>the real i3status thread output json list is passed to all modules as the i3status_output_json parameter, this allows any user module to change any of the i3status output by simply altering the given json on the list, inspired thx to @drestebon on issue #23</li> <li>add validation for the position parameter</li> <li>add cpu usage info to sysdata script, by Patrick Shan</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-01-12-py3status-v1-2/#contributors","title":"contributors","text":"<p>Many thanks to all contributors for their work and inspiration.</p> <ul> <li>Patrick Shan, @patrickshan</li> <li>@drestebon</li> <li>Andr\u00e9 Doser, @tasse</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-01-17-uwsgi-v2-0/","title":"uWSGI v2.0","text":"<p>Yesterday was a big day for the famous application container uWSGI. We released the brand new version 2.0 LTS along with quite a huge bump of the ebuild, closing 6 bugs at once. I thought I'd give some input about the ebuild changes and some quick notes about uWSGI. Many thanks again to\u00a0@dev-zero !</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2014/2014-01-17-uwsgi-v2-0/#new-plugins-selection-uwsgi_plugins","title":"New plugins selection : UWSGI_PLUGINS","text":"<p>We introduced a new USE_EXPAND named UWSGI_PLUGINS\u00a0so that you can now select which plugins to build individually. This is a great step as it makes the compilation more clear and lets you fine tune your uWSGI installation.</p> <p>Along this work, we had to describe each plugin which was also quite a challenge. To my knownledge, this has not been done anywhere else so here it is. Please ping me if you have something to add or if we failed to describe a plugin correctly.</p> <p>Migration note :\u00a0You will need to change your package.use configuration to switch to using UWSGI_PLUGINS. As an example, where you had the USE flag spooler enabled you'll now need to use uwsgi_plugins_spooler.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2014/2014-01-17-uwsgi-v2-0/#uwsgi-v20-highlights","title":"uWSGI v2.0 highlights","text":"<p>These are my biased favorites, go check for more, it's huge !</p> <ul> <li>A brand new and uber fast caching framework</li> <li>The High-Availability oriented\u00a0Legion subsystem</li> <li>The long awaited WebSocket support</li> <li>Integrated Transformations mechanisms</li> <li>The new Metrics subsystem</li> <li>A Chunked input API</li> <li>SNI support, virtual hosting for SSL nodes</li> </ul>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2014/2014-01-31-keepalived-v1-2-11-glusterfs-v3-4-2/","title":"keepalived v1.2.11 & glusterfs v3.4.2","text":"<p>Quick post for two quick bumps related to clustering.</p>","tags":["cluster","gentoo","glusterfs","keepalived","portage","release"]},{"location":"Tech%20Blog/2014/2014-01-31-keepalived-v1-2-11-glusterfs-v3-4-2/#glusterfs-342","title":"glusterfs-3.4.2","text":"<ul> <li>quite a lot of bug fixes and improvements</li> <li>contains a backport for libgfapi support for integrating with NFS Ganesha</li> <li>nfs/mount3: fix crash in subdir resolution</li> </ul>","tags":["cluster","gentoo","glusterfs","keepalived","portage","release"]},{"location":"Tech%20Blog/2014/2014-01-31-keepalived-v1-2-11-glusterfs-v3-4-2/#keepalived-1211","title":"keepalived-1.2.11","text":"<ul> <li>autoconf: better libnl3 detection</li> <li>Fix memory allocation for MD5 digest</li> <li>Quite some nice memory leak fixes on different components</li> <li>vrrp: dont try to load ip_vs module when not needed</li> <li>Pim van den Berg work on libipvs-2.6 to sync with libipvs from ipvsadm 1.27</li> <li>vrrp: extend ip parser to support default and default6</li> <li>vrrp: fix/extend gratuitous ARP handling (multiple people reported issues where MASTER didn't recover properly after outage due to no gratuitous ARP sent)</li> <li>Multiple fixes to\u00a0genhash</li> <li>vrrp: fix vrrp socket sync while leaving FAULT state (old old bug here)</li> <li>Full changelog here</li> </ul>","tags":["cluster","gentoo","glusterfs","keepalived","portage","release"]},{"location":"Tech%20Blog/2014/2014-02-18-mongodb-v2-4-9v2-2-7-rabbitmq-v3-2-3/","title":"mongoDB v2.4.9/v2.2.7, rabbitMQ v3.2.3","text":"<p>Quick post about some recent bumps.</p>","tags":["gentoo","mongodb","nosql","portage","rabbitmq","releases"]},{"location":"Tech%20Blog/2014/2014-02-18-mongodb-v2-4-9v2-2-7-rabbitmq-v3-2-3/#mongodb-249-mongodb-227","title":"mongodb-2.4.9 &amp; mongodb-2.2.7","text":"<p>IMPORTANT\u00a0: These versions fix a mongos bug which could lead it to report a write as successful when it was not. This affects all versions of MongoDB prior to and including v2.4.8.</p> <ul> <li>v2.4.9 changelog</li> <li>v2.2.7 changelog</li> </ul> <p>Stay tuned on mongoDB, the next post will probably talk about the release of pymongo v2.7 which supports some neat futures from the upcoming mongoDB v2.6 series.</p>","tags":["gentoo","mongodb","nosql","portage","rabbitmq","releases"]},{"location":"Tech%20Blog/2014/2014-02-18-mongodb-v2-4-9v2-2-7-rabbitmq-v3-2-3/#rabbitmq-323","title":"rabbitMQ-3.2.3","text":"<p>I skipped a bump post when releasing the v3.2.2 so you should check out the v3.2.3 changelog as well if you're willing to know more about those bug fix releases.</p>","tags":["gentoo","mongodb","nosql","portage","rabbitmq","releases"]},{"location":"Tech%20Blog/2014/2014-02-23-py3status-v1-3/","title":"py3status v1.3","text":"<p>I'm glad to announce the release of py3status v1.3 which brings to life a feature request from @tasse and @ttyE0. Guys, I hope this one will please you !</p>","tags":["gentoo","i3wm","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-02-23-py3status-v1-3/#whats-new","title":"what's new ?","text":"<p>Along with a localization bug\u00a0fix thanks to @zetok\u00a0from Poland, the main new feature is that py3status now supports a\u00a0standalone mode\u00a0which you can use when you only want your own modules displayed in an i3bar !</p> <p>As usual, this release is already available for my fellow Gentoo Linux users and on pypi !</p> <p>Changelog is here and quick to get, enjoy !</p>","tags":["gentoo","i3wm","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-03-07-couchbase-on-gentoo-linux/","title":"Couchbase on Gentoo Linux","text":"<p>Back in 2010 when I was comparing different NoSQL solutions I came across CouchDB. Even tho I went for mongoDB in the end, it was still a nice and promising technology even more since the merge with the Membase guys in late 2012 which lead to the actual Couchbase.</p> <p>I won't go into the details of Couchbase itself since it's way covered all around the net but I wanted to let you guys know that I've packaged most of the couchbase ecosystem for Gentoo Linux :</p> <ul> <li>dev-db/couchbase-server-community-2.2.0 : the community server edition (bin)</li> <li>dev-libs/libcouchbase-2.2.0 : the C client library</li> <li>dev-python/couchbase-1.2.0 : the python client library</li> </ul> <p>Those packages are still only available on my overlay\u00a0(ultrabug on layman) since I'm not sure about the interest of other users in the community and I still need to make sure it's production ready enough.</p> <p>If you're interested in seeing this package in portage, please say so !</p> <p>I dedicate this packaging to @atorgfr :)</p>","tags":["couchbase","gentoo","nosql","portage"]},{"location":"Tech%20Blog/2014/2014-03-17-nightster-bw/","title":"Nightster B&W","text":"<p>This shot was taken some days ago on a short trip near Paris, it's a nice addition to the very few pictures I have of my bike !</p> <p></p> <p>click to see full resolution, grain is very nice on this Ilford 3200 ISO film</p>","tags":["harley","ilford","nikon-fm2"]},{"location":"Tech%20Blog/2014/2014-04-04-convert-special-characters-to-ascii-in-python/","title":"Convert special characters to ASCII in python","text":"<p>I came across a recurrent problem at work which was to convert special characters such as the French-Latin accentuated letter \"\u00e9\" to ASCII\u00a0\"e\" (this is called transliteration).</p> <p>I wanted to avoid having to use an external library such as Unidecode\u00a0(which is great obviously) so I ended up wandering around the unicodedata\u00a0built-in library. Before I had to get too deep in the matter I found this StackOverflow topic which gives an interesting method to do so and works fine for me.</p> <p>def strip_accents(s):     \"\"\"     Sanitarize the given unicode string and remove all special/localized     characters from it.</p> <pre><code>Category \"Mn\" stands for Nonspacing_Mark\n\"\"\"\ntry:\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\nexcept:\n    return s\n</code></pre> <p>PS : thanks to @Flameeyes for his good remark on wording</p>","tags":["python"]},{"location":"Tech%20Blog/2014/2014-04-08-mongodb-2-4-10-pymongo-2-7/","title":"mongoDB 2.4.10 & pymongo 2.7","text":"<p>I'm pleased to announce those latest mongoDB related bumps. The next version bump will be for the brand new mongoDB 2.6 for which I'll add some improvements to the Gentoo ebuild so stay tuned ;)</p>","tags":["gentoo","mongodb","nosql","portage","pymongo"]},{"location":"Tech%20Blog/2014/2014-04-08-mongodb-2-4-10-pymongo-2-7/#mongodb-2410","title":"mongodb-2.4.10","text":"<ul> <li>fixes some memory leaks</li> <li>start elections if more than one primary is detected</li> <li>fixes issues about indexes building and replication on secondaries</li> <li>chunk size is decreased to 255 KB (from 256 KB) to avoid overhead with usePowerOf2Sizes option</li> </ul> <p>All mongodb-2.4.10 changelog here.</p>","tags":["gentoo","mongodb","nosql","portage","pymongo"]},{"location":"Tech%20Blog/2014/2014-04-08-mongodb-2-4-10-pymongo-2-7/#pymongo-27","title":"pymongo-2.7","text":"<ul> <li>of course, the main feature is the mongoDB 2.6 support</li> <li>new bulk write API (I love it)</li> <li>much improved concurrency control for MongoClient</li> <li>support for GridFS queries</li> </ul> <p>All pymongo-2.7 changelog here.</p>","tags":["gentoo","mongodb","nosql","portage","pymongo"]},{"location":"Tech%20Blog/2014/2014-04-16-py3status-v1-4/","title":"py3status v1.4","text":"<p>I'm glad to announce the release of py3status-1.4\u00a0which I'd like to dedicate to @guiniol who provided valuable debugging (a whole Arch VM) to help me solve the problem he was facing (see changelog).</p> <p>I'm gathering wish lists an have some (I hope) cool ideas for the next v1.5 release, feel free to post your most adventurous dreams !</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-04-16-py3status-v1-4/#changelog","title":"changelog","text":"<ul> <li>new ordering mechanism with verbose logging on debug mode. fixes rare cases where the modules methods were not always loaded in the same order and caused inconsistent ordering between reloads. thx to @guiniol for reporting/debugging and @IotaSpencer and @tasse for testing.</li> <li>debug: dont catch print() on debug mode</li> <li>debug: add position requested by modules</li> <li>Add new module ns_checker.py, by @nawadanp</li> <li>move README to markdown, change ordering</li> <li>update the README with the new options from --help</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-04-16-py3status-v1-4/#contributors","title":"contributors","text":"<p>Special thanks to this release's contributors !</p> <ul> <li>@nawadanp</li> <li>@guiniol</li> <li>@IotaSpencer</li> <li>@tasse</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/","title":"After vacation bug hunting","text":"<p>Two weeks vacations always seem short yet the 900+ mails waiting for sorting on my Gentoo Linux inbox was a reminder that our beloved distribution is well alive ! So I guess it was time for a little bug killing spree :)</p>","tags":["gentoo","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/#rabbitmq-v330","title":"rabbitMQ v3.3.0","text":"<p>This release improves performance in a variety of conditions, adds monitoring information to identify performance bottlenecks, adds dynamically manageable shovels, and allows Java-based clients to reconnect automatically after network failure.</p> <p>This release also corrects a number of defects in the broker and plugins, as well as introducing a host of smaller features as you can see on the changelog. Be warned that the behavior of the guest user has been altered !</p> <p>I also fixed a long awaiting bug to bump the rabbitMQ C client to v0.5.0</p>","tags":["gentoo","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/#redis-v289","title":"redis v2.8.9","text":"<p>Johan Bergstr\u00f6m is as always doing a great and helpful job and is actively working on redis, thanks mate !</p> <ul> <li>[NEW] The HyperLogLog data structure. You can read more about it \u00a0in this blog post</li> <li>[NEW] The Sorted Set data type has now support for lexicographic range queries, check the new commands ZRANGEBYLEX, ZLEXCOUNT and ZREMRANGEBYLEX, which are documented at http://redis.io</li> </ul>","tags":["gentoo","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/#py3status-v15","title":"py3status v1.5","text":"<ul> <li>fixes installation via pip</li> <li>added a --version command line argument to get the currently installed version of py3status</li> </ul>","tags":["gentoo","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/#upcoming-bumps","title":"upcoming bumps","text":"<p>You might be interested in what's next on the todo list :</p> <ul> <li>With the help of Thomas D. aka @Whissi, we're working on bumping and enhancing rsyslog to v7.6.3. For this a series of its dependencies have been bumped today as well.</li> <li>mongoDB v2.6.0 is also on track, as usual the guys @mongodb have broken the scons building so it's taking more time than it should to fix this hell (all help appreciated).</li> </ul>","tags":["gentoo","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-06-uwsgi-v2-0-4/","title":"uWSGI v2.0.4","text":"<p>Quick post for an interesting version bump of uWSGI which brings an experimental loopengine for\u00a0python3.4 asyncio\u00a0(aka tulip) !</p> <p>If you want to try it out, I added a python_asyncio\u00a0USE flag. I've also made some cleanups on the ebuild wrt python versions and dropped older versions of uWSGI.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2014/2014-05-06-uwsgi-v2-0-4/#highlights","title":"highlights","text":"<ul> <li>experimental asyncio loop engine (python 3.4 only)</li> <li>httprouter advanced timeout management</li> <li>purge LRU cache (v2) feature</li> <li>allow duplicate headers in http parsers</li> <li>faster on_demand Emperor management</li> <li>fixed segfault for unnamed loggers</li> </ul> <p>See the full changelog here.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2014/2014-05-14-mongodb-v2-6-1/","title":"mongoDB v2.6.1","text":"<p>This is a great pleasure to announce the version bump of mongoDB to the brand new v2.6 stable branch !</p> <p>This bump is not trivial and comes with a lot of changes, please read carefully as you will have to modify your mongodb configuration files !</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-14-mongodb-v2-6-1/#ebuild-changes","title":"ebuild changes","text":"<p>As a long time request and to be more in line with upstream's recommendations (and systemd support) I moved the configuration of the mongoDB daemons to /etc so make sure to adapt to the new YAML format.</p> <ul> <li>the mongodb configuration moved from /etc/conf.d/mongodb to the\u00a0new YAML formatted /etc/mongodb.conf</li> <li>the mongos configuration moved from /etc/conf.d/mongos to the new YAML formatted\u00a0/etc/mongos.conf</li> <li>the MMS agent configuration file has moved to /etc/mms-agent.conf</li> </ul> <p>The init scripts also have been taken care of :</p> <ul> <li>new and modern mongodb, mongos and mms-agent init scripts</li> <li>their /etc/conf.d/ configuration files are only used to modify the init script's behavior</li> </ul>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-14-mongodb-v2-6-1/#highlights","title":"highlights","text":"<p>The changelog is long and the goal of this post is not to give you an already well covered topic on the release notes but here are my favorite features :</p> <ul> <li>MongoDB preserves the order of the document fields\u00a0following write operations.</li> <li>A new write protocol integrates write operations with write concerns. The protocol also provides improved support for bulk operations.</li> <li>MongoDB can now use index intersection to fulfill queries supported by more than one index.</li> <li>Index Filters\u00a0to limit which indexes can become the winning plan for a query.</li> <li>Background index build allowed on secondaries.</li> <li>New cleanupOrphaned command to remove orphaned documents from a shard.</li> <li>usePowerOf2Sizes is now the default allocation strategy for all new collections.</li> <li>Removed upward limit of 20 000 connections for the maxIncomingConnections for mongod and mongos.</li> <li>New cursor.maxTimeMS() and corresponding maxTimeMS option for commands to specify a time limit.</li> </ul> <p>Make sure you follow the official upgrade plan to upgrade from a previous version, this release is not a simple drop-in replacement.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-14-mongodb-v2-6-1/#thanks","title":"thanks","text":"<p>Special thanks go to Johan Bergstr\u00f6m for his continuous efforts and responsiveness as well as Mike Limansky and Jason A. Donenfeld.</p>","tags":["gentoo","mongodb","nosql","portage","release"]},{"location":"Tech%20Blog/2014/2014-05-20-iran-tehran/","title":"Iran : Tehran","text":"<p>For our latest vacations, we spent two weeks in Iran and I must say that this country is astonishingly beautiful ! So forget what you think to know about this country as it's either biased or doesn't apply to its people who are very friendly and welcoming.</p> <p>We started with two days in Tehran, a gigantic and lovely city where an Iranian friend we met on the internet invited us to a treck in the surroundings mountains.</p> <p>The first thing to have to get acquainted with is how to cross the streets. Seriously the traffic is madness and uncontrolled (no traffic lights) so you have to watch out everywhere. Then you discover their nice and cooled subway which is clean and shiny and so cheap you try and count four times as you're sure you've heard the man wrong : 10 000 rials a ticket, yeah that's like 0,2\u20ac...</p> <p></p> <p></p> <p>Treck startpoint was Darban and then up to close to 3000m, thanks again Shayan. When we were awed by the beautiful mountains they had at the doorstep or their city, our Iranian friends told us that these mountains are also the cause of the massive pollution of Tehran in sping and summer because it blocks the winds from clearing the air...</p> <p></p> <p></p> <p></p> <p>Sunset from the Jamshidieh park, the tall tower on the background is the Milad Tower.</p> <p></p>","tags":["gf670w","iran","tehran"]},{"location":"Tech%20Blog/2014/2014-05-23-rsyslog-v7-6-3/","title":"rsyslog v7.6.3","text":"<p>This version bump was long overdue sorry and it has happened only thanks to the great work of Thomas D. aka @Whissi, thanks again mate.</p> <p>Please read carefully because this version introduces major ebuild changes, you'll probably have to adapt your current configuration !</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2014/2014-05-23-rsyslog-v7-6-3/#ebuild-changes","title":"ebuild changes","text":"<ul> <li>New default configuration with up to date syntax, we changed a lot of things so I'll quote the README coming with this bump.</li> </ul> <p>\"/var/log/syslog\" log file is now deprecated</p> <p>Beginning with rsyslog-7.6, the \"/var/log/syslog\" log file will no    longer being written per default. We are considering this file as    deprecated/obsolet for the typical user/system.    The content from this log file is still available through other    (dedicated) log files, see</p> <pre><code> - /var/log/cron.log\n - /var/log/daemon.log\n - /var/log/mail.log\n - /var/log/messages\n</code></pre> <p>If you really need the old \"/var/log/syslog\" log file, all you have to    do is uncommenting the corresponding configuration directive in    \"/etc/rsyslog.d/50-default.conf\".</p> <p>If you do so, don't forget to re-enable log rotation in    \"/etc/logrotate.d/rsyslog\", too.</p> <ul> <li>An additional input socket in /var/empty/dev/log (default chroot location) will be created per default</li> <li>brand new and modern init script</li> </ul>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2014/2014-05-23-rsyslog-v7-6-3/#rsyslog-763","title":"rsyslog-7.6.3","text":"<p>Coming from the rsyslog release announcement page, this is what happened with the 7.6 branch release :</p> <p>With 7.6 being the successor of the 7.5 development branch, everything that has been added there has now found its way into the stable version.</p> <p>The major additions consist of : - imrelp/omrelp now support TLS &amp; (zip) compression - impstats is now emitting resource usage counters, can directly emit delta values and can now be bound to a ruleset - mmpstrucdata is a new module to parse RFC5424 structured data into JSON message properties - mmutf8fix is a new module to fix invalid UTF-8 sequences - mmsequence is a new module that helps with action load balancing - new defaults for main/ruleset queues to be more enterprise-like</p> <p>Also the new stable version has undergone a lot of bug fixes, performance improvements and optimizations that make rsyslog 7.6 a lot more reliable and performing than before.</p>","tags":["gentoo","portage","release","rsyslog"]},{"location":"Tech%20Blog/2014/2014-05-25-iran-shiraz/","title":"Iran : Shiraz","text":"<p>Our Iranian friend booked us a night train from Tehran to our next stop in the South of Iran : Shiraz.</p> <p>The Tehran train station is well organized and as foreigners we had to show our passports to the station's police where we were greeted with the big smile by the policemen. The train itself was good, clean, comfortable and on schedule even if it was quite slow : departure at 20h and we arrived at 11h30 a few kilometers up from Shiraz.</p> <p>Travelling by train is always a great experience. As we woke up early in the morning, this is what was waiting for us behind the curtains :</p> <p></p> <p>Compared to Tehran, the streets are more typical and fitting what you'd expect from a desert city. That's the exact kind of town you want to get lost in, just walking around and going into its nice and large bazaar.</p> <p></p> <p></p> <p></p> <p></p> <p>Shiraz is known for its many palaces and beautiful gardens.</p> <p></p> <p></p> <p></p> <p></p> <p>This city is also the usual starting point to visit the famous Persepolis. While the place is great, I must admit that we loved the abandonned\u00a0park right next to it which is used as a playground and food camping by the schools and other Iranian tourists.</p> <p></p> <p></p>","tags":["gf670w","iran","shiraz"]},{"location":"Tech%20Blog/2014/2014-06-02-uwsgi-v2-0-5-1/","title":"uWSGI v2.0.5.1","text":"<p>This release is important to me (and my company) as it officially introduces a few features we developed for our needs and then contributed to uWSGI.</p> <p>Special congratulations to my co-worker\u00a0@btall for his first contribution and for those nice features to the metrics subsystem with many thanks as usual to @unbit for reviewing and merging them so quickly.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2014/2014-06-02-uwsgi-v2-0-5-1/#new-features","title":"new features","text":"<ul> <li>graceful reload of mule processes (Credits: Paul Egan) :\u00a0SIGHUP is now sent to mules instead of directly killing them, by default you have 60 seconds to react before a\u00a0SIGKILL</li> <li>\u2013metrics-no-cores, \u2013stats-no-cores, \u2013stats-no-metrics : don't calculate and process all those core related metrics (gevent anyone ?)</li> <li>reset_after_push\u00a0for metrics (Credits: Babacar Tall) : this metric attribute ensures that the metric value is reset to 0 or its hardcoded initial_value every time the metric is pushed to some external system (like carbon, or statsd)</li> <li>new metric_set_max and metric_set_min helpers (Credits: Babacar Tall)\u00a0: can be used to avoid having to call ``metric_get`` when you need a metric to be set at a maximal or minimal value. Another simple use case is to use the ``avg`` collector to calculate an average between some *max* and *min* set metrics. Available in C and python.</li> </ul> <p>See the full changelog here, especially some interesting bugfixes.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/","title":"Consul on Gentoo Linux","text":"<p>As a clustering and distributed architecture enthusiast, I'm naturally interested in software providing neat ways to coordinate any kind of state/configuration/you-name-it over a large number of machines.</p> <p>My quest, as many of you I guess, were so far limited to tools like zookeeper (packaged on my overlay but with almost no echo) and\u00a0doozerd (last commit nearly 6 months ago) which both cover some of the goals listed above with more or less flavors and elegance (sorry guys, JAVA is NOT elegant to me).</p> <p>I recently heard about consul, a new attempt to solve some of those problems in an interesting way while providing some rich fuctionnalities so I went on giving it a try and naturally started packaging it so others can too.</p>","tags":["cluster","consul","gentoo","portage"]},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/#wtf-is-consul","title":"WTF is consul ?","text":"<p>Consul is a few months' old project (and already available on Gentoo !) from the guys making Vagrant. I especially like its datacenter centric architecture, intuitive deployment and its DNS + HTTP API query mechanisms. This sounds promising so far !</p> <p>This is a descripion taken from the Hashicorp's blog :</p> <p>Consul is a solution for service discovery and configuration. Consul is completely distributed, highly available, and scales to thousands of nodes and services across multiple datacenters.</p> <p>Some concrete problems Consul solves: finding the services applications need (database, queue, mail server, etc.), configuring services with key/value information such as enabling maintenance mode for a web application, and health checking services so that unhealthy services aren\u2019t used. These are just a handful of important problems Consul addresses.</p> <p>Consul solves the problem of service discovery and configuration. Built on top of a foundation of rigorous academic research, Consul keeps your data safe and works with the largest of infrastructures. Consul embraces modern practices and is friendly to existing DevOps tooling.</p>","tags":["cluster","consul","gentoo","portage"]},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/#app-adminconsul","title":"app-admin/consul ?","text":"<p>This is a RFC and interest call about the packaging and availability of consul for Gentoo Linux.</p> <p>The latest version and live ebuilds are present in my overlay so if you are interested, please tell me (here, IRC, email, whatever) and I'll consider adding it to the portage tree.</p>","tags":["cluster","consul","gentoo","portage"]},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/#i-want-to-test-it","title":"I want to test it !","text":"<p>Now that would be helpful to get some feedback about the usability of the current packaging. So far the ebuild features what I think should cover a lot of use cases :</p> <ul> <li>full build from sources</li> <li>customizable consul agent init script with reload, telemetry and graceful stop support</li> <li>web UI built from sources and installation for easy deployment</li> </ul> <p># layman -a ultrabug</p>","tags":["cluster","consul","gentoo","portage"]},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/#emerge-av-consul","title":"emerge -av consul","text":"<p>Hope this interests some of you folks !</p>","tags":["cluster","consul","gentoo","portage"]},{"location":"Tech%20Blog/2014/2014-06-15-iran-yazd/","title":"Iran : Yazd","text":"<p>We took our first bus trip to reach Yazd\u00a0from Shiraz using the Hamsafar company. Booking a bus trip is as easy as it is cheap in Iran so this is by far the best way to get around even tho it's a bit slow mostly due to the police controls along the road.</p> <p>Yazd was a shock as it's a small and beautiful desert town with an unique athmosphere. This city still haunts me and remains my favorite of the trip. The baazar, the covered streets and the mud walls gives you a feeling which is difficult to describe.</p> <p>We stayed at the very pleasant Orient Hotel and spent one night in a caravanserai where I tried my luck and succeeded to rent a motorcycle for one day ! That was a fun and incredible experience and we'll always remember the look on the amuzed face of the Iranian people when they realized some tourists where riding a motorcycle among them.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Yazd is of course not only about desert and features some beautiful and peaceful gardens.</p> <p></p> <p></p>","tags":["gf670w","iran","yazd"]},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/","title":"Europython 2014","text":"<p>I had the chance to participate to europython 2014\u00a0as my company was sponsoring the event.</p> <p></p> <p>This was a great week where I got to meet some very interesting people and hear about some neat python use cases, libraries and new technologies so I thought I'd write a quick summary of my biased point of view.</p>","tags":["consul","europython","python","uwsgi","zeromq"]},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/#zeromq","title":"ZeroMQ","text":"<p>I had the chance to meet Pieter Hintjens and participate in a 3 hours workshop on ZeroMQ. This was very interesting and refreshing as to go in more depth into this technology which I've been using in production for several years now.</p> <p>Pieter is also quite a philosophical person and I strongly encourage you to listen to\u00a0his keynote. I ended up pinging him in real life for an issue I've been waiting for bug correction on the libzmq and it got answered nicely.</p>","tags":["consul","europython","python","uwsgi","zeromq"]},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/#uwsgi","title":"uWSGI","text":"<p>Another big thing in our python stack is the uWSGI application container which I love and follow closely even if my lack of knowledge in C++ prevents me for going too deep in the source code... I got the chance to speak with\u00a0Roberto De Ioris about the next 2.1 release and propose him two new features.</p> <ul> <li>Allow message spooling directly from a gevent greenlet : I'm glad to say it's been committed today and I successfully test it, huge feature !</li> <li>Consul.io integration in uWSGI : I wrote a RFC about it and it's going to be done for the great good of uWSGI users and distributed architecture lovers !</li> </ul> <p>Thanks a lot for your consideration Roberto !</p>","tags":["consul","europython","python","uwsgi","zeromq"]},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/#trends","title":"Trends","text":"<ul> <li>Not tested = broken !</li> <li>Python is strong and very lively in the Big Data world</li> <li>Asynchronous and distributed architectures get more and more traction and interest</li> </ul>","tags":["consul","europython","python","uwsgi","zeromq"]},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/#videos","title":"Videos","text":"<p>All the talks videos are already online, you should check them out !</p>","tags":["consul","europython","python","uwsgi","zeromq"]},{"location":"Tech%20Blog/2014/2014-08-12-hd-daymaker-led-headlamp/","title":"HD Daymaker LED Headlamp","text":"<p>Short post to share my experience with the Harley-Davidson\u00a0Daymaker LED Headlamp.</p> <p>I came to buy it because I was not satisfied with the standard lamp fitted on my sportster and I guess whoever has to drive by night would feel that unpleasant feeling to not actually be able to properly see what's going on in front of you.</p> <p>The LED Headlamp is worth the few hundred bucks it costs if at least for the sake of your own life but furthermore for the incredible improvement from the standard lamp. Don't hesitate a second just go for it and it's dead simple to mount yourself !</p> <p>See the difference (passing lights) :</p> <p></p> <p></p> <p>Now I feel way safer to drive on unlitten roads.</p>","tags":["harley"]},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/","title":"Using uWSGI and Consul to design a distributed application","text":"","tags":["consul","gentoo","python","uwsgi"]},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#foreword","title":"Foreword","text":"<p>Let's say we have to design an application that should span across multiple datacenters while being able to scale\u00a0as easily as firing up a new vm/container without the need to update any kind of configuration.</p> <p>Facing this kind of challenge is exciting and requires us to address a few key scaffolding points before actually starting to code something :</p> <ul> <li>having a robust and yet versatile application container to run our application</li> <li>having a datacenter aware, fault detecting and service discovery service</li> </ul> <p>Seeing the title of this article, the two components I'll demonstrate are obviously uWSGI and Consul which can now work together thanks to the uwsgi-consul plugin.</p> <p>While this article example is written in python, you can benefit from the same features in all the languages supported by uWSGI which includes go, ruby, perl ad php !</p>","tags":["consul","gentoo","python","uwsgi"]},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#our-first-service-discovering-application","title":"Our first service discovering application","text":"<p>The application will demonstrate how simple it is for a client to discover all the available servers running a specific service on a given port. The best part is that the services will be registered and deregistered automatically by uWSGI as they're loaded and unloaded.</p> <p>The demo application logic is as follows :</p> <ol> <li>uWSGI will load two server applications which are each responsible for providing the specified service on the given port</li> <li>uWSGI will automatically register the configured service\u00a0into Consul</li> <li>uWSGI will also automatically register a health check for the configured service\u00a0into Consul so that Consul will also be able to detect any failure of the service</li> <li>Consul will then respond to any client requesting the list of the available servers (nodes) providing the specified service</li> <li>The client will query Consul for the service and get either an empty response (no server available / loaded) or the list of the available servers</li> </ol> <p>Et voil\u00e0, the client can dynamically detect new/obsolete servers and start working !</p>","tags":["consul","gentoo","python","uwsgi"]},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#setting-up-uwsgi-and-its-consul-plugin","title":"Setting up uWSGI and its Consul plugin","text":"<p>On Gentoo Linux, you'll just have to run the following commands to get started (other users refer to the uWSGI documentation or your distro's package manager). The plugin will be built by hand as I'm still not sure how I'll package the uWSGI external plugins...</p> <p>$ sudo ACCEPT_KEYWORDS=\"~amd64\" emerge uwsgi $ cd /usr/lib/uwsgi/ $ sudo uwsgi --build-plugin https://github.com/unbit/uwsgi-consul $ cd -</p> <p>You'll have installed the uwsgi-consul plugin which you should see here :</p> <p>$ ls /usr/lib/uwsgi/consul_plugin.so /usr/lib/uwsgi/consul_plugin.so</p> <p>That's all we need to have uWSGI working with Consul.</p>","tags":["consul","gentoo","python","uwsgi"]},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#setting-up-a-consul-server","title":"Setting up a Consul server","text":"<p>Gentoo users will need to add the ultrabug\u00a0overlay (use layman) and then install consul (other users refer to the Consul documentation or your distro's package manager).</p> <p>$ sudo layman -a ultrabug $ sudo ACCEPT_KEYWORDS=\"~amd64\" USE=\"web\" emerge consul</p> <p>Running the server and its UI is also quite straightforward. For this example, we will run it directly from a dedicated terminal so you can also enjoy the logs and see what's going on (Gentoo users have an init script and conf.d ready for them shall they wish to go further).</p> <p>Open a new terminal and run :</p> <p>$ consul agent -data-dir=/tmp/consul-agent -server -bootstrap -ui-dir=/var/lib/consul/ui -client=0.0.0.0</p> <p>You'll see consul running and waiting for work. You can already enjoy the web UI by pointing your browser to\u00a0http://127.0.0.1:8500/ui/.</p>","tags":["consul","gentoo","python","uwsgi"]},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#running-the-application","title":"Running the application","text":"<p>To get this example running, we'll use the uwsgi-consul-demo code that I prepared.</p> <p>First of all we'll need the consulate python library (available on pypi via pip). Gentoo users can just install it (also from the ultrabug overlay added before) :</p> <p>$ sudo ACCEPT_KEYWORDS=\"~amd64\" emerge consulate</p> <p>Now let's clone the demo repository and get into the project's directory.</p> <p>$ git clone git@github.com:ultrabug/uwsgi-consul-demo.git $ cd uwsgi-consul-demo</p> <p>First, we'll run the client\u00a0which should report that no server is available yet. We will keep this terminal open to see the client detecting in real time the appearance and disappearance of the servers as we start and stop uwsgi :</p> <p>$ python client.py  no consul-demo-server available [...] no consul-demo-server available</p> <p>Open a new terminal and get inside the project's directory. Let's have uWSGI load the two servers and register them in Consul :</p> <p>$ uwsgi --ini uwsgi-consul-demo.ini --ini uwsgi-consul-demo.ini:server1 --ini uwsgi-consul-demo.ini:server2 [...] * server #1 is up on port 2001</p> <p>* server #2 is up on port 2002</p> <p>[consul] workers ready, let's register the service to the agent [consul] service consul-demo-server registered successfully [consul] workers ready, let's register the service to the agent [consul] service consul-demo-server registered successfully</p> <p>Now let's check back our client terminal, hooray it has discovered the two servers on the host named drakar (that's my local box) !</p> <p>consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2002 consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2001</p>","tags":["consul","gentoo","python","uwsgi"]},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#expanding-our-application","title":"Expanding our application","text":"<p>Ok it works great on our local machine but we want to see how to add more servers to the fun and scale dynamically.</p> <p>Let's add another machine (named cheetah here) to the fun and have servers running there also while our client is still running on our local machine.</p> <p>On cheetah :</p> <ul> <li>install uWSGI as described earlier</li> <li>install Consul as described earlier</li> </ul> <p>Run a Consul agent (no need of a server) and tell him to work with your already running consul server on your box (drakar in my case) :</p> <p>$ /usr/bin/consul agent -data-dir=/tmp/consul-agent -join drakar -ui-dir=/var/lib/consul/ui -client=0.0.0.0</p> <p>The -join  is the important part. <p>Now run uWSGI so it starts and registers two new servers on cheetah :</p> <p>$ uwsgi --ini uwsgi-consul-demo.ini --ini uwsgi-consul-demo.ini:server1 --ini uwsgi-consul-demo.ini:server2</p> <p>And check the miracle on your client terminal still running on your local box, the new servers have appeared and will disappear if you stop uwsgi on the cheetah node :</p> <p>consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2001 consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2002 consul-demo-server found on node cheetah (yy.yy.yy.yy) using port 2001 consul-demo-server found on node cheetah (yy.yy.yy.yy) using port 2002</p>","tags":["consul","gentoo","python","uwsgi"]},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#go-mad","title":"Go mad","text":"<p>Check the source code, it's so simple and efficient you'll cry ;)</p> <p>I hope this example has given you some insights and ideas for your current or future application designs !</p>","tags":["consul","gentoo","python","uwsgi"]},{"location":"Tech%20Blog/2014/2014-10-08-py3status-v1-6/","title":"py3status v1.6","text":"<p>Back from holidays, this new version of py3status\u00a0was due for a long time now as it features a lot of great contributions !</p> <p>This version is dedicated to the amazing @ShadowPrince who contributed 6 new modules :)</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-10-08-py3status-v1-6/#changelog","title":"Changelog","text":"<ul> <li>core : rename the 'examples' folder to 'modules'</li> <li>core : Fix include_paths default wrt issue #38, by Frank Haun</li> <li>new vnstat module, by Vasiliy Horbachenko</li> <li>new net_rate module, alternative module for tracking network rate, by Vasiliy Horbachenko</li> <li>new scratchpad-counter\u00a0module and window-title module for displaying current windows title, by Vasiliy Horbachenko</li> <li>new keyboard-layout module, by\u00a0Vasiliy Horbachenko</li> <li>new mpd_status module, by Vasiliy Horbachenko</li> <li>new\u00a0clementine module displaying the current \"artist - title\" playing in Clementine, by Fran\u00e7ois LASSERRE</li> <li>module clementine.py: Make python3 compatible, by Frank Haun</li> <li>add optional CPU temperature to the sysdata module, by Rayeshman</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-10-08-py3status-v1-6/#contributors","title":"Contributors","text":"<p>Huge thanks to this release's contributors :</p> <ul> <li>@ChoiZ</li> <li>@fhaun</li> <li>@rayeshman</li> <li>@ShadowPrince</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-10-08-py3status-v1-6/#whats-next","title":"What's next ?","text":"<p>The next 1.7 release of py3status will bring a neat and cool feature which I'm sure you'll love, stay tuned !</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2014/2014-10-14-one-month-in-turkey/","title":"One month in Turkey","text":"<p>Our latest roadtrip was as amazing as it was challenging because we decided that we'd spend an entire month in Turkey and use our own motorbike to get there from Paris.</p>","tags":["harley","roadtrip","turkey"]},{"location":"Tech%20Blog/2014/2014-10-14-one-month-in-turkey/#transportation","title":"Transportation","text":"<p>Our main idea was to spare ourselves from the long hours of road riding to Turkey so we decided from the start to use ferries to get there. Turns out that it's pretty easy as you have to go through Italy and Greece before you set foot in Bodrum, Turkey.</p> <ul> <li>Paris -&gt; Nice : train</li> <li>Nice -&gt; Parma (IT) -&gt; Ancona : road, (~7h drive)</li> <li>Ancona -&gt; Patras (GR) : ferry (21h)</li> <li>Patras -&gt; Piraeus (Athens) : road (~4h drive, constructions)</li> <li>Piraeus -&gt; Kos : ferry (~11h by night)</li> <li>Kos -&gt; Bodrum (TR) : ferry (1h)</li> </ul> <p>Turkish customs are very friendly and polite, it's really easy to get in with your own vehicle.</p>","tags":["harley","roadtrip","turkey"]},{"location":"Tech%20Blog/2014/2014-10-14-one-month-in-turkey/#tribute-to-the-nightster","title":"Tribute to the Nightster","text":"<p>This roadtrip added 6000 kms to our brave and astonishing Harley-Davidson Nightster. We encountered no problem at all with the bike even though we clearly didn't go easy on her. We rode on gravels, dirt and mud without her complaining, not to mention the weight of our luggages and the passengers ;)</p> <p>That's why this post will be dedicated to our bike and I'll share some of the photos I took of it during the trip. The real photos will come in some other posts.</p>","tags":["harley","roadtrip","turkey"]},{"location":"Tech%20Blog/2014/2014-10-14-one-month-in-turkey/#a-quick-photo-tour","title":"A quick photo tour","text":"<p>I can't describe well enough the pleasure and freedom feeling you get when travelling in motorbike so I hope those first photos will give you an idea.</p> <p>I have to admit that it's really impressive to leave your bike alone between the numerous trucks parking, loading/unloading their stuff a few centimeters from it.</p> <p></p> <p>We arrived in Piraeus easily, time to buy tickets for the next boat to Kos.</p> <p> </p> <p>Kos is quite a big island that you can discover best by ... riding around !</p> <p></p> <p>After Bodrum, where we only spent the night, you quickly discover the true nature of Turkish roads and scenery. Animals are everywhere and sometimes on the road such as those donkeys below.</p> <p></p> <p>This is a view from the Bozburun bay. Two photos for two bike layouts : beach version and fully loaded version ;)</p> <p> </p> <p>On the way to Cappadocia, near Karapinar :</p> <p></p> <p>The amazing landscapes of Cappadocia, after two weeks by the sea it felt cold up there.</p> <p> </p> <p>Our last picture from the bike next to the trail leading to our favorite and lonely \"private\" beach on the Dat\u00e7a peninsula.</p> <p></p>","tags":["harley","roadtrip","turkey"]},{"location":"Tech%20Blog/2014/2014-11-20-rip-ns2/","title":"RIP ns2","text":"<p>Today we did shutdown our now oldest running Gentoo Linux production server : ns2.</p> <p>Obviously this machine was happily spreading our DNS records around the world but what's remarkable about it is that it has been doing so for\u00a02717 straight days !</p> <p>$ uptime  13:00:45 up 2717 days,  2:20,  1 user,  load average: 0.13, 0.04, 0.01</p> <p>As I mentioned when we did shutdown stabber, our beloved firewall, our company has been running Gentoo Linux servers in production for a long time now and we're always a bit sad when we have to power off one of them.</p> <p>As usual, I want to take this chance to thank everyone contributing to Gentoo Linux ! Without our collective work, none of this would have been possible.</p>","tags":["gentoo"]},{"location":"Tech%20Blog/2014/2014-12-20-gentoo-linux-pxe-builder/","title":"Gentoo Linux PXE builder","text":"<p>Due to a bad hardware failure a few weeks ago at work, I had to rebuild a good part of our PXE stack and I ended up once again looking for the steps to build a PXE-ready Gentoo initramfs.</p> <p>Then I realized that, while I was at it, I wanted this PXE initramfs to feature more than a Live CD like boot because I use PXE to actually install my servers automatically using ansible. So why not embed all my needs straight into the PXE initramfs and automate the whole boring creation process of it ?</p> <p>That what the gentoo-pxe-builder\u00a0project is about and I thought I'd open source it in case it could help and spare some time to anyone else.</p> <p>The main idea is to provide a simple bash script which bases itself on the latest Gentoo liveCD kernel/initramfs to prepare a PXE suitable version which you can easily hack into without having to handle all the squashfs/cpio hassle to rebuild it.</p> <p>Quick steps it does for you :</p> <ul> <li>download the latest live CD</li> <li>extract the kernel / initramfs from it</li> <li>patch the embedded squashfs to make it PXE ready</li> <li>setup SSH and a default root password so you can connect to your PXE booted machine directly</li> <li>add a hackable local.d start script which will be executed at the end of the PXE boot</li> </ul> <p>The provided local.d start script provides IP address display so you can actually see the IP address being setup on your PXE host and it will also display\u00a0the real name of the network interfaces detected on the host based on udev deterministic naming.</p> <p>You can read everything in more details on the project's README.</p> <p>Of course it's mainly oriented to my use case and I'm sure the process / patching could be even more elegant so anyone feel free to contribute or ask/propose some features, I'll happily follow them up !</p>","tags":["gentoo","pxe"]},{"location":"Tech%20Blog/2015/2015-01-04-py3status-v2-0/","title":"py3status v2.0","text":"<p>I'm very pleased to announce the release of py3status v2.0 which I'd like to dedicate to the person who's behind all the nice improvements this release features : @tablet-mode !</p> <p>His idea\u00a0on issue #44 was to make py3status modules configurable. After some thoughts and merges of my own plans of development, we ended up with what I believe are the most ambitious features\u00a0py3status provides\u00a0so far.</p>","tags":["gentoo","py3status"]},{"location":"Tech%20Blog/2015/2015-01-04-py3status-v2-0/#features","title":"Features","text":"<p>The logic\u00a0behind this release is that\u00a0py3status now wraps and extends your i3status.conf which allows all the following crazy features :</p> <ul> <li>Handle all click events directly from your i3status config</li> </ul> <p>For all your i3bar modules i3status and py3status alike thanks to the new on_click parameter which you can use like any other i3status.conf parameter on all modules. It has never been so easy to handle click events !</p> <p>This is a quick and small example of what it looks like :</p> <pre><code># run thunar when I left click on the / disk info module\ndisk / {\n    format = \"/ %free\"\n    on_click 1 = \"exec thunar /\"\n}\n</code></pre> <ul> <li>All py3status contributed modules are now shipped and\u00a0usable directly without the need to copy them to your local folder. They also get to be configurable directly from your i3status config (see below)</li> </ul> <p>No need to copy and edit the contributed py3status modules you like and wish to use, you can now load and configure them directly from your i3status.conf.</p> <ul> <li>Load, configure and order py3status modules directly from your i3status config just like any other i3status module</li> </ul> <p>All py3status modules (contributed ones and user loaded ones)\u00a0are now loaded and ordered using the usual syntax order += in your i3status.conf !</p> <ul> <li>All modules have been improved, cleaned up and some of them got some love from contributors.</li> <li>Every click event now triggers a refresh of the clicked module, even for i3status modules.\u00a0This makes your i3bar more responsive than ever !</li> </ul>","tags":["gentoo","py3status"]},{"location":"Tech%20Blog/2015/2015-01-04-py3status-v2-0/#contributors","title":"Contributors","text":"<ul> <li>@AdamBSteele</li> <li>@obb</li> <li>@scotte</li> <li>@tablet-mode</li> </ul>","tags":["gentoo","py3status"]},{"location":"Tech%20Blog/2015/2015-01-04-py3status-v2-0/#thank-you","title":"Thank you","text":"<ul> <li>Jakub Jedelsky : py3status is now packaged on Fedora Linux.</li> <li>All of you users : py3status has broken the 100 stars on github, I'm still amazed by this. @Lujeni's prophecy has come true :)</li> <li>I still have some nice ideas in stock for even more functionalities, stay tuned !</li> </ul>","tags":["gentoo","py3status"]},{"location":"Tech%20Blog/2015/2015-02-27-mongodb-2-6-8-2-4-13-the-upcoming-3-0-0/","title":"mongoDB 2.6.8, 2.4.13 & the upcoming 3.0.0","text":"<p>I'm a bit slacking on those follow-up posts but with the upcoming mongoDB 3.x series and the recent new releases I guess it was about time I talked a bit about what was going on.</p>"},{"location":"Tech%20Blog/2015/2015-02-27-mongodb-2-6-8-2-4-13-the-upcoming-3-0-0/#mongodb-300_rcx","title":"mongodb-3.0.0_rcX","text":"<p>Thanks to the help of Tomas Mozes, we might get a release candidate version of the 3.0.0 version of mongoDB pretty soon in tree shall you want to test it on Gentoo. Feel free to contribute or give feedback in the bug, I'll do my best to keep up.</p> <p>What Tomas proposes matches what I had in mind so for now the plan is to :</p> <ul> <li>split the mongo tools (mongodump/export etc) to a new package : dev-db/mongo-tools or app-admin/mongo-tools ?</li> <li>split the MMS monitoring agent to its own package : app-admin/mms-monitoring-agent</li> <li>have a look at the MMS backup agent and maybe propose its own package if someone is interested in this ?</li> <li>after the first release, have a look at the MMS deployment automation to see how it could integrate with Gentoo</li> </ul>"},{"location":"Tech%20Blog/2015/2015-02-27-mongodb-2-6-8-2-4-13-the-upcoming-3-0-0/#mongodb-268-2413","title":"mongodb-2.6.8 &amp; 2.4.13","text":"<p>Released 2 days ago, they are already on portage ! The 2.4.13 is mostly a security (SSL v3) and tiny backport release whereas the 2.6.8 fixes quite a bunch of bugs.</p> <p>Please note that I will drop the 2.4.x releases when 3.0.0 hits the tree ! I will keep the latest 2.4.13 in my overlay if someone asks for it.</p>"},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/","title":"mongoDB 3.0.1","text":"<p>This is a quite awaited version bump coming to portage and I'm glad to announce it's made its way to the tree today !</p> <p>I'll right away thank a lot Tomas Mozes and Darko Luketic for their amazing help, feedback and patience !</p>","tags":["gentoo","mongodb","nosql","portage"]},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/#mongodb-301","title":"mongodb-3.0.1","text":"<p>I introduced quite some changes in this ebuild which I wanted to share with you and warn you about. MongoDB upstream have stripped quite a bunch of things out of the main mongo core repository which I have in turn split into ebuilds.</p> <p>Major changes :</p> <ul> <li>respect upstream's optimization flags : unless in debug build, user's optimization flags will be ignored to prevent crashes and weird behaviour.</li> <li>shared libraries for C/C++ are not built by the core mongo repository anymore, so I removed the static-libs USE flag.</li> <li>various dependencies optimization to trigger a rebuild of mongoDB when one of its linked dependency changes.</li> </ul>","tags":["gentoo","mongodb","nosql","portage"]},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/#app-adminmongo-tools","title":"app-admin/mongo-tools","text":"<p>The new tools USE flag allows you to pull a new ebuild named app-admin/mongo-tools which installs the commands listed below. Obviously, you can now just install this package if you only need those tools on your machine.</p> <ul> <li>mongodump / mongorestore</li> <li>mongoexport / mongoimport</li> <li>mongotop</li> <li>mongofiles</li> <li>mongooplog</li> <li>mongostat</li> <li>bsondump</li> </ul>","tags":["gentoo","mongodb","nosql","portage"]},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/#app-adminmms-agent","title":"app-admin/mms-agent","text":"<p>The MMS agent has now some real version numbers and I don't have to host their source on Gentoo's infra woodpecker. At the moment there is only the monitoring agent available, shall anyone request the backup one, I'll be glad to add its support too.</p>","tags":["gentoo","mongodb","nosql","portage"]},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/#dev-libsmongo-cxx-driver","title":"dev-libs/mongo-c(xx)-driver","text":"<p>I took this opportunity to add the dev-libs/mongo-cxx-driver to the tree and bump the mongo-c-driver one. Thank you to Balint SZENTE for his insight on this.</p>","tags":["gentoo","mongodb","nosql","portage"]},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/","title":"py3status v2.4","text":"<p>I'm very pleased to announce this new release of py3status because it is by far the most contributed one with a total of 33 files changed, 1625 insertions and 509 deletions !</p> <p>I'll start by thanking this release's contributors with a special mention for Federico Ceratto for his precious insights, his CLI idea and implementation and other modules contributions.</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#thank-you","title":"Thank you","text":"<ul> <li>Federico Ceratto</li> <li>@rixx (and her amazing reactivity)</li> <li>J.M. Dana</li> <li>@Gamonics</li> <li>@guilbep</li> <li>@lujeni</li> <li>@obb</li> <li>@shankargopal</li> <li>@thomas-</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#important","title":"IMPORTANT","text":"<p>In order to keep a clean and efficient code base, this is the last version of py3status supporting the legacy modules loading and ordering, this behavior will be dropped on the next 2.5 version !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#cli-commands","title":"CLI commands","text":"<p>py3status now supports some CLI commands which allows you to get information about all the available modules and their documentation.</p> <ul> <li>list all available modules</li> </ul> <p>if you specify your own inclusion folder(s) with the -i parameter, your modules will be listed too !</p> <p>$ py3status modules list Available modules:   battery_level          Display the battery level.   bitcoin_price          Display bitcoin prices using bitcoincharts.com.   bluetooth              Display bluetooth status.   clementine             Display the current \"artist - title\" playing in Clementine.   dpms                   Activate or deactivate DPMS and screen blanking.   glpi                   Display the total number of open tickets from GLPI.   imap                   Display the unread messages count from your IMAP account.   keyboard_layout        Display the current keyboard layout.   mpd_status             Display information from mpd.   net_rate               Display the current network transfer rate.   netdata                Display network speed and bandwidth usage.   ns_checker             Display DNS resolution success on a configured domain.   online_status          Display if a connection to the internet is established.   pingdom                Display the latest response time of the configured Pingdom checks.   player_control         Control music/video players.   pomodoro               Display and control a Pomodoro countdown.   scratchpad_counter     Display the amount of windows in your i3 scratchpad.   spaceapi               Display if your favorite hackerspace is open or not.   spotify                Display information about the current song playing on Spotify.   sysdata                Display system RAM and CPU utilization.   vnstat                 Display vnstat statistics.   weather_yahoo          Display Yahoo! Weather forecast as icons.   whoami                 Display the currently logged in user.   window_title           Display the current window title.   xrandr                 Control your screen(s) layout easily.</p> <ul> <li>get available modules details and configuration</li> </ul> <p>$ py3status modules details Available modules:   battery_level          Display the battery level.</p> <pre><code>                     Configuration parameters:\n                         - color_\\* : None means - get it from i3status config\n                         - format : text with \"text\" mode. percentage with % replaces {}\n                         - hide_when_full : hide any information when battery is fully charged\n                         - mode : for primitive-one-char bar, or \"text\" for text percentage output\n\n                     Requires:\n                         - the 'acpi' command line\n\n                     @author shadowprince, AdamBSteele\n                     @license Eclipse Public License\n                     ---\n</code></pre> <p>[...]</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#modules-changelog","title":"Modules changelog","text":"<ul> <li>new bluetooth module by J.M. Dana</li> <li>new online_status module by @obb</li> <li>new player_control module, by Federico Ceratto</li> <li>new spotify module, by Pierre Guilbert</li> <li>new xrandr module to handle your screens layout from your bar</li> <li>dpms module activate/deactivate the screensaver as well</li> <li>imap module various configuration and optimizations</li> <li>pomodoro module can use DBUS notify, play sounds and be paused</li> <li>spaceapi module bugfix for space APIs without 'lastchange' field</li> <li>keyboard_layout module incorrect parsing of \"setxkbmap -query\"</li> <li>battery_level module better python3 compatibility</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#other-highlights","title":"Other highlights","text":"<p>Full changelog here.</p> <ul> <li>catch daylight savings time change</li> <li>ensure modules methods are always iterated alphabetically</li> <li>refactor default config file detection</li> <li>rename and move the empty_class example module to the doc/ folder</li> <li>remove obsolete i3bar_click_events module</li> <li>py3status will soon be available on debian thx to Federico\u00a0Ceratto !</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/","title":"MongoDB 3.0 upgrade in production : first steps","text":"<p>We've been running a nice mongoDB cluster in production for several years now in my company.</p> <p>This cluster suits quite a wide range of use cases from very simple configuration collections to complex queried ones and real time analytics. This versatility has been the strong point of mongoDB for us since the start as it allows different teams to address their different problems using the same technology. We also run some dedicated replica sets for other purposes and network segmentation reasons.</p> <p>We've waited a long time to see the latest 3.0 release features happening. The new WiredTiger storage engine hit the fan at the right time for us since we had reached the limits of our main production cluster and were considering alternatives.</p> <p>So as surprising it may seem, it's the first of our mongoDB architecture we're upgrading to v3.0 as it has become a real necessity.</p> <p>This post is about sharing our first experience about an ongoing and carefully planned major upgrade of a production cluster and does not claim to be a definitive migration guide.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#upgrade-plan-and-hardware","title":"Upgrade plan and hardware","text":"<p>The upgrade process is well covered in the mongoDB documentation already but I will list the pre-migration base specs of every node of our cluster.</p> <ul> <li>mongodb v2.6.8</li> <li>RAID1 spinning HDD 15k rpm for the OS (Gentoo Linux)</li> <li>RAID10 4x SSD for mongoDB files under LVM</li> <li>64 GB RAM</li> </ul> <p>Our overall philosophy is to keep most of the configuration parameters to their default values to start with. We will start experimenting with them when we have sufficient metrics to compare with later.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#disk-repartitioning-considerations","title":"Disk (re)partitioning considerations","text":"<p>The master-get-all-the-writes architecture is still one of the main limitation of mongoDB and this does not change with v3.0 so you obviously need to challenge your current disk layout to take advantage of the new WiredTiger engine.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#mongodb-26-mmapv1","title":"mongoDB 2.6 MMAPv1","text":"<p>Considering our cluster data size, we were forced to use our 4 SSD in a RAID10 as it was the best compromise to preserve performance while providing sufficient data storage capacity.</p> <p>We often reached the limits of our I/O and moved the journal out of the RAID10 to the mostly idle OS RAID1 with no significant improvements.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#mongodb-30-wiredtiger","title":"mongoDB 3.0 WiredTiger","text":"<p>The main consideration point for us is the new feature allowing to store the indexes in a separate directory. So we anticipated the data storage consumption reduction thanks to the snappy compression and decided to split our RAID10 in two dedicated RAID1.</p> <p>Our test layout so far is :</p> <ul> <li>RAID1 SSD for the data</li> <li>RAID1 SSD for the indexes and journal</li> </ul>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#our-first-node-migration","title":"Our first node migration","text":"<p>After migrating our mongos and config servers to 3.0, we picked our worst performing secondary node to test the actual migration to WiredTiger. After all, we couldn't do worse right ?</p> <p>We are aware that the strong suit of WiredTiger is actually about having the writes directed to it and will surely share our experience of this aspect later.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#compression-is-bliss","title":"compression is bliss","text":"<p>To make this comparison accurate, we resynchronized this node totally before migrating to WiredTiger so we could compare a non fragmented MMAPv1 disk usage with the WiredTiger compressed one.</p> <p>While I can't disclose the actual values, compression worked like a charm for us with a gain ratio of 3,2 on disk usage (data + indexes) which is way beyond our expectations !</p> <p>This is the DB Storage graph from MMS, showing a gain ratio of 4 surely due to indexes being in a separate disk now.</p> <p></p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#memory-usage","title":"memory usage","text":"<p>As with the disk usage, the node had been running hot on MMAPv1 before the actual migration so we can compare memory allocation/consumption of both engines.</p> <p>There again the memory management of WiredTiger and its cache shows great improvement. For now, we left the default setting which has WiredTiger limit its cache to half the available memory of the system. We'll experiment with this setting later on.</p> <p></p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#connections","title":"connections","text":"<p>This I'm still not sure of the actual cause yet but the connections count is higher and more steady than before on this node.</p> <p></p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#first-impressions","title":"First impressions","text":"<p>The node is running smooth for several hours now. We are getting acquainted to the new metrics and statistics from WiredTiger. The overall node and I/O load is better than before !</p> <p>While all the above graphs show huge improvements there is no major change from our applications point of view. We didn't expect any since this is only one node in a whole cluster and that the main benefits will also come from master node migrations.</p> <p>I'll continue to share our experience and progress about our mongoDB 3.0 upgrade.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-13-uwsgi-gevent-and-pymongo-3-threads-mayhem/","title":"uWSGI, gevent and pymongo 3 threads mayhem","text":"<p>This is a quick heads-up post about a behaviour change when running a gevent based application using the new pymongo 3 driver under uWSGI and its gevent loop.</p> <p>I was naturally curious about testing this brand new and major update of the python driver for mongoDB so I just played it dumb : update and give a try on our existing code base.</p> <p>The first thing I noticed instantly is that a vast majority of our applications were suddenly unable to reload gracefully and were force killed by uWSGI after some time !</p> <p>worker 1 (pid: 9839) is taking too much time to die...NO MERCY !!!</p>","tags":["gentoo","mongodb","nosql","pymongo","python","uwsgi"]},{"location":"Tech%20Blog/2015/2015-05-13-uwsgi-gevent-and-pymongo-3-threads-mayhem/#uwsgis-gevent-wait-for-hub","title":"uWSGI's gevent-wait-for-hub","text":"<p>All our applications must be able to be gracefully reloaded at any time. Some of them are spawning quite a few greenlets on their own so as an added measure of making sure we never loose any running greenlet we use the gevent-wait-for-hub option, which is described as follow :</p> <p>wait for gevent hub's death instead of the control greenlet</p> <p>... which does not mean a lot but is explained in a previous uWSGI changelog :</p> <p>During shutdown only the greenlets spawned by uWSGI are taken in account, and after all of them are destroyed the process will exit.</p> <p>This is different from the old approach where the process wait for ALL the currently available greenlets (and monkeypatched threads).</p> <p>If you prefer the old behaviour just specify the option gevent-wait-for-hub</p>","tags":["gentoo","mongodb","nosql","pymongo","python","uwsgi"]},{"location":"Tech%20Blog/2015/2015-05-13-uwsgi-gevent-and-pymongo-3-threads-mayhem/#pymongo-3","title":"pymongo 3","text":"<p>Compared to its previous 2.x versions, one of the overall key aspect of the new pymongo 3 driver is its intensive usage of threads to handle server discovery and connection pools.</p> <p>Now we can relate this very fact to the gevent-wait-for-hub behaviour explained above :</p> <p>the process wait for ALL the currently available greenlets (and monkeypatched threads)</p> <p>This explained why our applications were hanging until the reload-mercy (force kill) timeout option of uWSGI hit the fan !</p>","tags":["gentoo","mongodb","nosql","pymongo","python","uwsgi"]},{"location":"Tech%20Blog/2015/2015-05-13-uwsgi-gevent-and-pymongo-3-threads-mayhem/#conclusion","title":"conclusion","text":"<p>When using pymongo 3 with the gevent-wait-for-hub option, you have to keep in mind that all of pymongo's threads (so monkey patched threads) are considered as active greenlets and will thus be waited for termination before uWSGI recycles the worker !</p> <p>Two options come in mind to handle this properly :</p> <ol> <li>stop using the gevent-wait-for-hub option and change your code to use a gevent pool group to make sure that all of your important greenlets are taken care of when a graceful reload happens (this is how we do it today, the gevent-wait-for-hub option usage was just over protective for us).</li> <li>modify your code to properly close all your pymongo connections on graceful reloads.</li> </ol> <p>Hope this will save some people the trouble of debugging this ;)</p>","tags":["gentoo","mongodb","nosql","pymongo","python","uwsgi"]},{"location":"Tech%20Blog/2015/2015-05-23-gevent-ssl-support-fixed-for-python-2-7-9/","title":"Gevent : SSL support fixed for python 2.7.9","text":"<p>Good news for gevent users blocked on python &lt; 2.7.9 due to broken SSL support since python upstream dropped the private API\u00a0_ssl.sslwrap that eventlet was using.</p> <p>This issue was starting to get old and problematic since GLSA 2015-0310 but I'm happy to say that almost 6 hours after the gevent-1.0.2 release, it is already available on portage !</p> <p>We were also affected by this issue at work so I'm glad that the tension between ops and devs this issue was causing will finally be over ;)</p>","tags":["gentoo","gevent","python"]},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/","title":"MongoDB 3.0 upgrade in production : step 2 failed","text":"<p>In my previous post regarding the migration of our production cluster to mongoDB 3.0 WiredTiger, we successfully upgraded all the secondaries of our replica-sets with decent performances and (almost, read on) no breakage.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/#step-2-plan","title":"Step 2 plan","text":"<p>The next step of our migration was to test our work load on WiredTiger primaries. After all, this is where the new engine would finally demonstrate all its capabilities.</p> <ul> <li>We thus scheduled a step down from our 3.0 MMAPv1 primary servers so that our WiredTiger secondaries would take over.</li> <li>Not migrating the primaries was a safety net in case something went wrong... And boy it went so wrong we're glad we played it safe that way !</li> <li>We rolled back after 10 minutes of utter bitterness.</li> </ul>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/#the-failure","title":"The failure","text":"<p>After all the wait and expectation, I can't express our level of disappointment at work when we saw that the WiredTiger engine could not handle our work load. Our application started immediately to throw 50 to 250 WriteConflict errors per minute !</p> <p>Turns out that we are affected by this bug and that, of course, we're not the only ones. So far it seems that it affects collections with :</p> <ul> <li>heavy insert / update work loads</li> <li>an unique index (or compound index)</li> </ul>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/#the-breakages","title":"The breakages","text":"<p>We also discovered that we're affected by a weird mongodump new behaviour where the dumped BSON file does not contain the number of\u00a0documents that mongodump said it was exporting. This is clearly a new problem because it happened right after all our secondaries switched to WiredTiger.</p> <p>Since we have to ensure a strong consistency of our exports and that the mongoDB guys don't seem so keen on moving on the bug (which I surely can understand) there is a large possibility that we'll have to roll back even the WiredTiger secondaries altogether.</p> <p>Not to mention that since the 3.0 version, we experience some CPU overloads crashing the entire server on our MMAPv1 primaries that we're still trying to tackle before opening another JIRA bug...</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/#sad-panda","title":"Sad panda","text":"<p>Of course, any new major release such as 3.0 causes its headaches and brings its lot of bugs. We were ready for this hence the safety steps we took to ensure that we could roll back on any problem.</p> <p>But as a long time advocate of mongoDB I must admit my frustration, even more after the time it took to get this 3.0 out and all the expectations that came with it.</p> <p>I hope I can share some better news on the next blog post.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-07-09-mongodb-3-0-upgrade-in-production-step-3-hope/","title":"MongoDB 3.0 upgrade in production : step 3 hope","text":"<p>In our previous attempt to upgrade our production cluster to 3.0, we had to roll back from the WiredTiger engine on primary servers.</p> <p>Since then, we switched back our whole cluster to 3.0 MMAPv1 which has brought us some better performances than 2.6 with no instability.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-07-09-mongodb-3-0-upgrade-in-production-step-3-hope/#production-checklist","title":"Production checklist","text":"<p>We decided to use this increase in performance to allow us some time to fulfil the entire production checklist from MongoDB, especially the migration to XFS. We're slowly upgrading our servers kernels and resynchronising our data set after migrating from ext4 to XFS.</p> <p>Ironically, the strong recommendation of XFS in the production checklist appeared 3 days after our failed attempt at WiredTiger... This is frustrating but gives some kind of hope.</p> <p>I'll keep on posting on our next steps and results.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-07-09-mongodb-3-0-upgrade-in-production-step-3-hope/#our-hero-wiredtiger-replica-set","title":"Our hero WiredTiger Replica Set","text":"<p>While we were battling with our production cluster, we got a spontaneous major increase in the daily volumes from another platform which was running on a single Replica Set. This application is write intensive and very disk I/O bound. We were killing the disk I/O with almost a continuous 100% usage on the disk write queue.</p> <p>Despite our frustration with WiredTiger so far, we decided to give it a chance considering that this time we were talking about a single Replica Set. We were very happy to see WiredTiger keep up to its promises with an almost shocking serenity.</p> <p>Disk I/O went down dramatically, almost as if nothing was happening any more. Compression did magic on our disk usage and our application went Roarrr !</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-07-22-designing-a-scalable-and-distributed-application/","title":"Designing a scalable and distributed application","text":"<p>These are the slides of my EuroPython 2015 talk.</p> <p>The source code and ansible playbooks are available on github !</p>","tags":["europython","python","talk"]},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/","title":"py3status v2.5","text":"<p>This new py3status comes with an amazing number of contributions and new modules !</p> <p>24 files changed, 1900 insertions(+), 263 deletions(-)</p> <p>I'm also glad to say that py3status becomes my first commit in the new git repository of Gentoo Linux !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#important","title":"IMPORTANT","text":"<p>Please note that this version has deprecated the legacy implicit module loading support to favour and focus on the generic i3status order += module loading/ordering !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#new-modules","title":"New modules","text":"<ul> <li>new aws_bill module, by Anthony Brodard</li> <li>new dropboxd_status module, by Tjaart van der Walt</li> <li>new external_script module, by Dominik</li> <li>new nvidia_temp module for displaying NVIDIA GPUs' temperature, by J.M. Dana</li> <li>new rate_counter module, by Amaury Brisou</li> <li>new screenshot module, by Amaury Brisou</li> <li>new static_string module, by Dominik</li> <li>new taskwarrior module, by James Smith</li> <li>new volume_status module, by Jan T.</li> <li>new whatismyip module displaying your public/external IP as well as your online status</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#changelog","title":"Changelog","text":"<p>As usual, full changelog is available here.</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#contributors","title":"Contributors","text":"<p>Along with all those who reported issues and helped fixed them, quick and surely not exhaustive list:</p> <ul> <li>Anthony Brodard</li> <li>Tjaart van der Walt</li> <li>Dominik</li> <li>J.M. Dana</li> <li>Amaury Brisou</li> <li>James Smith</li> <li>Jan T.</li> <li>Zopieux</li> <li>Horgix</li> <li>hlmtre</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#whats-next","title":"What's next ?","text":"<p>Well something tells me @Horgix is working hard on some standardization and on the core of py3status ! I'm sure some very interesting stuff will emerge from this, so thank you !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-08-27-py3status-v2-6/","title":"py3status v2.6","text":"<p>Ok I was a bit too hasty in my legacy module support code clean up and I broke quite a few things on the latest version 2.5 release sorry ! :(</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-08-27-py3status-v2-6/#highlights","title":"highlights","text":"<ul> <li>make use of pkgutil to detect properly installed modules even when they are zipped in egg files (manual install)</li> <li>add back legacy modules output support (tuple of position / response)</li> <li>new uname module inspired from issue 117 thanks to @ndalliard</li> <li>remove dead code</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-08-27-py3status-v2-6/#thanks","title":"thanks !","text":"<ul> <li>@coelebs on IRC for reporting, testing and the good spirit :)</li> <li>@ndalliard on github for the issue, debug and for inspiring the uname module</li> <li>@Horgix for responding to issues faster than me !</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-09-14-mongodb-3-0-upgrade-in-production-step-4-victory/","title":"MongoDB 3.0 upgrade in production : step 4 victory !","text":"<p>In my last post, I explained the new hope we had in following some newly added recommended steps before trying to migrate our production cluster to mongoDB 3.0 WiredTiger.</p> <p>The most demanding step was migrating all our production servers data storage filesystems to XFS which obviously required a resync of each node... But we ended up being there pretty fast and were about to try again as 3.0.5 was getting ready, until we saw this bug coming !</p> <p>I guess you can understand why we decided to wait for 3.0.6... which eventually got released with a more peaceful changelog this time.</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-09-14-mongodb-3-0-upgrade-in-production-step-4-victory/#the-306-crash-test","title":"The 3.0.6 crash test","text":"<p>We decided as usual to test the migration to WiredTiger in two phases.</p> <ol> <li>Migrate all our secondaries to the WiredTiger engine (full resync). Wait a week to see if this has any effect on our cluster.</li> <li>Switch all the MMapv1 primary nodes to secondary and let our WiredTiger secondary nodes become the primary nodes of our cluster. Pray hard that this time it will not break under our workload.</li> </ol> <p>Step 1 results were good, nothing major changed and even our mongo dumps were still working this time (yay!). One week later, everything was still working smoothly.</p> <p>Step 2 was the big challenge which failed horribly last time. Needless to say that we were quite stressed when doing the switch. But it worked smoothly and nothing broke + performances gains were huge !</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-09-14-mongodb-3-0-upgrade-in-production-step-4-victory/#the-results","title":"The results","text":"<p>Nothing speaks better than metrics, so I'll just comment them quickly as they speak by themselves. I obviously can't disclose the scales sorry.</p> <p>Insert-only operations gained 25x performance</p> <p></p> <p>Upsert-heavy operations gained 5x performance</p> <p></p> <p>Disk I/O also showed mercy to the disk overall usage. This is due to WiredTiger superior caching and disk flushing mechanisms.</p> <p></p> <p>Disk usage decreased dramatically thanks to WiredTiger compression</p> <p></p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-09-14-mongodb-3-0-upgrade-in-production-step-4-victory/#the-last-and-next-step","title":"The last and next step","text":"<p>As of today, we still run our secondaries with the MMapv1 engine and are waiting a few weeks to see if anything goes wrong in the long run. Shall we need to roll back, we'd be able to do so very easily.</p> <p>Then when we get enough uptime using WiredTiger, we will make the final switch to a full Roarring production cluster !</p>","tags":["gentoo","mongodb","nosql"]},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/","title":"Consistent Hashing 101","text":"<p>When working on distributed systems, we often have to distribute some kind of workload on different machines (nodes) of a cluster so we have to rely on a predictable and reliable key/value mapping algorithm.</p> <p>If you're not sure about what that means, just consider the following questions when working on a cluster with a lot of machines (nodes):</p> <ul> <li>how could I make sure that all the data for a given user always gets delivered and processed on the same machine ?</li> <li>how could I make sure that I store and query the same cache server for a given key ?</li> <li>how do I split and distribute chunks of a large file across multiple storage machines and then make sure I can still access it through all those machines at once ?</li> </ul> <p>A lot of technologies answer those kind of mapping questions by implementing a hashing based distribution of their workload (be it a distributed processing, file or cache).</p> <p>Consistent hashing is one of those implementations and I felt like taking some time to explain it further.</p>","tags":["cluster","python"]},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#example-use-case","title":"Example use case","text":"<p>Here is my attempt to explain what is consistent hashing and why it is needed on distributed systems. To make things fun, we'll take this simple use case:</p> <ul> <li>I have 5 broken cars</li> <li>There are 4 repair factories nearby</li> <li>I have to implement a way to figure out where to send each car to get it fixed</li> <li>I want to ensure that an even number of cars will get fixed on each factory</li> </ul>","tags":["cluster","python"]},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#_1","title":"Consistent Hashing 101","text":"<p>This gets down to two major questions to solve:</p> <ul> <li>what is my selection criteria ? this will be my key.</li> <li>what is the expected answer ? this is my value.</li> </ul>","tags":["cluster","python"]},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#static-mapping","title":"Static mapping","text":"<p>The first approach we could implement is to manually distribute the car based on their colour.</p> <ul> <li>key = car's colour</li> <li>value = factory number</li> </ul> <p>To implement this, you use what we usually call dictionaries on various languages : those are static data structures where you assign a value to a key.</p> <p>We would then write a mapping of \"car color\" -&gt; \"factory n\" and apply this simple rule to decide where to ship a broken car.</p> <p>{   \"yellow\": \"factory 1\",   \"orange\": \"factory 2\",   \"red\": \"factory 3\",   \"green\": \"factory 4\",   \"black\": \"factory 1\" }</p> <p>This way we could indeed distribute the car repairs, but we can already see that with that an uneven number of colours ends up in over provisioning the factory 1. But there's worse:</p> <p>What if I start getting only yellow broken cars ?</p> <ul> <li>I would end up sending all of them to the factory 1 and the other factories would remain almost empty !</li> </ul> <p></p> <p>This is a serious limitation. We need a dynamic way to calculate the car distribution between the factories, for this we will use a hash algorithm !</p>","tags":["cluster","python"]},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#hash-tables","title":"Hash tables","text":"<p>A hash table is a data structure where we apply a hash function (algorithm) on the key to compute an index (pointer) into an array of buckets (values) from which we get the value.</p> <p>MD5 gives very good hashing distribution and is widely available so this makes it a very good candidate for a hashing algorithm.</p> <p>We can relate it like this to our example :</p> <ul> <li>key = car's plate number</li> <li>hash function = md5</li> <li>array of values = [factory 1, factory 2, factory 3, factory 4]</li> </ul> <p>To find out where we send a car we just could do:</p> <p>hash = md5(car plate number) index = int(hash) % size_of(array) index = 0 if index &gt; size_of(array) factory = array[index]</p> <p>In python ? okay !</p> <p>import md5</p> <p>factories = [1, 2, 3, 4]</p> <p>def get_factory(plate):     hash = int(md5.new(plate).hexdigest(), 16)     index = hash % len(factories)     if index &gt; len(factories):         index = 0     return factories[index]</p> <p>get_factory('ah-993-xx')</p> <p>3</p> <p>get_factory('zz-6793-kh')</p> <p>3</p> <p>Wow it's amazingly simple right ? :)</p> <p>Now we have a way better car repair distribution !... until something bad happens:</p> <p>What if a factory burns ?</p> <p></p> <p>Our algorithm is based on the number of available factories so removing a factory from our array means that we will redistribute a vast majority of the key mappings from our hash table !</p> <p>Keep in mind that the more values (factories) you have in your array the worse this problem gets. In our case, given a car's plate number we are sure that we wouldn't be able to figure out where a vast majority of them were sent any more.</p> <p>factories = [1, 2, 4]</p> <p>get_factory('ah-993-xx')</p> <p>2 (was 3 before)</p> <p>get_factory('zz-6793-kh')</p> <p>1 (was 3 before)</p> <p>Even worse is that when factory 3 gets repaired and back in my hash table, I will once again loose track of all my dispatched cars... What we need is a more consistent way of sorting this out.</p>","tags":["cluster","python"]},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#consistent-hashing","title":"Consistent hashing","text":"<p>The response to this kind of problem is to implement a consistent hashing algorithm. The goal of this technique is to limit the number of remapped keys when the hash table is resized.</p> <p>This is possible by imagining our factories as a circle (ring) and the hash of our keys as points on the same circle. We would then select the next factory (value) by going through the circle, always on the same way, until we find a factory.</p> <p></p> <ul> <li>Red hashed plate number would go to factory 1</li> <li>Blue hashed plate number would go to factory 3</li> </ul> <p>This way, when a factory gets added or removed from the ring, we loose only a limited portion of the key/value mappings !</p> <p>Of course on a real world example we would implement a ring with a lot more of slots by adding the same factories on the ring multiple times. This way the affected range of mappings would be smaller and the impact even more balanced !</p> <p>For instance, uhashring being fully compatible and defaulting to ketama's ring distribution, you get 160 points per node on the ring !</p> <p>I hope I got this little example right and gave you some insight on this very interesting topic !</p>","tags":["cluster","python"]},{"location":"Tech%20Blog/2015/2015-10-20-uhashring-consistent-hashing-in-python/","title":"uhashring : consistent hashing in python","text":"<p>It's been quite some time since I wanted to use a consistent hashing based distribution of my workload in a quite big cluster at work. So when we finally reached the point where this became critical for some job processing I rushed to find out what python library I could use to implement this easily and efficiently.</p> <p>I was surprised not to find a clear \"winner\" for such a library. The more \"popular\" named hash_ring has a rather unoptimized source code and is dead (old issues, no answer). Some others stepped up since but with no clear interest for contributions and almost no added features for real world applications.</p> <p>So I packaged the ketama C library and its python binding on my overlay to get intimate with its algorithm. Then I started working on my own pure python library and released\u00a0uhashring on Pypi and on Gentoo portage !</p>","tags":["gentoo","python"]},{"location":"Tech%20Blog/2015/2015-10-20-uhashring-consistent-hashing-in-python/#features","title":"Features","text":"<ul> <li>instance-oriented usage so you can use your consistent hash ring object directly in your code (see advanced usage).</li> <li>a lot of convenient methods to use your consistent hash ring in real world applications.</li> <li>simple integration with other libs such as memcache through monkey patching.</li> <li>all the missing functions in the libketama C python binding (which is not even available on pypi).</li> <li>another and more performant consistent hash algorithm if you don't care about the ketama compatibility (see benchmark).</li> <li>native pypy support.</li> <li>tests of implementation, key distribution and ketama compatibility.</li> </ul>","tags":["gentoo","python"]},{"location":"Tech%20Blog/2015/2015-10-20-uhashring-consistent-hashing-in-python/#wtf","title":"WTF ?","text":"<p>Not so sure about hash tables and consistent hashing ? Read my consistent hashing 101 explanation attempt !</p>","tags":["gentoo","python"]},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/","title":"py3status v2.7","text":"<p>I'm more than two weeks late but I'm very glad to announce the release of py3status v2.7 which features a lot of interesting stuff !</p> <p>For this release I want to salute the significant work and help of Daniel Foerster (@pydsigner), who discovered and fixed a bug in the event detection loop.</p> <p>The result is a greatly improved click event detection and bar update speed with a largely reduced CPU consumption and less code !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/#highlights","title":"Highlights","text":"<ul> <li>major performance and click event detection improvements by Daniel Foerster</li> <li>support of %z on time and tztime modules fixes #110 and #123 thx to @derekdreery and @olhotak</li> <li>directive %Z and any other failure in parsing the time and tztime modules format will result in using i3status date output</li> <li>add ethernet, wireless and battery first instance detection and support. thx to @rekoil for reporting on IRC</li> <li>i3status.conf parser handles configuration values with the = char</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/#new-modules","title":"New modules","text":"<ul> <li>new rt module: display ongoing tickets from RT queues</li> <li>new xsel module: display xsel buffers, by umbsublime</li> <li>new window_title_async module, by Anon1234</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/#modules-enhancements","title":"Modules enhancements","text":"<ul> <li>battery_level module: major improvements, documentation, add format option, by Maxim Baz</li> <li>keyboard_layout module: color customisation, add format option, by Ali Mousavi</li> <li>mpd_status module: fix connection leak, by Thomas Sanchez</li> <li>pomodoro module: implement format option and add additional display features, by Christoph Schober</li> <li>spotify module: fix support for new versions, by Jimmy Garpeh\u00e4ll</li> <li>spotify module: add support for colors output based on the playback status, by Sondre Lefsaker</li> <li>sysdata module: trim spaces in `cpu_temp`, by beetleman</li> <li>whatismyip module: change default check URL and make it configurable</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/#thanks","title":"Thanks !","text":"<p>Once again, thanks to all contributors listed above !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2015/2015-12-31-uwsgi-v2-0-12/","title":"uWSGI v2.0.12","text":"<p>It's been a long time since I made a blog post about a uWSGI release but this one is special to me because it contains some features I asked for to a colleague of mine.</p> <p>For his first contributions to a big Open Source project, our fellow @shir0kamii added two features (spooler_get_task and -if-hostname-match) which were backported in this release and that we needed at work for quite a long time : congratulations again :)</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2015/2015-12-31-uwsgi-v2-0-12/#highlights","title":"Highlights","text":"<ul> <li>official PHP 7 support</li> <li>uwsgi.spooler_get_task API to easily read and inspect a spooler file from your code</li> <li>-if-hostname-match regexp on uWSGI configuration files to allow a more flexible configuration based on the hostname of the machine</li> </ul> <p>Of course, all of this is already available on Gentoo Linux !</p> <p>Full changelog here as usual.</p>","tags":["gentoo","portage","release","uwsgi"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/","title":"Gentoo Linux on DELL XPS 13 9350","text":"<p>As I found little help about this online I figured I'd write a summary piece about my recent experience in installing Gentoo Linux on a DELL XPS 13 9350.</p> <p>EDIT: TL;DR if you need a working kernel config, you can also get my 4.12.5 kernel config here.</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#uefi-or-mbr","title":"UEFI or MBR ?","text":"<p>This machine ships with a NVME SSD so don't think twice : UEFI is the only sane way to go.</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#bios-configuration","title":"BIOS configuration","text":"<p>I advise to use the pre-installed Windows 10 to update the XPS to the latest BIOS (1.1.7 at the time of writing). Then you need to change some stuff to boot and get the NVME SSD disk discovered by the live CD.</p> <ul> <li>Turn off Secure Boot</li> <li>Set SATA Operation to AHCI (will break your Windows boot but who cares)</li> </ul>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#live-cd","title":"Live CD","text":"<p>Go for the latest SystemRescueCD (it's Gentoo based, you won't be lost) as it's quite more up to date and supports booting on UEFI. Make it a Live USB for example using unetbootin and the ISO on a vfat formatted USB stick.</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#nvme-ssd-disk-partitioning","title":"NVME SSD disk partitioning","text":"<p>We'll be using GPT with UEFI. I found that using gdisk was the easiest. The disk itself is found on /dev/nvme0n1. Here it is the partition table I used :</p> <ul> <li>500Mo UEFI boot partition (type EF00)</li> <li>16Go Swap partition</li> <li>60Go Linux root partition</li> <li>400Go home partition</li> </ul> <p>The corresponding gdisk commands :</p> <p># gdisk /dev/nvme0n1</p> <p>Command: o \u21b5 This option deletes all partitions and creates a new protective MBR. Proceed? (Y/N): y \u21b5</p> <p>Command: n \u21b5 Partition Number: 1 \u21b5 First sector: \u21b5 Last sector: +500M \u21b5 Hex Code: EF00 \u21b5</p> <p>Command: n \u21b5 Partition Number: 2 \u21b5 First sector: \u21b5 Last sector: +16G \u21b5 Hex Code: 8200 \u21b5</p> <p>Command: n \u21b5 Partition Number: 3 \u21b5 First sector: \u21b5 Last sector: +60G \u21b5 Hex Code: \u21b5</p> <p>Command: n \u21b5 Partition Number: 4 \u21b5 First sector: \u21b5 Last sector: \u21b5 (for rest of disk) Hex Code: \u21b5</p> <p>Command: w \u21b5 Do you want to proceed? (Y/N): Y \u21b5</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#no-wifi-on-live-cd-no-panic","title":"No WiFi on Live CD ? no panic","text":"<p>If your live CD is old (pre 4.4 kernel), the integrated broadcom 4350 wifi card won't be available !</p> <p>My trick was to use my Android phone connected to my local WiFi as a USB modem which was detected directly by the live CD.</p> <ul> <li>get your Android phone connected on your local WiFi (unless you want to use your cellular data)</li> <li>plug in your phone using USB to your XPS</li> <li>on your phone, go to Settings / More / Tethering &amp; portable hotspot</li> <li>enable USB tethering</li> </ul> <p>Running ip addr will show the network card enp0s20f0u2 (for me at least), then if no IP address is set on the card, just ask for one :</p> <p># dhcpcd enp0s20f0u2</p> <p>Et voil\u00e0, you have now access to the internet.</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#proceed-with-installation","title":"Proceed with installation","text":"<p>The only thing to worry about is to format the UEFI boot partition as FAT32.</p> <p># mkfs.vfat -F 32 /dev/nvme0n1p1</p> <p>Then follow the Gentoo handbook as usual for the next steps of the installation process until you arrive to the kernel and the bootloader / grub part.</p> <p>From this moment I can already say that NO we won't be using GRUB at all so don't bother installing it. Why ? Because at the time of writing, the efi-64 support of GRUB was totally not working at all as it failed to discover the NVME SSD disk on boot.</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#kernel-sources-and-consideration","title":"Kernel sources and consideration","text":"<p>The trick here is that we'll setup the boot ourselves directly from the BIOS later so we only need to build a standalone kernel (meaning able to boot without an initramfs).</p> <p>EDIT: as of Jan. 10 2016, kernel 4.4 is available on portage so you don't need the patching below any more !</p> <p>Make sure you install and use at least a 4.3.x kernel (4.3.3 at the time of writing). Add sys-kernel/gentoo-sources to your /etc/portage/package.keywords file if needed. If you have a 4.4 kernel available, you can skip patching it below.</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#patching-43x-kernels-for-broadcom-4350-wifi-support","title":"Patching 4.3.x kernels for Broadcom 4350 WiFi support","text":"<p>To get the broadcom 4350 WiFi card working on 4.3.x, we need to patch the kernel sources. This is very easy to do thanks to Gentoo's user patches support. Do this before installing gentoo-sources (or reinstall it afterwards).</p> <p>This example is for gentoo-sources-4.3.3, adjust your version accordingly :</p> <p>(chroot) # mkdir -p /etc/portage/patches/sys-kernel/gentoo-sources-4.3.3 (chroot) # cd /etc/portage/patches/sys-kernel/gentoo-sources-4.3.3 (chroot) # wget http://ultrabug.fr/gentoo/xps9350/0001-bcm4350.patch</p> <p>When emerging the gentoo-sources package, you should see the patch being applied. Check that it worked by issuing :</p> <p>(chroot) # grep BRCM_CC_4350 /usr/src/linux/drivers/net/wireless/brcm80211/brcmfmac/chip.c case BRCM_CC_4350_CHIP_ID:</p> <p>The resulting kernel module will be called brcmfmac, make sure to load it on boot by adding it in your /etc/conf.d/modules :</p> <p>modules=\"brcmfmac\"</p> <p>EDIT: as of Jan. 7 2016, version\u00a020151207 of linux-firmware ships with the needed files so you don't need to download those any more !</p> <p>Then we need to download the WiFi card's firmware files which are not part of the linux-firmware package at the time of writing (20150012).</p> <p>(chroot) # emerge '&gt;=sys-kernel/linux-firmware-20151207'</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#do-this-only-if-you-dont-have-sys-kernellinux-firmware-20151207-available","title":"DO THIS ONLY IF YOU DONT HAVE &gt;=sys-kernel/linux-firmware-20151207 available !","text":"<p>(chroot) # cd /lib/firmware/brcm/ (chroot) # wget http://ultrabug.fr/gentoo/xps9350/BCM-0a5c-6412.hcd (chroot) # wget http://ultrabug.fr/gentoo/xps9350/brcmfmac4350-pcie.bin</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#kernel-config-build","title":"Kernel config &amp; build","text":"<p>I used genkernel to build my kernel. I've done a very few adjustments but these are the things to mind in this pre-built kernel :</p> <ul> <li>support for NVME SSD added as builtin</li> <li>it is builtin for ext4 only (other FS are not compiled in)</li> <li>support for DM_CRYPT and LUKS ciphers for encrypted /home</li> <li>the root partition is hardcoded in the kernel as /dev/nvme0n1p3 so if yours is different, you'll need to change CONFIG_CMDLINE and compile it yourself</li> <li>the CONFIG_CMDLINE above is needed because you can't pass kernel parameters using UEFI so you have to hardcode them in the kernel itself</li> <li>support for the intel graphic card DRM and framebuffer (there's a kernel bug with skylake CPUs which will spam the logs but it still works good)</li> </ul> <p>Get the kernel config and compile it :</p> <p>EDIT: updated kernel config to 4.4.4 with SD Card support.</p> <p>(chroot) # mkdir -p /etc/kernels (chroot) # cd /etc/kernels (chroot) # wget http://ultrabug.fr/gentoo/xps9350/kernel-config-x86_64-4.4.4-gentoo (chroot) # genkernel kernel</p> <p>The proposed kernel config here is for gentoo-sources-4.4.4 so make sure to rename the file for your current version.</p> <p>EDIT: you can use and download a recent working kernel config at the beginning of the article.</p> <p>This kernel is far from perfect but it works very good so far, sound, webcam and suspend work smoothly !</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#makeconf-settings-for-intel-graphics","title":"make.conf settings for intel graphics","text":"<p>I can recommend using the following on your /etc/portage/make.conf :</p> <p>INPUT_DRIVERS=\"evdev synaptics\" VIDEO_CARDS=\"intel i965\"</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#fstab-for-ssd","title":"fstab for SSD","text":"<p>Don't forget to make sure the noatime option is used on your fstab for / and /home !</p> <p>/dev/nvme0n1p1    /boot    vfat    noauto,noatime    1 2 /dev/nvme0n1p2    none     swap    sw                0 0 /dev/nvme0n1p3    /        ext4    noatime   0 1 /dev/nvme0n1p4    /home    ext4    noatime   0 1</p> <p>As pointed out by stefantalpalaru on comments, it is recommended to schedule a SSD TRIM on your crontab once in a while, see Gentoo Wiki on SSD for more details.</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#encrypted-home-auto-mounted-at-login","title":"encrypted /home auto-mounted at login","text":"<p>I advise adding the cryptsetup to your USE variable in /etc/portage/make.conf and then updating your @world with a emerge -NDuq @world.</p> <p>I assume you don't have created your user yet so your unmounted /home is empty. Make sure that :</p> <ul> <li>your /dev/nvme0n1p4 home partition is not mounted</li> <li>you removed the corresponding /home line from your /etc/fstab (we'll configure pam_mount to get it auto-mounted on login)</li> </ul> <p>AFAIK, the LUKS password you'll set on the first slot when issuing luksFormat below should be the same as your user's password !</p> <p>(chroot) # cryptsetup luksFormat -s 512 /dev/nvme0n1p4 (chroot) # cryptsetup luksOpen /dev/nvme0n1p4 crypt_home (chroot) # mkfs.ext4 /dev/mapper/crypt_home (chroot) # mount /dev/mapper/crypt_home /home (chroot) # useradd -m -G wheel,audio,video,plugdev,portage,users USERNAME (chroot) # passwd USERNAME (chroot) # umount /home (chroot) # cryptsetup luksClose crypt_home</p> <p>We'll use sys-auth/pam_mount to manage the mounting of our /home partition when a user logs in successfully, so make sure you emerge pam_mount first, then configure the following files :</p> <ul> <li>/etc/security/pam_mount.conf.xml (only line added is the volume one)</li> </ul> <p> <pre><code>    &lt;!-- debug should come before everything else,\n    since this file is still processed in a single pass\n    from top-to-bottom --&gt;\n</code></pre> <p></p> <pre><code>    &lt;!-- Volume definitions --&gt;\n</code></pre> <p></p> <pre><code>    &lt;!-- pam_mount parameters: General tunables --&gt;\n</code></pre> <p></p> <p></p> <p></p> <pre><code>    &lt;!-- pam_mount parameters: Volume-related --&gt;\n</code></pre> <p></p> <p></p> <ul> <li>/etc/pam.d/system-auth (only lines added are the ones with pam_mount.so)</li> </ul> <p>auth        required    pam_env.so  auth        required    pam_unix.so try_first_pass likeauth nullok  auth        optional    pam_mount.so auth        optional    pam_permit.so</p> <p>account     required    pam_unix.so  account     optional    pam_permit.so</p> <p>password    optional    pam_mount.so password    required    pam_cracklib.so difok=2 minlen=8 dcredit=2 ocredit=2 retry=3  password    required    pam_unix.so try_first_pass use_authtok nullok sha512 shadow  password    optional    pam_permit.so</p> <p>session     optional    pam_mount.so session     required    pam_limits.so  session     required    pam_env.so  session     required    pam_unix.so  session     optional    pam_permit.so</p> <p>That's it, easy heh ?! When you login as your user, pam_mount will decrypt your home partition using your user's password and mount it on /home !</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#uefi-booting-your-gentoo-linux","title":"UEFI booting your Gentoo Linux","text":"<p>The best (and weird ?) way I found for booting the installed Gentoo Linux and its kernel is to configure the UEFI boot directly from the XPS BIOS.</p> <p>The idea is that the BIOS can read the files from the EFI boot partition since it is formatted as FAT32. All we have to do is create a new boot option from the BIOS and configure it to use the kernel file stored in the EFI boot partition.</p> <ul> <li>reboot your machine</li> <li>get on the BIOS (hit F2)</li> <li>get on the General / Boot Sequence menu</li> <li>click Add</li> <li>set a name (like Gentoo 4.3.3) and find + select the kernel file (use the integrated file finder)</li> </ul> <p></p> <ul> <li>remove all unwanted boot options</li> </ul> <p></p> <ul> <li>save it and reboot</li> </ul> <p>Your Gentoo kernel and OpenRC will be booting now !</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#suggestions-corrections-enhancements","title":"Suggestions, corrections, enhancements ?","text":"<p>As I said, I wrote all this quickly to spare some time to whoever it could help. I'm sure there are a lot of improvements to be done still so I'll surely update this article later on.</p>","tags":["9350","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/","title":"py3status v2.9","text":"<p>py3status v2.9 is out with a good bunch of new modules, exciting improvements and fixes !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/#thanks","title":"Thanks","text":"<p>This release is made of their stuff, thank you contributors !</p> <ul> <li>@4iar</li> <li>@AnwariasEu</li> <li>@cornerman</li> <li>Alexandre Bonnetain</li> <li>Alexis 'Horgix' Chotard</li> <li>Andrwe Lord Weber</li> <li>Ben Oswald</li> <li>Daniel Foerster</li> <li>Iain Tatch</li> <li>Johannes Karoff</li> <li>Markus Weimar</li> <li>Rail Aliiev</li> <li>Themistokle Benetatos</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/#new-modules","title":"New modules","text":"<ul> <li>arch_updates module, by Iain Tatch</li> <li>deadbeef module to show current track playing, by Themistokle Benetatos</li> <li>icinga2 module, by Ben Oswald</li> <li>scratchpad_async module, by johannes karoff</li> <li>wifi module, by Markus Weimar</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/#fixes-and-enhancements","title":"Fixes and enhancements","text":"<ul> <li>Rail Aliiev implement flake8 check via travis-ci, we now have a new build-passing badge</li> <li>fix: handle format_time tztime parameter thx to @cornerman, fix issue #177</li> <li>fix: respect ordering of the ipv6 i3status module even on empty configuration, fix #158 as reported by @nazco</li> <li>battery_level module: add multiple battery support, by 4iar</li> <li>battery_level module: added formatting options, by Alexandre Bonnetain</li> <li>battery_level module: added option hide_seconds, by Andrwe Lord Weber</li> <li>dpms module: added color support, by Andrwe Lord Weber</li> <li>spotify module: added format_down option, by Andrwe Lord Weber</li> <li>spotify module: fixed color &amp; playbackstatus check, by Andrwe Lord Weber</li> <li>spotify module: workaround broken dbus, removed PlaybackStatus query, by christian</li> <li>weather_yahoo module: support woeid, add more configuration parameters, by Rail Aliiev</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/#whats-next","title":"What's next ?","text":"<p>Some major core enhancements and code clean up are coming up thanks to @cornerman, @Horgix and @pydsigner. The next release will be faster than ever and even less CPU consuming !</p> <p>Meanwhile, this 2.9 release is available on pypi and Gentoo portage, have fun !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-06-25-py3status-v3-0/","title":"py3status v3.0","text":"<p>Oh boy, this new version is so amazing in terms of improvements and contributions that it's hard to sum it up !</p> <p>Before going into more explanations I want to dedicate this release to tobes whose contributions, hard work and patience have permitted this ambitious 3.0 : THANK YOU !</p> <p>This is the graph of contributed commits since 2.9 just so you realise how much this version is thanks to him: I can't continue on without also thanking Horgix who started this madness by splitting the code base into modular files and pydsigner for his everlasting contributions and code reviews !</p> <p>The git stat since 2.9 also speaks for itself:</p> <p>73 files changed, 7600 insertions(+), 3406 deletions(-)</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-06-25-py3status-v3-0/#so-whats-new","title":"So what's new ?","text":"<ul> <li>the monolithic code base have been split into modules responsible for the given tasks py3status performs</li> <li>major improvements on modules output orchestration and execution resulting in considerable CPU consumption reduction and i3bar responsiveness</li> <li>refactoring of user notifications with added dbus support and rate limiting</li> <li>improved modules error reporting</li> <li>py3status can now survive an i3status crash and will try to respawn it</li> <li>a new 'container' module output type gives the ability to group modules together</li> <li>refactoring of the time and tztime modules support brings the support of all the time macros (%d, %Z etc)</li> <li>support for stopping py3status and its modules when i3bar hide mode is used</li> <li>refactoring of general, contribution and most noticeably modules documentation</li> <li>more details on the rest of the changelog</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-06-25-py3status-v3-0/#modules","title":"Modules","text":"<p>Along with a cool list of improvements on the existing modules, these are the new modules:</p> <ul> <li>new group module to cycle display of several modules (check it out, it's insanely handy !)</li> <li>new fedora_updates module to check for your Fedora packages updates</li> <li>new github module to check a github repository and notifications</li> <li>new graphite module to check metrics from graphite</li> <li>new insync module to check your current insync status</li> <li>new timer module to have a simple countdown displayed</li> <li>new twitch_streaming module to check is a Twitch Streamer is online</li> <li>new vpn_status module to check your VPN status</li> <li>new xrandr_rotate module to rotate your screens</li> <li>new yandexdisk_status module to display Yandex.Disk status</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-06-25-py3status-v3-0/#contributors","title":"Contributors","text":"<p>And of course thank you to all the others who made this version possible !</p> <ul> <li>@egeskow</li> <li>Alex Caswell</li> <li>Johannes Karoff</li> <li>Joshua Pratt</li> <li>Maxim Baz</li> <li>Nathan Smith</li> <li>Themistokle Benetatos</li> <li>Vladimir Potapev</li> <li>Yongming Lai</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/","title":"RethinkDB on Gentoo Linux","text":"<p>It was about time I added a new package to portage and I'm very glad it to be RethinkDB and its python driver !</p> <ul> <li>dev-db/rethinkdb</li> <li>dev-python/python-rethinkdb</li> </ul> <p>For those of you who never heard about this database, I urge you to go about their excellent website and have a good read.</p>","tags":["gentoo","nosql","portage","release","rethinkdb"]},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/#packaging-rethinkdb","title":"Packaging RethinkDB","text":"<p>RethinkDB has been under my radar for quite a long time now and when they finally got serious enough about high availability I also got serious about using it at work... and obviously \"getting serious\" + \"work\" means packaging it for Gentoo Linux :)</p> <p>Quick notes on packaging for Gentoo Linux:</p> <ul> <li>This is a C++ project so it feels natural and easy to grasp</li> <li>The configure script already offers a way of using system libraries instead of the bundled ones which is in line with Gentoo's QA policy</li> <li>The only grey zone about the above statement is the web UI which is used precompiled</li> </ul> <p>RethinkDB has a few QA violations which the ebuild is addressing by modifying the sources:</p> <ul> <li>There is a configure.default which tries to force some configure options</li> <li>The configure is missing some options to avoid auto installing some docs and init scripts</li> <li>The build system does its best to guess the CXX compiler but it should offer an option to set it directly</li> <li>The build system does not respect users' CXXFLAGS and tries to force the usage of -03</li> </ul>","tags":["gentoo","nosql","portage","release","rethinkdb"]},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/#getting-our-hands-into-rethinkdb","title":"Getting our hands into RethinkDB","text":"<p>At work, we finally found the excuse to get our hands into RethinkDB when we challenged ourselves with developing a quizz game for our booth as a sponsor of Europython 2016.</p> <p>It was a simple game where you were presented a question and four possible answers and you had 60 seconds to answer as much of them as you could. The trick is that we wanted to create an interactive game where the participant had to play on a tablet but the rest of the audience got to see who was currently playing and follow their score progression + their ranking for the day and the week in real time on another screen !</p> <p>Another challenge for us in the creation of this game is that we only used technologies that were new to us and even switched jobs so the backend python guys would be doing the frontend javascript et vice et versa. The stack finally went like this :</p> <ul> <li>Game quizz frontend : Angular2 (TypeScript)</li> <li>Game questions API : Go</li> <li>Real time scores frontend : Angular2 + autobahn</li> <li>Real time scores API : python 3.5 asyncio + autobahn</li> <li>Database : RethinkDB</li> </ul> <p>As you can see on the stack we chose RethinkDB for its main strength : real time updates pushed to the connected clients. The real time scores frontend and API were bonded together using autobahn while the API was using the changefeeds (realtime updates coming from the database) and broadcasting them to the frontend.</p>","tags":["gentoo","nosql","portage","release","rethinkdb"]},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/#what-we-learnt-about-rethinkdb","title":"What we learnt about RethinkDB","text":"<ul> <li>We're sure that we want to use it in production !</li> <li>The ReQL query language is a pipeline so its syntax is quite tricky to get familiar with (even more when coming from mongoDB like us), it is as powerful as it can be disconcerting</li> <li>Realtime changefeeds have limitations which are sometimes not so easy to understand/find out (especially the order_by / secondary index part)</li> <li>Changefeeds limitations is a constraint you have to take into account in your data modeling !</li> <li>Changefeeds + order_by can do the ordering for you when using the include_offsets option, this is amazing</li> <li>The administration web UI is awesome</li> <li>The python 3.5 asyncio proper support is still not merged, this is a pain !</li> </ul>","tags":["gentoo","nosql","portage","release","rethinkdb"]},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/#try-it-out","title":"Try it out","text":"<p>Now that you can emerge rethinkdb I encourage you to try this awesome database.</p> <p>Be advised that the ebuild also provides a way of configuring your rethinkdb instance by running emerge --config dev-db/rethinkdb !</p> <p>I'll now try to get in touch with upstream to get Gentoo listed on their website.</p>","tags":["gentoo","nosql","portage","release","rethinkdb"]},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/","title":"py3status v3.3","text":"<p>Ok I slacked by not posting for v3.1 and v3.2 and I should have since those previous versions were awesome and feature rich.</p> <p>But v3.3 is another major milestone which was made possible by tremendous contributions from @tobes as usual and also greatly thanks to the hard work of @guiniol and @pferate who I'd like to mention and thank again !</p> <p>Also, I'd like to mention that @tobes has become the first collaborator of the py3status project !</p> <p>Instead of doing a changelog review, I'll highlight some of the key features that got introduced and extended during those versions.</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#the-py3-helper","title":"The py3 helper","text":"<p>Writing powerful py3status modules have never been so easy thanks to the py3 helper !</p> <p>This magical object is added automatically to modules and provides a lot of useful methods to help normalize and enhance modules capabilities. This is a non exhaustive list of such methods:</p> <ul> <li>format_units: to pretty format units (KB, MB etc)</li> <li>notify_user: send a notification to the user</li> <li>time_in: to handle module cache expiration easily</li> <li>safe_format: use the extended formatter to handle the module's output in a powerful way (see below)</li> <li>check_commands: check if the listed commands are available on the system</li> <li>command_run: execute the given command</li> <li>command_output: execute the command and get its output</li> <li>play_sound: sound notifications !</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#powerful-control-over-the-modules-output","title":"Powerful control over the modules' output","text":"<p>Using the self.py3.safe_format helper will unleash a feature rich formatter that one can use to conditionally select the output of a module based on its content.</p> <ul> <li>Square brackets <code>[]</code> can be used. The content of them will be removed from the output if there is no valid placeholder contained within. They can also be nested.</li> <li>A pipe (vertical bar) <code>|</code> can be used to divide sections the first valid section only will be shown in the output.</li> <li>A backslash <code>\\</code> can be used to escape a character eg <code>[</code> will show <code>[</code> in the output.</li> <li><code>\\?</code> is special and is used to provide extra commands to the format string, example <code>\\?color=#FF00FF</code>. Multiple commands can be given using an ampersand <code>&amp;</code> as a separator, example <code>\\?color=#FF00FF&amp;show</code>.</li> <li><code>{&lt;placeholder&gt;}</code> will be converted, or removed if it is None or empty. Formatting can also be applied to the placeholder eg <code>{number:03.2f}</code>.</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#example-format_string","title":"Example format_string:","text":"<p>This will show <code>artist - title</code> if artist is present, <code>title</code> if title but no artist, and <code>file</code> if file is present but not artist or title.</p> <p>\"[[{artist} - ]{title}]|{file}\"</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#more-code-and-documentation-tests","title":"More code and documentation tests","text":"<p>A lot of efforts have been put into py3status automated CI and feature testing allowing more confidence in the advanced features we develop while keeping a higher standard on code quality.</p> <p>This is such as even modules' docstrings are now tested for bad formatting :)</p> <p>Colouring and thresholds</p> <p>A special effort have been put in normalizing modules' output colouring with the added refinement of normalized thresholds to give users more power over their output.</p> <ul> <li>See the colouring documentation</li> <li>See the thresholds documentation</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#new-modules-on-and-on","title":"New modules, on and on !","text":"<ul> <li>new clock module to display multiple times and dates informations in a flexible way, by @tobes</li> <li>new coin_balance module to display balances of diverse crypto-currencies, by Felix Morgner</li> <li>new diskdata module to shows both usage data and IO data from disks, by @guiniol</li> <li>new exchange_rate module to check for your favorite currency rates, by @tobes</li> <li>new file_status module to check the presence of a file, by @ritze</li> <li>new frame module to group and display multiple modules inline, by @tobes</li> <li>new gpmdp module for Google Play Music Desktop Player by @Spirotot</li> <li>new kdeconnector module to display information about Android devices, by @ritze</li> <li>new mpris module to control MPRIS enabled music players, by @ritze</li> <li>new net_iplist module to display interfaces and their IPv4 and IPv6 IP addresses, by @guiniol</li> <li>new process_status module to check the presence of a process, by @ritze</li> <li>new rainbow module to enlight your day, by @tobes</li> <li>new tcp_status module to check for a given TCP port on a host, by @ritze</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#changelog","title":"Changelog","text":"<p>The changelog is very big and the next 3.4 milestone is very promising with amazing new features giving you even more power over your i3bar, stay tuned !</p>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#thank-you-contributors","title":"Thank you contributors","text":"<p>Still a lot of new timer contributors which I take great pride in as I see it as py3status being an accessible project.</p> <ul> <li>@btall</li> <li>@chezstov</li> <li>@coxley</li> <li>Felix Morgner</li> <li>Gabriel F\u00e9ron</li> <li>@guiniol</li> <li>@inclementweather</li> <li>@jakubjedelsky</li> <li>Jan Mr\u00e1zek</li> <li>@m45t3r</li> <li>Maxim Baz</li> <li>@pferate</li> <li>@ritze</li> <li>@rixx</li> <li>@Spirotot</li> <li>@Stautob</li> <li>@tjaartvdwalt</li> <li>Yuli Khodorkovskiy</li> <li>@ZeiP</li> </ul>","tags":["gentoo","py3status","release"]},{"location":"Tech%20Blog/2017/2017-01-27-py3status-v3-4/","title":"py3status v3.4","text":"<p>Another community driven and incredible update of py3status has been released !</p> <p>Our contributor star for this release is without doubt @lasers who is showing some amazing energy with challenging ideas and some impressive modules QA clean ups !</p> <p>Thanks a lot as usual to @tobes who is basically leading the development of py3status now days with me being in a merge button mode most of the time.</p> <p>By looking at the issues and pull requests I can already say that the 3.5 release will be grand !</p>","tags":["gentoo","portage","py3status"]},{"location":"Tech%20Blog/2017/2017-01-27-py3status-v3-4/#highlights","title":"Highlights","text":"<ul> <li>support of python 3.6 thanks to @tobes</li> <li>a major effort in modules standardization, almost all of them support the format parameter now thanks to @lasers</li> <li>modules documentation has been cleaned up</li> <li>new do_not_disturb module to toggle notifications, by @maximbaz</li> <li>new rss_aggregator module to display your unread feed items, by @raspbeguy</li> <li>whatsmyip module: added geolocation support using ip-api.com, by @vicyap with original code from @neutronst4r</li> </ul> <p>See the full changelog here.</p> <p>Thank you guys !</p>","tags":["gentoo","portage","py3status"]},{"location":"Tech%20Blog/2017/2017-04-10-py3status-v3-5/","title":"py3status v3.5","text":"<p>Howdy folks,</p> <p>I'm obviously slacking a bit on my blog and I'm ashamed to say that it's not the only place where I do. py3status is another of them and it wouldn't be the project it is today without @tobes.</p> <p>In fact, this new 3.5 release has witnessed his takeover on the top contributions on the project, so I want to extend a warm thank you and lots of congratulations on this my friend :)</p> <p>Also, an amazing new contributor from the USA has come around in the nickname of @lasers. He has been doing a tremendous job on module normalization, code review and feedbacks. His high energy is amazing and more than welcome.</p> <p>This release is mainly his, so thank you @lasers !</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-04-10-py3status-v3-5/#whats-new","title":"What's new ?","text":"<p>Well the changelog has never been so large that I even don't know where to start. I guess the most noticeable change is the gorgeous and brand new documentation of py3status on readthedocs !</p> <p>Apart from the enhanced guides and sections, what's amazing behind this new documentation is the level of automation efforts that @lasers and @tobes put into it. They even generate modules' screenshots programmatically ! I would never have thought of it possible :D</p> <p></p> <p>The other main efforts on this release is about modules normalization where @lasers put so much energy in taking advantage of the formatter features and bringing all the modules to a new level of standardization. This long work brought to light some lack of features or bugs which got corrected along the way.</p> <p>Last but not least, the way py3status notifies you when modules fail to load/execute got changed. Now modules which fail to load or execute will not pop up a notification (i3 nagbar or dbus) but display directly in the bar where they belong. Users can left click to show the error and right click to discard them from their bar !</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-04-10-py3status-v3-5/#new-modules","title":"New modules","text":"<p>Once again, new and recurring contributors helped the project get better and offer a cool set of modules, thank you contributors !</p> <ul> <li>air_quality module, to display the air quality of your place, by @beetleman and @lasers</li> <li>getjson module to display fields from a json url, by @vicyap</li> <li>keyboard_locks module to display keyboard locks states, by @lasers</li> <li>systemd module to check the status of a systemd unit, by @adrianlzt</li> <li>tor_rate module to display the incoming and outgoing data rates of a Tor daemon instance, by @fmorgner</li> <li>xscreensaver module, by @lasers and @neutronst4r</li> </ul> <p>Special mention to @maximbaz for his continuous efforts and help. And also a special community mention to @valdur55 for his responsiveness and help for other users on IRC !</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-04-10-py3status-v3-5/#whats-next","title":"What's next ?","text":"<p>The 3.6 version will focus on the following ideas, some sane and some crazy :)</p> <ul> <li>we will continue to work on the ability to add/remove/move modules in the bar at runtime</li> <li>i3blocks and i3pystatus support, to embed their configurations and modules inside py3status</li> <li>formatter optimizations</li> <li>finish modules normalization</li> <li>write more documentation and clean up the old ones</li> </ul> <p>Stay tuned</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/","title":"Hardening SSH authentication using Yubikey (1/2)","text":"<p>I recently worked a bit at how we could secure better our SSH connections to our servers at work.</p> <p>So far we are using the OpenSSH public key only mechanism which means that there is no password set on the servers for our users. While this was satisfactory for a time we think that this still suffers some disadvantages such as:</p> <ul> <li>we cannot enforce SSH private keys to have a passphrase on the user side</li> <li>the security level of the whole system is based on the protection of the private key which means that it's directly tied to the security level of the desktop of the users</li> </ul> <p>This lead us to think about adding a 2nd factor authentication to SSH and about the usage of security keys.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#meet-the-yubikey","title":"Meet the Yubikey","text":"<p>Yubikeys are security keys made by Yubico. They can support multiple modes and work with the U2F open authentication standard which is why they got my attention.</p> <p>I decided to try the Yubikey 4 because it can act as a smartcard while offering these interesting features:</p> <ul> <li>Challenge-Response</li> <li>OTP</li> <li>GPG</li> <li>PIV</li> </ul> <p></p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#method-1-ssh-using-pam_ssh-pam_yubico","title":"Method 1 - SSH using pam_ssh + pam_yubico","text":"<p>The first method I found satisfactory was to combine pam_ssh authentication module along with pam_yubico as a 2nd factor. This allows server side passphrase enforcement on SSH and the usage of the security key to login.</p> <p>TL;DR: two gotchas before we begin</p> <ul> <li>This method is still quite complex to setup but allows you to use the cheaper U2F only Yubikeys.</li> <li>The second method based on PIV is the solution we chose as it fits better our use cases and ecosystem.</li> </ul> <p>Note</p> <p>Keep a root SSH session to your servers while deploying/testing this so you can revert any change you make and avoid to lock yourself out of your servers.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#setup-pam_ssh","title":"Setup pam_ssh","text":"<p>Use pam_ssh on the servers to force usage of a passphrase on a private key. The idea behind pam_ssh is that the passphrase of your SSH key serves as your SSH password.</p> <p>Generate your SSH key pair with a passphrase on your local machine.</p> <pre><code>$ ssh-keygen -f identity\nGenerating public/private rsa key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in identity.\nYour public key has been saved in identity.pub.\nThe key fingerprint is:\nSHA256:a2/HNCe28+bpMZ2dIf9bodnBwnmD7stO5sdBOV6teP8 alexys@yazd\nThe key's randomart image is:\n+---[RSA 2048]----+\n|                 |\n|                 |\n|                o|\n|            . ++o|\n|        S    BoOo|\n|         .  B %+O|\n|        o  + %+\\*=|\n|       . .. @ .\\*+|\n|         ....%B.E|\n+----[SHA256]-----+\n</code></pre> <p>You then must copy your private key (named identity with no extension) to your servers under\u00a0 the ~/.ssh/login-keys.d/ folder.</p> <p>In your $HOME on the servers, you will get something like this:</p> <pre><code>.ssh/\n\u251c\u2500\u2500 known_hosts\n\u2514\u2500\u2500 login-keys.d\n    \u2514\u2500\u2500 identity\n</code></pre> <p>Then you can enable the pam_ssh authentication. Gentoo users should enable the\u00a0pam_ssh USE flag for sys-auth/pambase and re-install.</p> <p>Add this at the beginning of the file /etc/pam.d/ssh</p> <pre><code>auth    required    pam_ssh.so debug\n</code></pre> <p>The debug flag can be removed after you tested it correctly.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#disable-public-key-authentication","title":"Disable public key authentication","text":"<p>Because it takes precedence over the PAM authentication mechanism, you have to disable OpenSSH PubkeyAuthentication authentication on /etc/ssh/sshd_config:</p> <pre><code>PubkeyAuthentication no\n</code></pre> <p>Enable PAM authentication on /etc/ssh/sshd_config</p> <pre><code>ChallengeResponseAuthentication no\nPasswordAuthentication no\nUsePAM yes\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#test-pam_ssh","title":"Test pam_ssh","text":"<p>Now you should be prompted for your SSH passphrase to login through SSH.</p> <pre><code>$ ssh cheetah\nSSH passphrase:\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#setup-2nd-factor-using-pam_yubico","title":"Setup 2nd factor using pam_yubico","text":"<p>Now we will make use of our Yubikey security key to add a 2nd factor authentication to login through SSH on our servers.</p> <p>Because the Yubikey is not physically plugged on the server, we cannot use an offline Challenge-Response mechanism, so we will have to use a third party to validate the challenge. Yubico gracefully provide an API for this and the pam_yubico module is meant to use it easily.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#preparing-your-account-using-your-yubikey-on-your-machine","title":"Preparing your account using your Yubikey (on your machine)","text":"<p>First of all, you need to get your Yubico API key ID from the following URL:</p> <ul> <li>https://upgrade.yubico.com/getapikey/</li> <li>enter your email address</li> <li>tap your Yubikey to generate an OTP in the Yubikey OTP field</li> </ul> <p>You will get a Client ID (this you will use) and Secret Key (this you will keep safe).</p> <p>Then you will need to create an authorization mapping file which basically link your account to a Yubikey fingerprint (modhex). This is equivalent to saying \"this Yubikey belongs to this user and can authenticate him\".</p> <p>First, get your modhex:</p> <ul> <li>go to https://demo.yubico.com/php-yubico/Modhex_Calculator.php</li> <li>select OTP</li> <li>tap your key</li> <li>click \"convert all formats\"</li> <li>the modhex is listed as Modhex encoded: xxccccxxuuxx</li> </ul> <p>Using this modhex, create your mapping file named\u00a0authorized_yubikeys which will be copied to ~/.yubico/authorized_yubikeys on the servers (replace LOGIN_USERNAME with your actual account login name).</p> <pre><code>LOGIN_USERNAME:xxccccxxuuxx\n</code></pre> <p>NOTE: this mapping file can be a centralized one (in /etc for example) to handle all the users from a server. See the the authfile option on the doc.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#setting-up-openssh-on-your-servers","title":"Setting up OpenSSH (on your servers)","text":"<p>You must install pam_yubico on the servers. For Gentoo, it's as simple as:</p> <pre><code># emerge sys-auth/pam_yubico\n</code></pre> <p>Copy your authentication mapping file to your home under the .yubico folder on all servers. You should get this:</p> <pre><code>.yubico/\n\u2514\u2500\u2500 authorized_yubikey\n</code></pre> <p>Configure pam to use pam_yubico. Add this after the pam_ssh on the file /etc/pam.d/ssh which should look like this now:</p> <pre><code>auth    required    pam_ssh.so\nauth    required    pam_yubico.so id=YOUR_API_ID debug debug_file=/var/log/auth.log\n</code></pre> <p>The debug and debug_file flags can be removed after you tested it correctly.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#testing-pam_yubico","title":"Testing pam_yubico","text":"<p>Now you should be prompted for your SSH passphrase and then for your Yubikey OTP to login through SSH.</p> <pre><code>$ ssh cheetah\nSSH passphrase:\nYubiKey for \\`ultrabug':\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#about-the-yubico-api-dependency","title":"About the Yubico API dependency","text":"<p>Careful readers will notice that using pam_yubico introduces a strong dependency on the Yubico API availability. If the API becomes unreachable or your internet connection goes down then your servers would be unable to authenticate you!</p> <p>The solution I found to this problem is to instruct pam to ignore the Yubikey authentication when pam_yubico is unable to contact the API.</p> <p>In this case, the module will return a AUTHINFO_UNAVAIL code to PAM which we can act upon using the following syntax. The /etc/pam.d/ssh first lines should be changed to this:</p> <pre><code>auth    required    pam_ssh.so\nauth    [success=done authinfo_unavail=ignore new_authtok_reqd=done default=die]    pam_yubico.so id=YOUR_API_ID debug debug_file=/var/log/auth.log\n</code></pre> <p>Now you can be sure to be able to use your Yubikey even if the API is down or unreachable ;)</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/","title":"Hardening SSH authentication using Yubikey (2/2)","text":"<p>In my previous blog post, I demonstrated how to use a Yubikey to add a 2nd factor (2FA) authentication to SSH using pam_ssh and pam_yubico.</p> <p>In this article, I will go further and demonstrate another method using Yubikey's Personal Identity Verification (PIV) capability.</p> <p>This one has the huge advantage to allow a 2nd factor authentication while using the public key authentication mechanism of OpenSSH and thus does not need any kind of setup on the servers.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#method-2-ssh-using-yubikey-and-piv","title":"Method 2 - SSH using Yubikey and PIV","text":"<p>Yubikey 4 and NEO also act as smartcards supporting the PIV standard which allows you to store a private key on your security key through PKCS#11. This is an amazing feature which is also very good for our use case.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#tools-installation","title":"Tools installation","text":"<p>For this to work, we will need some tools on our local machines to setup our Yubikey correctly.</p> <p>Gentoo users should install those packages:</p> <pre><code># emerge dev-libs/opensc sys-auth/ykpers sys-auth/yubico-piv-tool sys-apps/pcsc-lite app-crypt/ccid sys-apps/pcsc-tools sys-auth/yubikey-personalization-gui\n</code></pre> <p>Gentoo users should also allow the pcscd service to be hotplugged (started automatically upon key insertion) by modifying their /etc/rc.conf and having:</p> <pre><code>rc_hotplug=\"pcscd\"\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#yubikey-setup","title":"Yubikey setup","text":"<p>The idea behind the Yubikey setup is to generate and store a private key in our Yubikey and to secure it via a PIN code.</p> <p>First, insert your Yubikey and let's change its USB operating mode to OTP+CCID.</p> <pre><code>$ ykpersonalize -m2\nFirmware version 4.3.4 Touch level 783 Program sequence 3\n\nThe USB mode will be set to: 0x2\n\nCommit? (y/n) [n]: y\n</code></pre> <p>Then, we will create a new management key:</p> <pre><code>key=`dd if=/dev/random bs=1 count=24 2&gt;/dev/null | hexdump -v -e '/1 \"%02X\"'`\necho $key\nD59E46FE263DDC052A409C68EB71941D8DD0C5915B7C143A\n</code></pre> <p>Replace the default management key (if prompted, copy/paste the key printed above):</p> <pre><code>$ yubico-piv-tool -a set-mgm-key -n $key --key 010203040506070801020304050607080102030405060708\n</code></pre> <p>Then change the default PIN code and PUK code of your Yubikey</p> <pre><code>$ yubico-piv-tool -a change-pin -P 123456 -N &lt;NEW PIN&gt;\n\n$ yubico-piv-tool -a change-puk -P 12345678 -N &lt;NEW PUK&gt;\n</code></pre> <p>Now that your Yubikey is secure, let's proceed with the PCKS#11 certificate generation. You will be prompted for your management key that you generated before.</p> <pre><code>$ yubico-piv-tool -s 9a -a generate -o public.pem -k\n</code></pre> <p>Then create a self-signed certificate (only used for libpcks11) and import it in the Yubikey:</p> <pre><code>$ yubico-piv-tool -a verify-pin -a selfsign-certificate -s 9a -S \"/CN=SSH key/\" -i public.pem -o cert.pem\n$ yubico-piv-tool -a import-certificate -s 9a -i cert.pem\n</code></pre> <p>Here you are! You can now export your public key to use with OpenSSH:</p> <pre><code>$ ssh-keygen -D opensc-pkcs11.so -e\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWtqI37jwxYMJ9XLq9VwHgJlhZViPVAGIUfMm8SAlfs6cka4Cj570lkoGK04r8JAVJFy/iKfhGpL9N9XuartfIoq6Cg/6Qvg3REupuqs51V2cBaC/gnWIQ7qZqlzBulvcOvzNfHFD/lX42J58+E8tWnYg6GzIsImFZQVpmI6SxNfSmVQIqxIufInrbQaI+pKXntdTQC9wyNK5FAA8TXAdff5ZDnmetsOTVble9Ia5m6gqM7MnxNZ56uDpn+6lCxRZSW+Ln2PDE7sivVcST4qpfwY4P4Lrb3vrjCGODFg4xmGNKXsLi2+uZbs5rW7bg4HFO50kKDucPV1M+rBWA9999\n</code></pre> <p>Copy to your servers your SSH public key to your usual ~/.ssh/authorized_keys file in your $HOME.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#testing-piv-secured-ssh","title":"Testing PIV secured SSH","text":"<p>Plug-in your Yubikey, and then SSH to your remote server using the opensc-pkcs11 library. You will be prompted for your PIN and then successfully logged in :)</p> <pre><code>$ ssh -I opensc-pkcs11.so cheetah\nEnter PIN for 'PIV_II (PIV Card Holder pin)':\n</code></pre> <p>You can then configure SSH to use it by default for all your hosts in your ~/.ssh/config</p> <pre><code>Host=\\*\nPKCS11Provider /usr/lib/opensc-pkcs11.so\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#using-piv-with-ssh-agent","title":"Using PIV with ssh-agent","text":"<p>You can also use ssh-agent to avoid typing your PIN every time.</p> <p>When asked for the passphrase, enter your PIN:</p> <pre><code>$ ssh-add -s /usr/lib/opensc-pkcs11.so\nEnter passphrase for PKCS#11:\nCard added: /usr/lib/opensc-pkcs11.so\n</code></pre> <p>You can verify that it worked by listing the available keys in your ssh agent:</p> <pre><code>$ ssh-add -L\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWtqI37jwxYMJ9XLq9VwHgJlhZViPVAGIUfMm8SAlfs6cka4Cj570lkoGK04r8JAVJFy/iKfhGpL9N9XuartfIoq6Cg/6Qvg3REupuqs51V2cBaC/gnWIQ7qZqlzBulvcOvzNfHFD/lX42J58+E8tWnYg6GzIsImFZQVpmI6SxNfSmVQIqxIufInrbQaI+pKXntdTQC9wyNK5FAA8TXAdff5ZDnmetsOTVble9Ia5m6gqM7MnxNZ56uDpn+6lCxRZSW+Ln2PDE7sivVcST4qpfwY4P4Lrb3vrjCGODFg4xmGNKXsLi2+uZbs5rW7bg4HFO50kKDucPV1M+rBWA9999 /usr/lib64/opensc-pkcs11.so\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#enjoy","title":"Enjoy!","text":"<p>Now you have a flexible yet robust way to authenticate your users which you can also extend by adding another type of authentication on your servers using PAM.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/","title":"Hardening SSH authentication using Yubikey (3/2)","text":"<p>In my previous blog post, I demonstrated how to use the PIV feature of a Yubikey to add a 2nd factor authentication to SSH.</p> <p>Careful readers such as Grzegorz Kulewski pointed out that using the GPG capability of the Yubikey was also a great, more versatile and more secure option on the table (I love those community insights):</p> <ul> <li>GPG keys and subkeys are indeed more flexible and can be used for case-specific operations (signing, encryption, authentication)</li> <li>GPG is more widely used and one could use their Yubikey smartcard for SSH, VPN, HTTP auth and code signing</li> <li>The Yubikey 4 GPG feature supports 4096 bit keys (limited to 2048 for PIV)</li> </ul> <p>While I initially looked at the GPG feature, its apparent complexity got me to discard it for my direct use case (SSH). But I couldn't resist the good points of Grzegorz and here I got back into testing it. Thank you again\u00a0Grzegorz for the excuse you provided ;)</p> <p>So let's get through with the GPG feature of the Yubikey to authenticate our SSH connections. Just like the PIV method, this one has the\u00a0 advantage to allow a 2nd factor authentication while using the public key authentication mechanism of OpenSSH and thus does not need any kind of setup on the servers.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#method-3-ssh-using-yubikey-and-gpg","title":"Method 3 - SSH using Yubikey and GPG","text":"","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#acknowledgement","title":"Acknowledgement","text":"<p>The first choice you have to make is to decide whether you allow your master key to be stored on the Yubikey or not. This choice will be guided by how you plan to use and industrialize your usage of the GPG based SSH authentication.</p> <p>Consider this to choose whether to store the master key on the Yubikey or not:</p> <ul> <li>(con) it will not allow the usage of the same GPG key on multiple Yubikeys</li> <li>(con) if you loose your Yubikey, you will have to revoke your entire GPG key and start from scratch (since the secret key is stored on the Yubikey)</li> <li>(pro) by storing everything on the Yubikey, you won't necessary have to have an offline copy of your master key (and all the process that comes with it)</li> <li>(pro) it is easier to generate and store everything on the key and is then a good starting point for new comers or rare GPG users</li> </ul> <p>Because I want to demonstrate and enforce the most straightforward way of using it, I will base this article on generating and storing everything on a Yubikey 4. You can find useful links at the end of the article pointing to reference on how to do it differently.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#tools-installation","title":"Tools installation","text":"<p>For this to work, we will need some tools on our local machine to setup our Yubikey correctly.</p> <p>Gentoo users should install those packages:</p> <pre><code># emerge -av dev-libs/opensc sys-auth/ykpers app-crypt/ccid sys-apps/pcsc-tools app-crypt/gnupg\n</code></pre> <p>Gentoo users should also allow the pcscd service to be hotplugged (started automatically upon key insertion) by modifying their /etc/rc.conf and having:</p> <pre><code>rc_hotplug=\"pcscd\"\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#yubikey-setup","title":"Yubikey setup","text":"<p>The idea behind the Yubikey setup is to generate and store the GPG keys directly on our Yubikey and to secure them via a PIN code (and an admin PIN code).</p> <ul> <li>default PIN code: 123456</li> <li>default admin PIN code: 12345678</li> </ul>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#usb-operating-mode","title":"USB operating mode","text":"<p>First, insert your Yubikey and let's change its USB operating mode to OTP+U2F+CCID with MODE_FLAG_EJECT flag.</p> <pre><code>$ ykpersonalize -m86\n\nFirmware version 4.3.4 Touch level 783 Program sequence 3\n\nThe USB mode will be set to: 0x86\n\nCommit? (y/n) [n]: y\n</code></pre> <p>Note</p> <p>If you have an older version of Yubikey (before Sept. 2014), use <code>-m82</code> instead.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#generate-a-new-gpg-key-on-the-yubikey","title":"Generate a new GPG key on the Yubikey","text":"<p>Let's open the smartcard for edition.</p> <pre><code>$ gpg --card-edit --expert\n\nReader ...........: Yubico Yubikey 4 OTP U2F CCID (0005435106) 00 00\nApplication ID ...: A7560001240102010006054351060000\nVersion ..........: 2.1\nManufacturer .....: Yubico\nSerial number ....: 75435106\nName of cardholder: [not set]\nLanguage prefs ...: [not set]\nSex ..............: unspecified\nURL of public key : [not set]\nLogin data .......: [not set]\nSignature PIN ....: forced\nKey attributes ...: rsa2048 rsa2048 rsa2048\nMax. PIN lengths .: 127 127 127\nPIN retry counter : 3 0 3\nSignature counter : 0\nSignature key ....: [none]\nEncryption key....: [none]\nAuthentication key: [none]\nGeneral key info..: [none]\n</code></pre> <p>Then switch to admin mode.</p> <pre><code>gpg/card&gt; admin\nAdmin commands are allowed\n</code></pre> <p>We can start generating the Signature, Encryption and Authentication keys on the Yubikey. During the process, you will be prompted alternatively for the PIN and admin PIN.</p> <pre><code>gpg/card&gt; generate\nMake off-card backup of encryption key? (Y/n)\n\nPlease note that the factory settings of the PINs are\n    PIN = '123456'     Admin PIN = '12345678'\nYou should change them using the command --change-pin\n</code></pre> <p>Note</p> <p>I advise you say Yes to the off-card backup of the encryption key.</p> <p>Yubikey 4 users can choose a 4096 bits key, let's go for it for every key type.</p> <pre><code>What keysize do you want for the Signature key? (2048) 4096\nThe card will now be re-configured to generate a key of 4096 bits\nNote: There is no guarantee that the card supports the requested size.\n      If the key generation does not succeed, please check the\n      documentation of your card to see what sizes are allowed.\n\nWhat keysize do you want for the Encryption key? (2048) 4096\nThe card will now be re-configured to generate a key of 4096 bits\n\nWhat keysize do you want for the Authentication key? (2048) 4096\nThe card will now be re-configured to generate a key of 4096 bits\n</code></pre> <p>Then you're asked for the expiration of your key. I choose 1 year but it's up to you (leave 0 for no expiration).</p> <pre><code>Please specify how long the key should be valid.\n         0 = key does not expire\n      &lt;n&gt;  = key expires in n days\n      &lt;n&gt;w = key expires in n weeks\n      &lt;n&gt;m = key expires in n months\n      &lt;n&gt;y = key expires in n years\nKey is valid for? (0) 1y\nKey expires at mer. 15 mai 2018 21:42:42 CEST\nIs this correct? (y/N) y\n</code></pre> <p>Finally you give GnuPG details about your user ID and you will be prompted for a passphrase (make it strong).</p> <pre><code>GnuPG needs to construct a user ID to identify your key.\n\nReal name: Ultrabug\nEmail address: ultrabug@nospam.com\nComment:\nYou selected this USER-ID:\n    \"Ultrabug &lt;ultrabug@nospam.com&gt;\"\n\nChange (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\n</code></pre> <p>If you chose to make an off-card backup of your key, you will also get notified of its location as well the revocation certificate.</p> <pre><code>gpg: Note: backup of card key saved to '/home/ultrabug/.gnupg/sk_8E407636C9C32C38.gpg'\ngpg: key 22A73AED8E766F01 marked as ultimately trusted\ngpg: revocation certificate stored as '/home/ultrabug/.gnupg/openpgp-revocs.d/A1580FD98C0486D94C1BE63B22A73AED8E766F01.rev'\npublic and secret key created and signed.\n</code></pre> <p>Make sure to store that backup in a secure and offline location!</p> <p>You can verify that everything went good and take this chance to note the public key ID.</p> <pre><code>gpg/card&gt; verify\n\nReader ...........: Yubico Yubikey 4 OTP U2F CCID (0001435106) 00 00\nApplication ID ...: A7560001240102010006054351060000\nVersion ..........: 2.1\nManufacturer .....: Yubico\nSerial number ....: 75435106\nName of cardholder: [not set]\nLanguage prefs ...: [not set]\nSex ..............: unspecified\nURL of public key : [not set]\nLogin data .......: [not set]\nSignature PIN ....: forced\nKey attributes ...: rsa4096 rsa4096 rsa4096\nMax. PIN lengths .: 127 127 127\nPIN retry counter : 3 0 3\nSignature counter : 4\nSignature key ....: A158 0FD9 8C04 86D9 4C1B E63B 22A7 3AED 8E76 6F01\n created ....: 2017-05-16 20:43:17\nEncryption key....: E1B6 7009 907D 1D94 B200 37D7 8E40 7636 C9C3 2C38\n created ....: 2017-05-16 20:43:17\nAuthentication key: AAED AB8E E055 41B2 EFFF 62A4 164F 873A 75D2 AD6B\n created ....: 2017-05-16 20:43:17\nGeneral key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug &lt;ultrabug@nospam.com&gt;\nsec&gt; rsa4096/22A73AED8E766F01 created: 2017-05-16 expires: 2018-05-16\n card-no: 0001 05435106\nssb&gt; rsa4096/164F873A75D2AD6B created: 2017-05-16 expires: 2018-05-16\n card-no: 0001 05435106\nssb&gt; rsa4096/8E407636C9C32C38 created: 2017-05-16 expires: 2018-05-16\n card-no: 0001 05435106\n</code></pre> <p>You'll find the public key ID on the \"General key info\" line (22A73AED8E766F01):</p> <pre><code>General key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug &lt;ultrabug@nospam.com&gt;\n\nQuit the card edition.\n\ngpg/card&gt; quit\n</code></pre> <p>It is then convenient to upload your public key to a key server, whether public or on your own web server (you can also keep it to be used and imported directly from an USB stick).</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#export-the-gpg-public-key","title":"Export the GPG public key","text":"<pre><code>$ gpg --armor --export 22A73AED8E766F01 &gt; 22A73AED8E766F01.asc\n</code></pre> <p>Then upload it to your http server or a public server (needed if you want to be able to easily use the key on multiple machines):</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#upload-it-to-your-http-server","title":"Upload it to your http server","text":"<pre><code>$ scp 22A73AED8E766F01.asc user@server:public_html/static/22A73AED8E766F01.asc\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#or-upload-it-to-a-public-keyserver","title":"OR upload it to a public keyserver","text":"<pre><code>$ gpg --keyserver hkps://hkps.pool.sks-keyservers.net --send-key 22A73AED8E766F01\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#finish-the-yubikey-setup","title":"Finish the Yubikey setup","text":"<p>Now we can finish up the Yubikey setup. Let's edit the card again:</p> <pre><code>$ gpg --card-edit --expert\n\nReader ...........: Yubico Yubikey 4 OTP U2F CCID (0001435106) 00 00\nApplication ID ...: A7560001240102010006054351060000\nVersion ..........: 2.1\nManufacturer .....: Yubico\nSerial number ....: 75435106\nName of cardholder: [not set]\nLanguage prefs ...: [not set]\nSex ..............: unspecified\nURL of public key : [not set]\nLogin data .......: [not set]\nSignature PIN ....: forced\nKey attributes ...: rsa4096 rsa4096 rsa4096\nMax. PIN lengths .: 127 127 127\nPIN retry counter : 3 0 3\nSignature counter : 4\nSignature key ....: A158 0FD9 8C04 86D9 4C1B E63B 22A7 3AED 8E76 6F01\n created ....: 2017-05-16 20:43:17\nEncryption key....: E1B6 7009 907D 1D94 B200 37D7 8E40 7636 C9C3 2C38\n created ....: 2017-05-16 20:43:17\nAuthentication key: AAED AB8E E055 41B2 EFFF 62A4 164F 873A 75D2 AD6B\n created ....: 2017-05-16 20:43:17\nGeneral key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug &lt;ultrabug@nospam.com&gt;\nsec&gt; rsa4096/22A73AED8E766F01 created: 2017-05-16 expires: 2018-05-16\n card-no: 0001 05435106\nssb&gt; rsa4096/164F873A75D2AD6B created: 2017-05-16 expires: 2018-05-16\n card-no: 0001 05435106\nssb&gt; rsa4096/8E407636C9C32C38 created: 2017-05-16 expires: 2018-05-16\n card-no: 0001 05435106\n\ngpg/card&gt; admin\n</code></pre> <p>Make sure that the Signature PIN is forced to request that your PIN is entered when your key is used. If it is listed as \"not forced\", you can enforce it by entering the following command:</p> <pre><code>gpg/card&gt; forcesig\n</code></pre> <p>It is also good practice to set a few more settings on your key.</p> <pre><code>gpg/card&gt; login\nLogin data (account name): ultrabug\n\ngpg/card&gt; lang\nLanguage preferences: en\n\ngpg/card&gt; name\nCardholder's surname: Bug\nCardholder's given name: Ultra\n</code></pre> <p>Now we need to setup the PIN and admin PIN on the card.</p> <pre><code>gpg/card&gt; passwd\ngpg: OpenPGP card no. A7560001240102010006054351060000 detected\n\n1 - change PIN\n2 - unblock PIN\n3 - change Admin PIN\n4 - set the Reset Code\nQ - quit\n\nYour selection? 1\nPIN changed.\n\n1 - change PIN\n2 - unblock PIN\n3 - change Admin PIN\n4 - set the Reset Code\nQ - quit\n\nYour selection? 3\nPIN changed.\n\n1 - change PIN\n2 - unblock PIN\n3 - change Admin PIN\n4 - set the Reset Code\nQ - quit\n\nYour selection? Q\n</code></pre> <p>If you uploaded your public key on your web server or a public server, configure it on the key:</p> <pre><code>gpg/card&gt; url\nURL to retrieve public key: http://ultrabug.fr/keyserver/22A73AED8E766F01.asc\n\ngpg/card&gt; quit\n</code></pre> <p>Now we can quit the gpg card edition, we're done on the Yubikey side!</p> <pre><code>gpg/card&gt; quit\n</code></pre>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#ssh-client-setup","title":"SSH client setup","text":"<p>This is the setup on the machine(s) where you will be using the GPG key. The idea is to import your key from the card to your local keyring so you can use it on gpg-agent (and its ssh support).</p> <p>You can skip the fetch/import part below if you generated the key on the same machine than you are using it. You should see it listed when executing gpg -k.</p> <p>Plug-in your Yubikey and load the smartcard.</p> <pre><code>$ gpg --card-edit --expert\n</code></pre> <p>Then fetch the key from the URL to import it to your local keyring.</p> <pre><code>gpg/card&gt; fetch\n</code></pre> <p>Then you're done on this part, exit gpg and update/display&amp; your card status.</p> <pre><code>gpg/card&gt; quit\n</code></pre> <pre><code>$ gpg --card-status\n</code></pre> <p>You can verify the presence of the key in your keyring:</p> <pre><code>$ gpg -K\nsec&gt;  rsa4096 2017-05-16 [SC] [expires: 2018-05-16]\nA1580FD98C0486D94C1BE63B22A73AED8E766F01\n      Card serial no. = 0001 05435106\nuid           [ultimate] Ultrabug &lt;ultrabug@nospam.com&gt;\nssb&gt;  rsa4096 2017-05-16 [A] [expires: 2018-05-16]\nssb&gt;  rsa4096 2017-05-16 [E] [expires: 2018-05-16]\n</code></pre> <p>Note</p> <p>The \"Card serial no.\" showing that the key is actually stored on a smartcard.</p> <p>Now we need to configure gpg-agent to enable ssh support, edit your ~/.gnupg/gpg-agent.conf configuration file and make sure that the enable-ssh-support is present:</p> <pre><code>default-cache-ttl 7200\nmax-cache-ttl 86400\nenable-ssh-support\n</code></pre> <p>Then you will need to update your ~/.bashrc file to automatically start gpg-agent and override ssh-agent's environment variables. Add this at the end of your ~/.bashrc file (or equivalent, MacOSX users should place this in their ~/.bash_profile file).</p> <pre><code># start gpg-agent if it's not running\n# then override SSH authentication socket to use gpg-agent\nexport GPG_TTY=\"$(tty)\"\nexport SSH_AUTH_SOCK=$(gpgconf --list-dirs agent-ssh-socket)\ngpgconf --launch gpg-agent\n</code></pre> <p>To simulate a clean slate, unplug your card then kill any running gpg-agent:</p> <pre><code>$ killall gpg-agent\n</code></pre> <p>Then plug back your card and source your ~/.bashrc file:</p> <pre><code>$ source ~/.bashrc\n</code></pre> <p>Your GPG key is now listed in you ssh identities!</p> <pre><code>$ ssh-add -l\n4096 SHA256:a4vsJM6Sw1Rt8orvPnI8nvNUwHbRQ67ylnoTxruozK9 cardno:000105435106 (RSA)\n</code></pre> <p>You will now be able to get the SSH public key hash to copy to your remote servers using:</p> <pre><code>$ ssh-add -L\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCVDq24Ld/bOzc3yNnY6fF7FNfZnb6wRVdN2xMo1YiA5pz20y+2P1ynq0rb6l/wsSip0Owq4G6fzaJtT1pBUAkEJbuMvZBrbYokb2mZ78iFZyzAkCT+C9YQtvPClFxSnqSL38jBpunZuuiFfejM842dWdMNK3BTcjeTQdTbmY+VsVOw7ppepRh7BWslb52cVpg+bHhB/4H0BUUd/KHZ5sor0z6e1OkqIV8UTiLY2laCCL8iWepZcE6n7MH9KItqzX2I9HVIuLCzhIib35IRp6d3Whhw3hXlit4/irqkIw0g7s9jC8OwybEqXiaeQbmosLromY3X6H8++uLnk7eg9RtCwcWfDq0Qg2uNTEarVGgQ1HXFr8WsjWBIneC8iG09idnwZNbfV3ocY5+l1REZZDobo2BbhSZiS7hKRxzoSiwRvlWh9GuIv8RNCDss9yNFxNUiEIi7lyskSgQl3J8znDXHfNiOAA2X5kVH0s6AQx4hQP9Dl1X2Em4zOz+yJEPFnAvE+XvBys1yuUPq1c3WKMWzongZi8JNu51Yfj7Trm74hoFRn+CREUNpELD9JignxlvkoKAJpWVLdEu1bxJ7jh7kcMQfVEflLbfkEPLV4nZS4sC1FJR88DZwQvOudyS69wLrF3azC1Gc/fTgBiXVVQwuAXE7vozZk+K4hdrGq4u7Gw== cardno:000105435106\n</code></pre> <p>This is what ends up in ~/.ssh/authorized_keys on your servers.</p> <p>When connecting to your remote server, you will be prompted for the PIN!</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#conclusion","title":"Conclusion","text":"<p>Using the GPG feature of your Yubikey is very convenient and versatile. Even if it is not that hard after all, it is interesting and fair to note that the PIV method is indeed more simple to implement.</p> <p>When you need to maintain a large number of security keys in an organization and that their usage is limited to SSH, you will be inclined to stick with PIV if 2048 bits keys are acceptable for you.</p> <p>However, for power users and developers, usage of GPG is definitely something you need to consider for its versatility and enhanced security.</p>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#useful-links","title":"Useful links","text":"<p>You may find those articles useful to setup your GPG key differently and avoid having the secret key tied to your Yubikey.</p> <ul> <li>https://www.yubico.com/support/knowledge-base/categories/articles/use-yubikey-openpgp/#generatelocal</li> <li>https://www.esev.com/blog/post/2015-01-pgp-ssh-key-on-yubikey-neo/</li> <li>https://www.jfry.me/articles/2015/gpg-smartcard/</li> </ul>","tags":["gentoo","security","ssh","yubikey"]},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/","title":"Load balancing Hadoop Hive with F5 BIG-IP","text":"<p>In our quest to a highly available HiveServer2, we faced so many problems and a clear lack of documentation when it came to do it with F5 BIG-IP load balancers that I think it's worth a blog post to help around.</p> <p>We are using the Cloudera Hadoop distribution but this applies whatever your distribution.</p>","tags":["cloudera","f5","hadoop","hive"]},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#hive-ha-configuration","title":"Hive HA configuration","text":"<p>This appears to be well documented at a first glance but the HiveServer2 (HS2) documentation vanished at the time of writing.</p> <ul> <li>Hive Metastore HA</li> <li>HiveServer2 HA = 404</li> </ul> <p>Anyway, using Cloudera Manager to set up HS2 HA is not hard but there are a few gotchas that I want to highlight and that you will need to be careful with:</p> <ul> <li>As for every Keberos based service, make sure you have a dedicated IP for the HiveServer2 Load Balancer URL and that it's reverse DNS is setup properly. Else you will get GSSAPI errors.</li> <li>When running a secure cluster with Kerberos, the HiveServer2 Load Balancer URL is to be used as your connection host (obvious) AND in your Kerberos principal connection string (maybe less obvious).</li> </ul> <p>Example beeline connection string before HA:</p> <p>!connect jdbc:hive2://hive-server:10000/default;principal=hive/_HOST@REALM.COM</p> <p>and with HA (notice we changed also the _HOST):</p> <p>!connect jdbc:hive2://ha-hive-fqdn:10000/default;principal=hive/ha-hive-fqdn@REALM.COM</p> <p>We found out the kerberos principal gotcha the hard way... The reason behind this is that the _HOST is basically a macro that will get resolved to the client host name which will then be used to validate the kerberos ticket. When running in load balanced/HA mode , the actual source IP will be replaced by the load balancer's IP (SNAT) and the kerberos reverse DNS lookup will then fail!</p> <p>So if you do not use the HS2 HA URL in the kerberos principal string, you will get Kerberos GSSAPI errors when the load balanding SNAT will be used (see next chapter).</p> <p>This will require you to update all your jobs using HS2 to reflect these changes before load balancing HS2 with F5 BIG-IP.</p>","tags":["cloudera","f5","hadoop","hive"]},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#load-balancing-hiveserver2-with-f5","title":"Load balancing HiveServer2 with F5","text":"<p>Our pals at Cloudera have brought a good doc for Impala HA with F5 and they instructed we followed it to set up HS2 HA too because they had nothing better.</p>","tags":["cloudera","f5","hadoop","hive"]},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#kerberos-gssapi-problem","title":"Kerberos GSSAPI problem","text":"<p>When we applied it the first time and tried to switch to using the F5, all our jobs failed because of the kerberos _HOST principal problem mentioned on the previous chapter. This one is not that hard to find out and debug with a google search and explained on Cloudera community forums.</p> <p>We then migrated (again) all our jobs to update the principal connection strings before migrating again to the F5 load balancers.</p>","tags":["cloudera","f5","hadoop","hive"]},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#connection-reset-problems","title":"Connection Reset problems","text":"<p>After our next migration to F5 load balancers, we had most of our jobs running well and the Kerberos problems vanished but we faced a new problem: some of our jobs failed with Connection Reset errors on HiveServer2:</p> <p>java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Connection reset</p> <p>After some debugging and traffic analysis we found out that the F5 were actually responsible for those connection reset but we struggled to understand why.</p> <p>It turned out that the Protocol Profile set up on the Virtual Server was the root cause of the problem and specifically its idle timeout setting default of 300s:</p> <p>Note the Reset on Timeout setting as well which is responsible for the actual Reset packet sent by the F5 to the client.</p> <p>This could also be proven by the Virtual Server statistics showing an increasing Connection Expires count.</p> <p>The solution is to create a new Protocol Profile based on the fastL4 with a higher Idle Timeout setting and update our Virtual Server to use this profile instead of the default one.</p> <p>It seemed sensible in our case to increase the 5 minutes expiration to 1 day, so let's call our new profile fastL4-24h-idle-timeout:</p> <p>Then change the Hive Virtual Server configuration to use this Protocol Profile:</p> <p>You will see no more expired connections on the Virtual Server statistics!</p>","tags":["cloudera","f5","hadoop","hive"]},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#connection-mirroring","title":"Connection mirroring","text":"<p>When creating a Virtual Server, there is a hidden but critical option to enable named Connection Mirroring under the Configuration: Advanced drop-down.</p> <p>Make sure you enable this feature so that your Hive queries and applications can survive a failover of your F5 load balancers. If you do not enable this, you will experience stalled connections and jobs which could take up to multiple hours before failing!</p> <p></p>","tags":["cloudera","f5","hadoop","hive"]},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#job-design-consideration","title":"Job design consideration","text":"<p>We could argue whether or not a default 5 minutes idle timeout is reasonable or not for Hive or any other Hadoop component but it is important to point out that the jobs which were affected also had sub-optimal design pattern in the first place. This also explains why most of our jobs (including also long running ones) were not affected.</p> <p>The affected jobs allowed were Talend jobs where the Hive connection was established at the beginning of the job, used at that time and then the job went on doing other things before using the Hive connection again.</p> <p>When those in between computation took more than 300s, the remaining of the job failed because the initial Hive connection got reset by the F5:</p> <p>This is clearly not a good job design for long processing jobs and you should refrain from doing it. Instead open a connection to Hive when you need it, use it and close it properly. Shall you need it later in your job, open a new connection to Hive and use that one.</p> <p>This will also have the benefit of not keeping open idle connections to Hive itself and favour resources allocation fairness across your jobs.</p> <p>I hope this will be of help to anyone facing these kind of issues.</p>","tags":["cloudera","f5","hadoop","hive"]},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/","title":"ScyllaDB meets Gentoo Linux","text":"<p>I am happy to announce that my work on packaging ScyllaDB for Gentoo Linux is complete!</p> <p>Happy or curious users are very welcome to share their thoughts and ping me to get it into portage (which will very likely happen).</p>","tags":["gentoo","nosql","portage","scylla"]},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/#why-scylla","title":"Why Scylla?","text":"<p>Ever heard of the Cassandra NoSQL database and Java GC/Heap space problems?... if you do, you already get it ;)</p> <p>I will not go into the details as their website does this way better than me but I got interested into Scylla because it fits the Gentoo Linux philosophy very well. If you remember my writing about packaging Rethinkdb for Gentoo Linux, I think that we have a great match with Scylla as well!</p> <ul> <li>it is written in C++ so it plays very well with emerge</li> <li>the code quality is so great that building it does not require heavy patching on the ebuild (feels good to be a packager)</li> <li>the code relies on system libs instead of bundling them in the sources (hurrah!)</li> <li>performance tuning is handled by smart scripting and automation, allowing the relationship between the project and the hardware is strong</li> </ul> <p>I believe that these are good enough points to go further and that such a project can benefit from a source based distribution like Gentoo Linux. Of course compiling on multiple systems is a challenge for such a database but one does not improve by staying in their comfort zone. </p>","tags":["gentoo","nosql","portage","scylla"]},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/#upstream-contributions","title":"Upstream &amp; contributions","text":"<p>Packaging is a great excuse to get to know the source code of a project but more importantly the people behind it.</p> <p>So here I got to my first contributions to Scylla to get Gentoo Linux as a detected and supported Linux distribution in the different scripts and tools used to automatically setup the machine it will run upon (fear not, I contributed bash &amp; python, not C++)...</p> <p>Even if I expected to contribute using Github PRs and got to change my habits to a git-patch+mailing list combo, I got warmly welcomed and received positive and genuine interest in the contributions. They got merged quickly and thanks to them you can install and experience Scylla in Gentoo Linux without heavy patching on our side.</p> <p>Special shout out to Pekka, Avi and Vlad for their welcoming and insightful code reviews!</p> <p>I've some open contributions about pushing further on the python code QA side to get the tools to a higher level of coding standards. Seeing how upstream is serious about this I have faith that it will get merged and a good base for other contributions.</p> <p>Last note about reaching them is that I am a bit sad that they're not using IRC freenode to communicate (I instinctively joined #scylla and found myself alone) but they're on Slack (those \"modern folks\") and pretty responsive to the mailing lists ;)</p>","tags":["gentoo","nosql","portage","scylla"]},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/#java-scylla","title":"Java &amp; Scylla","text":"<p>Even if scylla is a rewrite of Cassandra in C++, the project still relies on some external tools used by the Cassandra community which are written in Java.</p> <p>When you install the scylla package on Gentoo, you will see that those two packages are Java based dependencies:</p> <ul> <li>app-admin/scylla-tools</li> <li>app-admin/scylla-jmx</li> </ul> <p>It pained me a lot to package those (thanks to help of @monsieurp) but they are building and working as expected so this gets the packaging of the whole Scylla project pretty solid.</p>","tags":["gentoo","nosql","portage","scylla"]},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/#emerge-dev-dbscylla","title":"emerge dev-db/scylla","text":"<p>The scylla packages are located in the ultrabug overlay for now until I test them even more and ultimately put them in production. Then they'll surely reach the portage tree with the approval of the Gentoo java team for the app-admin/ packages listed above.</p> <p>I provide a live ebuild (scylla-9999 with no keywords) and ebuilds for the latest major version (2.0_rc1 at time of writing).</p> <p>It's as simple as:</p> <p>$ sudo layman -a ultrabug $ sudo emerge -a dev-db/scylla $ sudo emerge --config dev-db/scylla</p> <p>Try it out and tell me what you think, I hope you'll start considering and using this awesome database!</p>","tags":["gentoo","nosql","portage","scylla"]},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/","title":"py3status v3.6","text":"<p>After four months of cool contributions and hard work on normalization and modules' clean up, I'm glad to announce the release of py3status v3.6!</p> <p>Milestone 3.6 was mainly focused about existing modules, from their documentation to their usage of the py3 helper to streamline their code base.</p> <p>Other improvements were made about error reporting while some sneaky bugs got fixed along the way.</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/#highlights","title":"Highlights","text":"<p>Not an extensive list, check the changelog.</p> <ul> <li>LOTS of modules streamlining (mainly the hard work of @lasers)</li> <li>error reporting improvements</li> <li>py3-cmd performance improvements</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/#new-modules","title":"New modules","text":"<ul> <li>i3blocks support (yes, py3status can now wrap i3blocks thanks to @tobes)</li> <li>cmus module: to control your cmus music player, by @lasers</li> <li>coin_market module: to display custom cryptocurrency data, by @lasers</li> <li>moc module: to control your moc music player, by @lasers</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/#milestone-37","title":"Milestone 3.7","text":"<p>This milestone will give a serious kick into py3status performance. We'll do lots of profiling and drastic work to reduce py3status CPU and memory footprints!</p> <p>For now we've been relying a lot on threads, which is simple to operate but not that CPU/memory friendly. Since i3wm users rightly care for their efficiency we think it's about time we address this kind of points in py3status.</p> <p>Stay tuned, we have some nice ideas in stock :)</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/#thanks-contributors","title":"Thanks contributors!","text":"<p>This release is their work, thanks a lot guys!</p> <ul> <li>aethelz</li> <li>alexoneill</li> <li>armandg</li> <li>Cypher1</li> <li>docwalter</li> <li>enguerrand</li> <li>fmorgner</li> <li>guiniol</li> <li>lasers</li> <li>markrileybot</li> <li>maximbaz</li> <li>tablet-mode</li> <li>paradoxisme</li> <li>ritze</li> <li>rixx</li> <li>tobes</li> <li>valdur55</li> <li>vvoland</li> <li>yabbes</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-10-13-gentoo-linux-listed-rethinkdbs-website/","title":"Gentoo Linux listed RethinkDB's website","text":"<p>The rethinkdb's website has (finally) been updated and Gentoo Linux is now listed on the installation page!</p> <p>Meanwhile, we have bumped the ebuild to version 2.3.6 with fixes for building on gcc-6 thanks to Peter Levine who kindly proposed a nice PR on github.</p>","tags":["gentoo","nosql","rethinkdb"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/","title":"Gentoo Linux on DELL XPS 13 9365 and 9360","text":"<p>Since I received some positive feedback about my previous DELL XPS 9350 post, I am writing this summary about my recent experience in installing Gentoo Linux on a DELL XPS 13 9365.</p> <p>This installation notes goals:</p> <ul> <li>UEFI boot using Grub</li> <li>Provide you with a complete and working kernel configuration</li> <li>Encrypted disk root partition using LUKS</li> <li>Be able to type your LUKS passphrase to decrypt your partition using your local keyboard layout</li> </ul>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#grub-uefi-luks-installation","title":"Grub &amp; UEFI &amp; Luks installation","text":"<p>This installation is a fully UEFI one using grub and booting an encrypted root partition (and home partition). I was happy to see that since my previous post, everything got smoother. So even if you can have this installation working using MBR, I don't really see a point in avoiding UEFI now.</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#bios-configuration","title":"BIOS configuration","text":"<p>Just like with its ancestor, you should:</p> <ul> <li>Turn off Secure Boot</li> <li>Set SATA Operation to AHCI</li> </ul>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#live-cd","title":"Live CD","text":"<p>Once again, go for the latest SystemRescueCD (it\u2019s Gentoo based, you won\u2019t be lost) as it\u2019s quite more up to date and supports booting on UEFI. Make it a Live USB for example using unetbootin and the ISO on a vfat formatted USB stick.</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#nvme-ssd-disk-partitioning","title":"NVME SSD disk partitioning","text":"<p>We\u2019ll obviously use GPT with UEFI. I found that using gdisk was the easiest. The disk itself is found on /dev/nvme0n1. Here it is the partition table I used :</p> <ul> <li>10Mo UEFI BIOS partition (type EF02)</li> <li>500Mo UEFI boot partition (type EF00)</li> <li>2Go Swap partition</li> <li>475Go Linux root partition</li> </ul> <p>The corresponding gdisk commands :</p> <p># gdisk /dev/nvme0n1</p> <p>Command: o \u21b5 This option deletes all partitions and creates a new protective MBR. Proceed? (Y/N): y \u21b5</p> <p>Command: n \u21b5 Partition Number: 1 \u21b5 First sector: \u21b5 Last sector: +10M \u21b5 Hex Code: EF02 \u21b5</p> <p>Command: n \u21b5 Partition Number: 2 \u21b5 First sector: \u21b5 Last sector: +500M \u21b5 Hex Code: EF00 \u21b5</p> <p>Command: n \u21b5 Partition Number: 3 \u21b5 First sector: \u21b5 Last sector: +2G \u21b5 Hex Code: 8200 \u21b5</p> <p>Command: n \u21b5 Partition Number: 4 \u21b5 First sector: \u21b5 Last sector: \u21b5 (for rest of disk) Hex Code: \u21b5</p> <p>Command: p \u21b5 Disk /dev/nvme0n1: 1000215216 sectors, 476.9 GiB Logical sector size: 512 bytes Disk identifier (GUID): A73970B7-FF37-4BA7-92BE-2EADE6DDB66E Partition table holds up to 128 entries First usable sector is 34, last usable sector is 1000215182 Partitions will be aligned on 2048-sector boundaries Total free space is 2014 sectors (1007.0 KiB)</p> <p>Number  Start (sector)    End (sector)  Size       Code  Name    1            2048           22527   10.0 MiB    EF02  BIOS boot partition    2           22528         1046527   500.0 MiB   EF00  EFI System    3         1046528         5240831   2.0 GiB     8200  Linux swap    4         5240832      1000215182   474.4 GiB   8300  Linux filesystem</p> <p>Command: w \u21b5 Do you want to proceed? (Y/N): Y \u21b5</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#no-wifi-on-live-cd-no-panic","title":"No WiFi on Live CD ? no panic","text":"<p>Once again on my (old?) SystemRescueCD stick, the integrated Intel 8265/8275 wifi card is not detected.</p> <p>So I used my old trick with my Android phone connected to my local WiFi as a USB modem which was detected directly by the live CD.</p> <ul> <li>get your Android phone connected on your local WiFi (unless you want to use your cellular data)</li> <li>plug in your phone using USB to your XPS</li> <li>on your phone, go to Settings / More / Tethering &amp; portable hotspot</li> <li>enable USB tethering</li> </ul> <p>Running ip addr will show the network card enp0s20f0u2 (for me at least), then if no IP address is set on the card, just ask for one :</p> <p># dhcpcd enp0s20f0u2</p> <p>You have now access to the internet.</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#proceed-with-installation","title":"Proceed with installation","text":"<p>The only thing to prepare is to format the UEFI boot partition as FAT32. Do not worry about the UEFI BIOS partition (/dev/nvme0n1p1), grub will take care of it later.</p> <p># mkfs.vfat -F 32 /dev/nvme0n1p2</p> <p>Do not forget to use cryptsetup to encrypt your /dev/nvme0n1p4 partition! In the rest of the article, I'll be using its device mapper representation.</p> <p># cryptsetup luksFormat -s 512 /dev/nvme0n1p4</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#cryptsetup-luksopen-devnvme0n1p4-root","title":"cryptsetup luksOpen /dev/nvme0n1p4 root","text":"","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#mkfsext4-devmapperroot","title":"mkfs.ext4 /dev/mapper/root","text":"<p>Then follow the Gentoo handbook as usual for the stage3 related next steps. Make sure you mount and bind the following to your /mnt/gentoo LiveCD installation folder (the /sys binding is important for grub UEFI):</p> <p># mount -t proc none /mnt/gentoo/proc</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#mount-o-bind-dev-mntgentoodev","title":"mount -o bind /dev /mnt/gentoo/dev","text":"","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#mount-o-bind-sys-mntgentoosys","title":"mount -o bind /sys /mnt/gentoo/sys","text":"","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#makeconf-settings","title":"make.conf settings","text":"<p>I strongly recommend using at least the following on your /etc/portage/make.conf :</p> <p>GRUB_PLATFORM=\"efi-64\" INPUT_DEVICES=\"evdev synaptics\" VIDEO_CARDS=\"intel i965\"</p> <p>USE=\"bindist cryptsetup\"</p> <p>The GRUB_PLATFORM one is important for later grub setup and the cryptsetup USE flag will help you along the way.</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#fstab-for-ssd","title":"fstab for SSD","text":"<p>Don't forget to make sure the noatime option is used on your fstab for / and /home.</p> <p>/dev/nvme0n1p2    /boot    vfat    noauto,noatime    1 2 /dev/nvme0n1p3    none     swap    sw                0 0 /dev/mapper/root  /        ext4    noatime   0 1</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#kernel-configuration-and-compilation","title":"Kernel configuration and compilation","text":"<p>I suggest you use a recent &gt;=sys-kernel/gentoo-sources-4.13.0 along with genkernel.</p> <ul> <li>You can download my kernel configuration file (iptables, docker, luks &amp; stuff included)</li> <li>Put the kernel configuration file into the /etc/kernels/ directory (with a training s)</li> <li>Rename the configuration file with the exact version of your kernel</li> </ul> <p>Then you'll need to configure genkernel to add luks support, firmware files support and keymap support if your keyboard layout is not QWERTY.</p> <p>In your /etc/genkernel.conf, change the following options:</p> <p>LUKS=\"yes\" FIRMWARE=\"yes\" KEYMAP=\"1\"</p> <p>Then run genkernel all to build your kernel and luks+firmware+keymap aware initramfs.</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#grub-uefi-bootloader-with-luks-and-custom-keymap-support","title":"Grub UEFI bootloader with LUKS and custom keymap support","text":"<p>Now it's time for the grub magic to happen so you can boot your wonderful Gentoo installation using UEFI and type your password using your favourite keyboard layout.</p> <ul> <li>make sure your boot vfat partition is mounted on /boot</li> <li>edit your /etc/default/grub configuration file with the following:</li> </ul> <p>GRUB_CMDLINE_LINUX=\"crypt_root=/dev/nvme0n1p4 keymap=fr\"</p> <p>This will allow your initramfs to know it has to read the encrypted root partition from the given partition and to prompt for its password in the given keyboard layout (french here).</p> <p>Now let's install the grub UEFI boot files and setup the UEFI BIOS partition.</p> <p># grub-install --efi-directory=/boot --target=x86_64-efi /dev/nvme0n1 Installing for x86_64-efi platform. Installation finished. No error reported</p> <p>It should report no error, then we can generate the grub boot config:</p> <p># grub-mkconfig -o /boot/grub/grub.cfg</p> <p>You're all set!</p> <p>You will get a gentoo UEFI boot option, you can disable the Microsoft Windows one from your BIOS to get straight to the point.</p> <p>Hope this helped!</p>","tags":["9360","9365","dell","gentoo","uefi","xps"]},{"location":"Tech%20Blog/2017/2017-12-12-py3status-v3-7/","title":"py3status v3.7","text":"<p>This important release has been long awaited as it focused on improving overall performance of py3status as well as dramatically decreasing its memory footprint!</p> <p>I want once again to salute the impressive work of @lasers, our amazing contributors from the USA who has become top one contributor of 2017 in term of commits and PRs.</p> <p>Thanks to him, this release brings a whole batch of improvements and QA clean ups on various modules. I encourage you to go through the changelog to see everything.</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-12-12-py3status-v3-7/#highlights","title":"Highlights","text":"<p>Deep rework of the usage and scheduling of threads to run modules has been done by @tobes.</p> <ul> <li>now py3status does not keep one thread per module running permanently but instead uses a queue to spawn a thread to execute the module only when its cache expires</li> <li>this new scheduling and usage of threads allows py3status to run under asynchronous event loops and gevent will be supported on the upcoming 3.8</li> <li>memory footprint of py3status got largely reduced thanks to the threads modifications and thanks to a nice hunt on ever growing and useless variables</li> <li>modules error reporting is now more detailed</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-12-12-py3status-v3-7/#milestone-38","title":"Milestone 3.8","text":"<p>The next release will bring some awesome new features such as gevent support, environment variable support in config file and per module persistent data storage as well as new modules!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-12-12-py3status-v3-7/#thanks-contributors","title":"Thanks contributors!","text":"<p>This release is their work, thanks a lot guys!</p> <ul> <li>JohnAZoidberg</li> <li>lasers</li> <li>maximbaz</li> <li>pcewing</li> <li>tobes</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2017/2017-12-22-talks-page/","title":"Talks page","text":"<p>I finally decided to put up a page on this website referencing the talks I've been giving over the last few years.</p> <p>You'll see that I'm quite obsessed with fault tolerance and scalability designs... I guess I can't deny it any more :)</p> <p></p>","tags":["talks"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/","title":"Evaluating ScyllaDB for production 1/2","text":"<p>I have recently been conducting a quite deep evaluation of ScyllaDB to find out if we could benefit from this database in some of our intensive and latency critical data streams and jobs.</p> <p>I'll try to share this great experience within two posts:</p> <ol> <li>The first one (you're reading) will walk through how to prepare yourself for a successful Proof Of Concept based evaluation with the help of the ScyllaDB team.</li> <li>The second post will cover the technical aspects and details of the POC I've conducted with the various approaches I've followed to find the most optimal solution.</li> </ol> <p>But let's start with how I got into this in the first place...</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#selecting-scylladb","title":"Selecting ScyllaDB","text":"<p>I got interested in ScyllaDB because of its philosophy and engagement and I quickly got into it by being a modest contributor and its Gentoo Linux packager (not in portage yet).</p> <p></p> <p>Of course, I didn't pick an interest in that technology by chance:</p> <p>We've been using MongoDB in (mass) production at work for a very very long time now. I can easily say we were early MongoDB adopters. But there's no wisdom in saying that MongoDB is not suited for every use case and the Hadoop stack has come very strong in our data centers since then, with a predominance of Hive for the heavy duty and data hungry workflows.</p> <p>One thing I was never satisfied with MongoDB was its primary/secondary architecture which makes you lose write throughput and is even more horrible when you want to set up what they call a \"cluster\" which is in fact some mediocre abstraction they add on top of replica-sets. To say the least, it is inefficient and cumbersome to operate and maintain.</p> <p>So I obviously had Cassandra on my radar for a long time, but I was pushed back by its Java stack, heap size and silly tuning... Also, coming from the versatile MongoDB world, Cassandra's CQL limitations looked dreadful at that time...</p> <p>The day I found myself on ScyllaDB's webpage and read their promises, I was sure to be challenging our current use cases with this interesting sea monster.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#setting-up-a-poc-with-the-people-at-scylladb","title":"Setting up a POC with the people at ScyllaDB","text":"<p>Through my contributions around my packaging of ScyllaDB for Gentoo Linux, I got to know a bit about the people behind the technology. They got interested in why I was packaging this in the first place and when I explained my not-so-secret goal of challenging our production data workflows using Scylla, they told me that they would love to help!</p> <p>I was a bit surprised at first because this was the first time I ever saw a real engagement of the people behind a technology into someone else's POC.</p> <p>Their pitch is simple, they will help (for free) anyone conducting a serious POC to make sure that the outcome and the comprehension behind it is the best possible. It is a very mature reasoning to me because it is easy to make false assumptions and conclude badly when testing a technology you don't know, even more when your use cases are complex and your expectations are very high like us.</p> <p>Still, to my current knowledge, they're the only ones in the data industry to have this kind of logic in place since the start. So I wanted to take this chance to thank them again for this!</p> <p>The POC includes:</p> <ul> <li>no bullshit, simple tech-to-tech relationship</li> <li>a private slack channel with multiple ScyllaDB's engineers</li> <li>video calls to introduce ourselves and discuss our progress later on</li> <li>help in schema design and logic</li> <li>fast answers to every question you have</li> <li>detailed explanations on the internals of the technology</li> <li>hardware sizing help and validation</li> <li>funny comments and French jokes (ok, not suitable for everyone)</li> </ul> <p></p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#lessons-for-a-successful-poc","title":"Lessons for a successful POC","text":"<p>As I said before, you've got to be serious in your approach to make sure your POC will be efficient and will lead to an unbiased and fair conclusion.</p> <p>This is a list of the main things I consider important to have prepared before you start.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#have-some-background","title":"Have some background","text":"<p>Make sure to read some literature to have the key concepts and words in mind before you go. It is even more important if like me you do not come from the Cassandra world.</p> <p>I found that the Cassandra: The Definitive Guide book at O'Reilly is a great read. Also, make sure to go around ScyllaDB's documentation.</p> <p></p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#work-with-a-shared-reference-document","title":"Work with a shared reference document","text":"<p>Make sure you share with the ScyllaDB guys a clear and detailed document explaining exactly what you're trying to achieve and how you are doing it today (if you plan on migrating like we did).</p> <p>I made a google document for this because it felt the easiest. This document will be updated as you go and will serve as a reference for everyone participating in the POC.</p> <p>This shared reference document is very important, so if you don't know how to construct it or what to put in it, here is how I structured it:</p> <ol> <li>Who's participating at <ul> <li>photo + name + speciality</li> </ul> <li>Who's participating at ScyllaDB</li> <li>POC hardware<ul> <li>if you have your own bare metal machines you want to run your POC on, give every detail about their number and specs</li> <li>if not, explain how you plan to setup and run your scylla cluster</li> </ul> </li> <li>Reference infrastructure<ul> <li>give every details on the technologies and on the hardware of the servers that are currently responsible for running your workflows</li> <li>explain your clusters and their speciality</li> </ul> </li> <li>Use case #1 : <ul> <li>Context<ul> <li>give context about your use case by explaining it without tech words, think from the business / user point of view</li> </ul> </li> <li>Current implementations<ul> <li>that's where you get technical</li> <li>technology names and where they come into play in your current stack</li> <li>insightful data volumes and cardinality</li> <li>current schema models</li> </ul> </li> <li>Workload related to this use case<ul> <li>queries per second per data source / type</li> <li>peek hours or no peek hours?</li> <li>criticality</li> </ul> </li> <li>Questions we want to answer to<ul> <li>remember, the NoSQL world is lead by query-based-modeling schema design logic, cassandra/scylla is no exception</li> <li>write down the real questions you want your data model(s) to be able to answer to</li> <li>group them and rate them by importance</li> </ul> </li> <li>Validated models<ul> <li>this one comes during the POC when you have settled on the data models</li> <li>write them down, explain them or relate them to the questions they answer to</li> <li>copy/paste some code showcasing how to work with them</li> </ul> </li> <li>Code examples<ul> <li>depending on the complexity of your use case, you may have multiple constraints or ways to compare your current implementation with your POC</li> <li>try to explain what you test and copy/paste the best code you came up with to validate each point</li> </ul> </li> </ul>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#have-monitoring-in-place","title":"Have monitoring in place","text":"<p>ScyllaDB provides a monitoring platform based on Docker, Prometheus and Grafana that is efficient and simple to set up. I strongly recommend that you set it up, as it provides valuable insights almost immediately, and on an ongoing basis.</p> <p>Also you should strive to give access to your monitoring to the ScyllaDB guys, if that's possible for you. They will provide with a fixed IP which you can authorize to access your grafana dashboards so they can have a look at the performances of your POC cluster as you go. You'll learn a great deal about ScyllaDB's internals by sharing with them.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#know-when-to-stop","title":"Know when to stop","text":"<p>The main trap in a POC is to work without boundaries. Since you're looking for the best of what you can get out of a technology, you'll get tempted to refine indefinitely.</p> <p>So this is good to have at least an idea on the minimal figures you'd like to reach to get satisfied with your tests. You can always push a bit further but not for too long!</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#plan-some-high-availability-tests","title":"Plan some high availability tests","text":"<p>Even if you first came to ScyllaDB for its speed, make sure to test its high availability capabilities based on your experience.</p> <p>Most importantly, make sure you test it within your code base and guidelines. How will your code react and handle a failure, partial and total? I was very surprised and saddened to discover so little literature on the subject in the Cassandra community.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#poc-production","title":"POC != production","text":"<p>Remember that even when everything is right on paper, production load will have its share of surprises and unexpected behaviours. So keep a good deal of flexibility in your design and your capacity planning to absorb them.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#make-time","title":"Make time","text":"<p>Our POC lasted almost 5 months instead of estimated 3, mostly because of my agenda's unwillingness to cooperate...</p> <p>As you can imagine this interruption was not always optimal, for either me or the ScyllaDB guys, but they were kind not to complain about it. So depending on how thorough you plan to be, make sure you make time matching your degree of demands. The reference document is also helpful to get back to speed.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#feedback-for-the-scylladb-guys","title":"Feedback for the ScyllaDB guys","text":"<p>Here are the main points I noted during the POC that the guys from ScyllaDB could improve on.</p> <p>They are subjective of course but it's important to give feedback so here it goes. I'm fully aware that everyone is trying to improve, so I'm not pointing any fingers at all.</p> <p>I shared those comments already with them and they acknowledged them very well.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#more-video-meetings-on-start","title":"More video meetings on start","text":"<p>When starting the POC, try to have some pre-scheduled video meetings to set it right in motion. This will provide a good pace as well as making sure that everyone is on the same page.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#make-a-poc-kick-starter-questionnaire","title":"Make a POC kick starter questionnaire","text":"<p>Having a minimal plan to follow with some key points to set up just like the ones I explained before would help. Maybe also a minimal questionnaire to make sure that the key aspects and figures have been given some thought since the start. This will raise awareness on the real answers the POC aims to answer.</p> <p>To put it simpler: some minimal formalism helps to check out the key aspects and questions.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#develop-a-higher-client-driver-expertise","title":"Develop a higher client driver expertise","text":"<p>This one was the most painful to me, and is likely to be painful for anyone who, like me, is not coming from the Cassandra world.</p> <p>Finding good and strong code examples and guidelines on the client side was hard and that's where I felt the most alone. This was not pleasant because a technology is definitely validated through its usage which means on the client side.</p> <p>Most of my tests were using python and the python-cassandra driver so I had tons of questions about it with no sticking answers. Same thing went with the spark-cassandra-connector when using scala where some key configuration options (not documented) can change the shape of your results drastically (more details on the next post).</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#high-availability-guidelines-and-examples","title":"High Availability guidelines and examples","text":"<p>This one still strikes me as the most awkward on the Cassandra community. I literally struggled with finding clear and detailed explanations about how to handle failure more or less gracefully with the python driver (or any other driver).</p> <p>This is kind of a disappointment to me for a technology that position itself as highly available... I'll get into more details about it on the next post.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#a-clearer-sizing-documentation","title":"A clearer sizing documentation","text":"<p>Even if there will never be a magic formula, there are some rules of thumb that exist for sizing your hardware for ScyllaDB. They should be written down more clearly in a maybe dedicated documentation (sizing guide is labeled as admin guide at time of writing).</p> <p>Some examples:</p> <ul> <li>RAM per core ? what is a core ? relation to shard ?</li> <li>Disk / RAM maximal ratio ?</li> <li>Multiple SSDs vs one NMVe ?</li> <li>Hardware RAID vs software RAID ? need a RAID controller at all ?</li> </ul> <p>Maybe even provide a bare metal complete example from two different vendors such as DELL and HP.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#whats-next","title":"What's next?","text":"<p>In the next post, I'll get into more details on the POC itself and the technical learnings we found along the way. This will lead to the final conclusion and the next move we engaged ourselves with.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/","title":"Evaluating ScyllaDB for production 2/2","text":"<p>In my previous blog post, I shared 7 lessons on our experience in evaluating Scylla for production.</p> <p>Those lessons were focused on the setup and execution of the POC and I promised a more technical blog post with technical details and lessons learned from the POC, here it is!</p> <p>Before you read on, be mindful that our POC was set up to test workloads and workflows, not to benchmark technologies. So even if the Scylla figures are great, they have not been the main drivers of the actual conclusion of the POC.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#business-context","title":"Business context","text":"<p>As a data driven company working in the Marketing and Advertising industry, we help our clients make sense of multiple sources of data to build and improve their relationship with their customers and prospects.</p> <p>Dealing with multiple sources of data is nothing new but their volume has dramatically changed during the past decade. I will spare you with the Big-Data-means-nothing term and the technical challenges that comes with it as you already heard enough of it.</p> <p>Still, it is clear that our line of business is tied to our capacity at mixing and correlating a massive amount of different types of events (data sources/types) coming from various sources which all have their own identifiers (think primary keys):</p> <ul> <li>Web navigation tracking: identifier is a cookie that's tied to the tracking domain (we have our own)</li> <li>CRM databases: usually the email address or an internal account ID serve as an identifier</li> <li>Partners' digital platform: identifier is also a cookie tied to their tracking domain</li> </ul> <p>To try to make things simple, let's take a concrete example:</p> <p>You work for UNICEF and want to optimize their banner ads budget by targeting the donors of their last fundraising campaign.</p> <ul> <li>Your reference user database is composed of the donors who registered with their email address on the last campaign: main identifier is the email address.</li> <li>To buy web display ads, you use an Ad Exchange partner such as AppNexus or DoubleClick (Google). From their point of view, users are seen as cookie IDs which are tied to their own domain.</li> </ul> <p>So you basically need to be able to translate an email address to a cookie ID for every partner you work with.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#use-case-id-matching-tables","title":"Use case: ID matching tables","text":"<p>We operate and maintain huge ID matching tables for every partner and a great deal of our time is spent translating those IDs from one to another. In SQL terms, we are basically doing JOINs between a dataset and those ID matching tables.</p> <ul> <li>You select your reference population</li> <li>You JOIN it with the corresponding ID matching table</li> <li>You get a matched population that your partner can recognize and interact with</li> </ul> <p></p> <p>Those ID matching tables have a pretty high read AND write throughput because they're updated and queried all the time.</p> <p>Usual figures are JOINs between a 10+ Million dataset and 1.5+ Billion ID matching tables.</p> <p>The reference query basically looks like this:</p> <p>SELECT count(m.partnerid) FROM population_10M_rows AS p JOIN partner_id_match_400M_rows AS m ON p.id = m.id</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#current-implementations","title":"Current implementations","text":"<p>We operate a lambda architecture where we handle real time ID matching using MongoDB and batch ones using Hive (Apache Hadoop).</p> <p>The first downside to note is that it requires us to maintain two copies of every ID matching table. We also couldn't choose one over the other because neither MongoDB nor Hive can sustain both the read/write lookup/update ratio while performing within the low latencies that we need.</p> <p>This is an operational burden and requires quite a bunch of engineering to ensure data consistency between different data stores.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#production-hardware-overview","title":"Production hardware overview:","text":"<ul> <li>MongoDB is running on a 15 nodes (5 shards) cluster<ul> <li>64GB RAM, 2 sockets, RAID10 SAS spinning disks, 10Gbps dual NIC</li> </ul> </li> <li>Hive is running on 50+ YARN NodeManager instances<ul> <li>128GB RAM, 2 sockets, JBOD SAS spinning disks, 10Gbps dual NIC</li> </ul> </li> </ul>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#target-implementation","title":"Target implementation","text":"<p>The key question is simple: is there a technology out there that can sustain our ID matching tables workloads while maintaining consistently low upsert/write and lookup/read latencies?</p> <p>Having one technology to handle both use cases would allow:</p> <ul> <li>Simpler data consistency</li> <li>Operational simplicity and efficiency</li> <li>Reduced costs</li> </ul>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#poc-hardware-overview","title":"POC hardware overview:","text":"<p>So we decided to find out if Scylla could be that technology. For this, we used three decommissioned machines that we had in the basement of our Paris office.</p> <ul> <li>2 DELL R510<ul> <li>19GB RAM, 2 socket 8 cores, RAID0 SAS spinning disks, 1Gbps NIC</li> </ul> </li> <li>1 DELL R710<ul> <li>19GB RAM, 2 socket 4 cores, RAID0 SAS spinning disks, 1Gbps NIC</li> </ul> </li> </ul> <p>I know, these are not glamorous machines and they are even inconsistent in specs, but we still set up a 3 node Scylla cluster running Gentoo Linux with them.</p> <p>Our take? If those three lousy machines can challenge or beat the production machines on our current workloads, then Scylla can seriously be considered for production.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-1-validate-a-schema-model","title":"Step 1: Validate a schema model","text":"<p>Once the POC document was complete and the ScyllaDB team understood what we were trying to do, we started iterating on the schema model using a query based modeling strategy.</p> <p>So we wrote down and rated the questions that our model(s) should answer to, they included stuff like:</p> <ul> <li>What are all our cookie IDs associated to the given partner ID ?</li> <li>What are all the cookie IDs associated to the given partner ID over the last N months ?</li> <li>What is the last cookie ID/date for the given partner ID ?</li> <li>What is the last date we have seen the given cookie ID / partner ID couple ?</li> </ul> <p>As you can imagine, the reverse questions are also to be answered so ID translations can be done both ways (ouch!).</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#prototyping","title":"Prototyping","text":"<p>This is no news that I'm a Python addict so I did all my prototyping using Python and the cassandra-driver.</p> <p>I ended up using a test-driven data modelling strategy using pytest. I wrote tests on my dataset so I could concentrate on the model while making sure that all my questions were being answered correctly and consistently.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#schema","title":"Schema","text":"<p>In our case, we ended up with three denormalized tables to answer all the questions we had. To answer the first three questions above, you could use the schema below:</p> <p>CREATE TABLE IF NOT EXISTS ids_by_partnerid(  partnerid text,  id text,  date timestamp,  PRIMARY KEY ((partnerid), date, id)  )  WITH CLUSTERING ORDER BY (date DESC)</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#note-on-clustering-key-ordering","title":"Note on clustering key ordering","text":"<p>One important learning I got in the process of validating the model is about the internals of Cassandra's file format that resulted in the choice of using a descending order DESC on the date clustering key as you can see above.</p> <p>If your main use case of querying is to look for the latest value of an history-like table design like ours, then make sure to change the default ASC order of your clustering key to DESC. This will ensure that the latest values (rows) are stored at the beginning of the sstable file effectively reducing the read latency when the row is not in cache!</p> <p>Let me quote Glauber Costa's detailed explanation on this:</p> <p>Basically in Cassandra's file format, the index points to an entire partition (for very large partitions there is a hack to avoid that, but the logic is mostly the same). So if you want to read the first row, that's easy you get the index to the partition and read the first row. I__f you want to read the last row, then you get the index to the partition and do a linear scan to the next.</p> <p>This is the kind of learning you can only get from experts like Glauber and that can justify the whole POC on its own!</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-2-set-up-scylla-grafana-monitoring","title":"Step 2: Set up scylla-grafana-monitoring","text":"<p>As I said before, make sure to set up and run the scylla-grafana-monitoring project before running your test workloads. This easy to run solution will be of great help to understand the performance of your cluster and to tune your workload for optimal performances.</p> <p></p> <p>If you can, also discuss with the ScyllaDB team to allow them to access the Grafana dashboard. This will be very valuable since they know where to look better than we usually do... I gained a lot of understandings thanks to this!</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#note-on-scrape-interval","title":"Note on scrape interval","text":"<p>I advise you to lower the Prometheus scrape interval to have a shorter and finer sampling of your metrics. This will allow your dashboard to be more reactive when you start your test workloads.</p> <p>For this, change the prometheus/prometheus.yml file like this:</p> <p>scrape_interval: 2s # Scrape targets every 2 seconds (5s default) scrape_timeout: 1s # Timeout before trying to scrape a target again (4s default)</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#test-your-monitoring","title":"Test your monitoring","text":"<p>Before going any further, I strongly advise you to run a stress test on your POC cluster using the cassandra-stress tool and share the results and their monitoring graphs with the ScyllaDB team.</p> <p>This will give you a common understanding of the general performances of your cluster as well as help in outlining any obvious misconfiguration or hardware problem.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#key-graphs-to-look-at","title":"Key graphs to look at","text":"<p>There are a lot of interesting graphs so I'd like to share the ones that I have been mainly looking at. Remember that depending on your test workloads, some other graphs may be more relevant for you.</p> <ul> <li>number of open connections</li> </ul> <p>You'll want to see a steady and high enough number of open connections which will prove that your clients are pushed at their maximum (at the time of testing this graph was not on Grafana and you had to add it yourself)</p> <ul> <li>cache hits / misses</li> </ul> <p>Depending on your reference dataset, you'll obviously see that cache hits and misses will have a direct correlation with disk I/O and overall performances. Running your test workloads multiple times should trigger higher cache hits if your RAM is big enough.</p> <ul> <li>per shard/node distribution</li> </ul> <p>The Requests Served per shard graph should display a nicely distributed load between your shards and nodes so that you're sure that you're getting the best out of your cluster.</p> <p>The same is true for almost every other \"per shard/node\" graph: you're looking for evenly distributed load.</p> <ul> <li>sstable reads</li> </ul> <p>Directly linked with your disk performances, you'll be trying to make sure that you have almost no queued sstable reads.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-3-get-your-reference-data-and-metrics","title":"Step 3: Get your reference data and metrics","text":"<p>We obviously need to have some reference metrics on our current production stack so we can compare them with the results on our POC Scylla cluster.</p> <p>Whether you choose to use your current production machines or set up a similar stack on the side to run your test workloads is up to you. We chose to run the vast majority of our tests on our current production machines to be as close to our real workloads as possible.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#prepare-a-reference-dataset","title":"Prepare a reference dataset","text":"<p>During your work on the POC document, you should have detailed the usual data cardinality and volume you work with. Use this information to set up a reference dataset that you can use on all of the platforms that you plan to compare.</p> <p>In our case, we chose a 10 Million reference dataset that we JOINed with a 400+ Million extract of an ID matching table. Those volumes seemed easy enough to work with and allowed some nice ratio for memory bound workloads.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#measure-on-your-current-stack","title":"Measure on your current stack","text":"<p>Then it's time to load this reference datasets on your current platforms.</p> <ul> <li>If you run a MongoDB cluster like we do, make sure to shard and index the dataset just like you do on the production collections.</li> <li>On Hive, make sure to respect the storage file format of your current implementations as well as their partitioning.</li> </ul> <p>If you chose to run your test workloads on your production machines, make sure to run them multiple times and at different hours of the day and night so you can correlate the measures with the load on the cluster at the time of the tests.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#reference-metrics","title":"Reference metrics","text":"<p>For the sake of simplicity I'll focus on the Hive-only batch workloads. I performed a count on the JOIN of the dataset and the ID matching table using Spark 2 and then I also ran the JOIN using a simple Hive query through Beeline.</p> <p>I gave the following definitions on the reference load:</p> <ul> <li>IDLE: YARN available containers and free resources are optimal, parallelism is very limited</li> <li>NORMAL: YARN sustains some casual load, parallelism exists but we are not bound by anything still</li> <li>HIGH: YARN has pending containers, parallelism is high and applications have to wait for containers before executing</li> </ul> <p>There's always an error margin on the results you get and I found that there was not significant enough differences between the results using Spark 2 and Beeline so I stuck with a simple set of results:</p> <ul> <li>IDLE: 2 minutes, 15 seconds</li> <li>NORMAL: 4 minutes</li> <li>HIGH: 15 minutes</li> </ul>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-4-get-scylla-in-the-mix","title":"Step 4: Get Scylla in the mix","text":"<p>It's finally time to do your best to break Scylla or at least to push it to its limits on your hardware... But most importantly, you'll be looking to understand what those limits are depending on your test workloads as well as outlining out all the required tuning that you will be required to do on the client side to reach those limits.</p> <p>Speaking about the results, we will have to differentiate two cases:</p> <ol> <li>The Scylla cluster is fresh and its cache is empty (cold start): performance is mostly Disk I/O bound</li> <li>The Scylla cluster has been running some test workload already and its cache is hot: performance is mostly Memory bound with some Disk I/O depending on the size of your RAM</li> </ol>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#spark-2-scala-test-workload","title":"Spark 2 / Scala test workload","text":"<p>Here I used Scala (yes, I did) and DataStax's spark-cassandra-connector so I could use the magic\u00a0joinWithCassandraTable function.</p> <ul> <li>spark-cassandra-connector-2.0.1-s_2.11.jar</li> <li>Java 7</li> </ul> <p>I had to stick with the 2.0.1 version of the spark-cassandra-connector because newer version (2.0.5 at the time of testing) were performing bad with no apparent reason. The ScyllaDB team couldn't help on this.</p> <p>You can interact with your test workload using the spark2-shell:</p> <p>spark2-shell --jars jars/commons-beanutils_commons-beanutils-1.9.3.jar,jars/com.twitter_jsr166e-1.1.0.jar,jars/io.netty_netty-all-4.0.33.Final.jar,jars/org.joda_joda-convert-1.2.jar,jars/commons-collections_commons-collections-3.2.2.jar,jars/joda-time_joda-time-2.3.jar,jars/org.scala-lang_scala-reflect-2.11.8.jar,jars/spark-cassandra-connector-2.0.1-s_2.11.jar</p> <p>Then use the following Scala imports:</p> <p>// main connector import import com.datastax.spark.connector._</p> <p>// the joinWithCassandraTable failed without this (dunno why, I'm no Scala guy) import com.datastax.spark.connector.writer._ implicit val rowWriter = SqlRowWriter.Factory</p> <p>Finally I could run my test workload to select the data from Hive and JOIN it with Scylla easily:</p> <p>val df_population = spark.sql(\"SELECT id FROM population_10M_rows\") val join_rdd = df_population.rdd.repartitionByCassandraReplica(\"test_keyspace\", \"partner_id_match_400M_rows\").joinWithCassandraTable(\"test_keyspace\", \"partner_id_match_400M_rows\") val joined_count = join_rdd.count()</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#notes-on-tuning-spark-cassandra-connector","title":"Notes on tuning spark-cassandra-connector","text":"<p>I experienced pretty crappy performances at first. Thanks to the easy Grafana monitoring, I could see that Scylla was not being the bottleneck at all and that I instead had trouble getting some real load on it.</p> <p>So I engaged in a thorough tuning of the spark-cassandra-connector with the help of Glauber... and it was pretty painful but we finally made it and got the best parameters to get the load on the Scylla cluster close to 100% when running the test workloads.</p> <p>This tuning was done in the spark-defaults.conf file:</p> <ul> <li>have a fixed set of executors and boost their overhead memory</li> </ul> <p>This will increase test results reliability by making sure you always have a reserved number of available workers at your disposal.</p> <p>spark.dynamicAllocation.enabled=false spark.executor.instances=30 spark.yarn.executor.memoryOverhead=1024</p> <ul> <li>set the split size to 1MB</li> </ul> <p>Default is 8MB but Scylla uses a split size of 1MB so you'll see a great boost of performance and stability by setting this setting to the right number.</p> <p>spark.cassandra.input.split.size_in_mb=1</p> <ul> <li>align driver timeouts with server timeouts</li> </ul> <p>It is advised to make sure that your read request timeouts are the same on the driver and the server so you do not get stalled states waiting for a timeout to happen on one hand. You can do the same with write timeouts if your test workloads are write intensive.</p> <p>/etc/scylla/scylla.yaml</p> <p>read_request_timeout_in_ms: 150000</p> <p>spark-defaults.conf</p> <p>spark.cassandra.connection.timeout_ms=150000 spark.cassandra.read.timeout_ms=150000</p> <p>// optional if you want to fail / retry faster for HA scenarios spark.cassandra.connection.reconnection_delay_ms.max=5000 spark.cassandra.connection.reconnection_delay_ms.min=1000 spark.cassandra.query.retry.count=100</p> <ul> <li>adjust your reads per second rate</li> </ul> <p>Last but surely not least, this setting you will need to try and find out the best value for yourself since it has a direct impact on the load on your Scylla cluster. You will be looking at pushing your POC cluster to almost 100% load.</p> <p>spark.cassandra.input.reads_per_sec=6666</p> <p>As I said before, I could only get this to work perfectly using the 2.0.1 version of the spark-cassandra-connector driver. But then it worked very well and with great speed.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#spark-2-results","title":"Spark 2 results","text":"<p>Once tuned, the best results I was able to reach on this hardware are listed below. It's interesting to see that with spinning disks, the cold start result can compete with the results of a heavily loaded Hadoop cluster where pending containers and parallelism are knocking down its performances.</p> <ul> <li>hot cache: 2min</li> <li>cold cache: 12min</li> </ul> <p>Wow! Those three refurbished machines can compete with our current production machines and implementations, they can even match an idle Hive cluster of a medium size!</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#python-test-workload","title":"Python test workload","text":"<p>I couldn't conclude on a Scala/Spark 2 only test workload. So I obviously went back to my language of choice Python only to discover at my disappointment that there is no joinWithCassandraTable equivalent available on pyspark...</p> <p>I tried with some projects claiming otherwise with no success until I changed my mind and decided that I probably didn't need Spark 2 at all. So I went into the crazy quest of beating Spark 2 performances using a pure Python implementation.</p> <p>This basically means that instead of having a JOIN like helper, I had to do a massive amount of single \"id -&gt; partnerid\" lookups. Simple but greatly inefficient you say? Really?</p> <p>When I broke down the pieces, I was left with the following steps to implement and optimize:</p> <ul> <li>Load the 10M rows worth of population data from Hive</li> <li>For every row, lookup the corresponding partnerid in the ID matching table from Scylla</li> <li>Count the resulting number of matches</li> </ul> <p>The main problem to compete with Spark 2 is that it is a distributed framework and Python by itself is not. So you can't possibly imagine outperforming Spark 2 with your single machine.</p> <p>However, let's remember that Spark 2 is shipped and ran on executors using YARN so we are firing up JVMs and dispatching containers all the time. This is a quite expensive process that we have a chance to avoid using Python!</p> <p>So what I needed was a distributed computation framework that would allow to load data in a partitioned way and run the lookups on all the partitions in parallel before merging the results. In Python, this framework exists and is named Dask!</p> <p>You will obviously need to have to deploy a dask topology (that's easy and well documented) to have a comparable number of dask workers than of Spark 2 executors (30 in my case) .</p> <p>The corresponding Python code samples are here.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#hive-scylla-results","title":"Hive + Scylla results","text":"<p>Reading the population id's from Hive, the workload can be split and executed concurrently on multiple dask workers.</p> <ul> <li>read the 10M population rows from Hive in a partitioned manner</li> <li>for each partition (slice of 10M), query Scylla to lookup the possibly matching partnerid</li> <li>create a dataframe from the resulting matches</li> <li>gather back all the dataframes and merge them</li> <li>count the number of matches</li> </ul> <p>The results showed that it is possible to compete with Spark 2 with Dask:</p> <ul> <li>hot cache: 2min (rounded up)</li> <li>cold cache: 6min</li> </ul> <p>Interestingly, those almost two minutes can be broken down like this:</p> <ul> <li>distributed read data from Hive: 50s</li> <li>distributed lookup from Scylla: 60s</li> <li>merge + count: 10s</li> </ul> <p>This meant that if I could cut down the reading of data from Hive I could go even faster!</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#parquet-scylla-results","title":"Parquet + Scylla results","text":"<p>Going further on my previous remark I decided to get rid of Hive and put the 10M rows population data in a parquet file instead. I ended up trying to find out the most efficient way to read and load a parquet file from HDFS.</p> <p>My conclusion so far is that you can't be the amazing libhdfs3 + pyarrow combo. It is faster to load everything on a single machine than loading from Hive on multiple ones!</p> <p>The results showed that I could almost get rid of a whole minute in the total process, effectively and easily beating Spark 2!</p> <ul> <li>hot cache: 1min 5s</li> <li>cold cache: 5min</li> </ul>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#notes-on-the-python-cassandra-driver","title":"Notes on the Python cassandra-driver","text":"<p>Tests using Python showed robust queries experiencing far less failures than the spark-cassandra-connector, even more during the cold start scenario.</p> <ul> <li>The usage of execute_concurrent() provides a clean and linear interface to submit a large number of queries while providing some level of concurrency control</li> <li>Increasing the concurrency parameter from 100 to 512 provided additional throughput, but increasing it more looked useless</li> <li>Protocol version 4 forbids the tuning of connection requests / number to some sort of auto configuration. All tentative to hand tune it (by lowering protocol version to 2) failed to achieve higher throughput</li> <li>Installation of libev on the system allows the cassandra-driver to use it to handle concurrency instead of asyncore with a somewhat lower load footprint on the worker node but no noticeable change on the throughput</li> <li>When reading a parquet file stored on HDFS, the hdfs3 + pyarrow combo provides an insane speed (less than 10s to fully load 10M rows of a single column)</li> </ul>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-5-play-with-high-availability","title":"Step 5: Play with High Availability","text":"<p>I was quite disappointed and surprised by the lack of maturity of the Cassandra community on this critical topic. Maybe the main reason is that the cassandra-driver allows for too many levels of configuration and strategies.</p> <p>I wrote this simple bash script to allow me to simulate node failures. Then I could play with handling those failures and retries on the Python client code.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#binbash","title":"!/bin/bash","text":"<p>iptables -t filter -X iptables -t filter -F</p> <p>ip=\"0.0.0.0/0\" for port in 9042 9160 9180 10000 7000; do     iptables -t filter -A INPUT -p tcp --dport ${port} -s ${ip} -j DROP     iptables -t filter -A OUTPUT -p tcp --sport ${port} -d ${ip} -j DROP done</p> <p>while true; do     trap break INT     clear     iptables -t filter -vnL     sleep 1 done</p> <p>iptables -t filter -X iptables -t filter -F iptables -t filter -vnL</p> <p>This topic is worth going in more details on a dedicated blog post that I shall write later on while providing code samples.</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#concluding-the-evaluation","title":"Concluding the evaluation","text":"<p>I'm happy to say that Scylla passed our production evaluation and will soon go live on our infrastructure!</p> <p>As I said at the beginning of this post, the conclusion of the evaluation has not been driven by the good figures we got out of our test workloads. Those are no benchmarks and never pretended to be but we could still prove that performances were solid enough to not be a blocker in the adoption of Scylla.</p> <p>Instead we decided on the following points of interest (in no particular order):</p> <ul> <li>data consistency</li> <li>production reliability</li> <li>datacenter awareness</li> <li>ease of operation</li> <li>infrastructure rationalisation</li> <li>developer friendliness</li> <li>costs</li> </ul> <p></p> <p>On the side, I tried Scylla on two other different use cases which proved interesting to follow later on to displace MongoDB again...</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#moving-to-production","title":"Moving to production","text":"<p>Since our relationship was great we also decided to partner with ScyllaDB and support them by subscribing to their enterprise offerings. They also accepted to support us using Gentoo Linux!</p> <p>We are starting with a three nodes heavy duty cluster:</p> <ul> <li>DELL R640<ul> <li>dual socket 2,6GHz 14C, 512GB RAM, Samsung 17xxx NVMe 3,2 TB</li> </ul> </li> </ul> <p>I'm eager to see ScyllaDB building up and will continue to help with my modest contributions. Thanks again to the ScyllaDB team for their patience and support during the POC!</p>","tags":["gentoo","nosql","scylla"]},{"location":"Tech%20Blog/2018/2018-04-03-py3status-v3-8/","title":"py3status v3.8","text":"<p>Another long awaited release has come true thanks to our community!</p> <p>The changelog is so huge that I had to open an issue and cry for help to make it happen... thanks again @lasers for stepping up once again :)</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-04-03-py3status-v3-8/#highlights","title":"Highlights","text":"<ul> <li>gevent support (-g option) to switch from threads scheduling to greenlets and reduce resources consumption</li> <li>environment variables support in i3status.conf to remove sensible information from your config</li> <li>modules can now leverage a persistent data store</li> <li>hundreds of improvements for various modules</li> <li>we now have an official debian package</li> <li>we reached 500 stars on github #vanity</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-04-03-py3status-v3-8/#milestone-39","title":"Milestone 3.9","text":"<ul> <li>try to release a version faster than every 4 months (j/k) ;)</li> </ul> <p>The next release will focus on bugs and modules improvements / standardization.</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-04-03-py3status-v3-8/#thanks-contributors","title":"Thanks contributors!","text":"<p>This release is their work, thanks a lot guys!</p> <ul> <li>alex o'neill</li> <li>anubiann00b</li> <li>cypher1</li> <li>daniel foerster</li> <li>daniel schaefer</li> <li>girst</li> <li>igor grebenkov</li> <li>james curtis</li> <li>lasers</li> <li>maxim baz</li> <li>nollain</li> <li>raspbeguy</li> <li>regnat</li> <li>robert ricci</li> <li>s\u00e9bastien delafond</li> <li>themistokle benetatos</li> <li>tobes</li> <li>woland</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/","title":"A botspot story","text":"<p>I felt like sharing a recent story that allowed us identify a bot in a haystack thanks to Scylla.</p> <p></p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#the-scenario","title":"The scenario","text":"<p>While working on loading up 2B+ of rows into Scylla from Hive (using Spark), we noticed a strange behaviour in the performances of one of our nodes:</p> <p></p> <p>So we started wondering why that server in blue was having those peaks of load and was clearly diverging from the two others... As we obviously expect the three nodes to behave the same, there were two options on the table:</p> <ol> <li>hardware problem on the node</li> <li>bad data distribution (bad schema design? consistent hash problem?)</li> </ol> <p>We shared this with our pals from ScyllaDB and started working on finding out what was going on.</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#the-investigation","title":"The investigation","text":"","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#hardware","title":"Hardware?","text":"<p>Hardware problem was pretty quickly evicted, nothing showed up on the monitoring and on the kernel logs. I/O queues and throughput were good:</p> <p></p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#data-distribution","title":"Data distribution?","text":"<p>Avi Kivity (ScyllaDB's CTO) quickly got the feeling that something was wrong with the data distribution and that we could be facing a hotspot situation. He quickly nailed it down to shard 44 thanks to the scylla-grafana-monitoring platform.</p> <p>Data is distributed between shards that are stored on nodes (consistent hash ring). This distribution is done by hashing the primary key of your data which dictates the shard it belongs to (and thus the node(s) where the shard is stored).</p> <p>If one of your keys is over represented in your original data set, then the shard it belongs to can be overly populated and the related node overloaded. This is called a hotspot situation.</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#tracing-queries","title":"tracing queries","text":"<p>The first step was to trace queries in Scylla to try to get deeper into the hotspot analysis. So we enabled tracing using the following formula to get about 1 trace per second in the system_traces namespace.</p> <p>tracing probability = 1 / expected requests per second throughput</p> <p>In our case, we were doing between 90K req/s and 150K req/s so we settled for 100K req/s to be safe and enabled tracing on our nodes like this:</p> <p># nodetool settraceprobability 0.00001</p> <p>Turns out tracing didn't help very much in our case because the traces do not include the query parameters in Scylla 2.1, it is becoming available in the soon to be released 2.2 version.</p> <p>NOTE: traces expire on the tables, make sure your TRUNCATE the events and sessions tables while iterating. Else you will have to wait for the next gc_grace_period (10 days by default) before they are actually removed. If you do not do that and generate millions of traces like we did, querying the mentioned tables will likely time out because of the \"tombstoned\" rows even if there is no trace inside any more.</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#looking-at-cfhistograms","title":"looking at cfhistograms","text":"<p>Glauber Costa was also helping on the case and got us looking at the cfhistograms of the tables we were pushing data to. That proved to be clearly highlighting a hotspot problem:</p> <p>histograms Percentile  SSTables     Write Latency      Read Latency    Partition Size        Cell Count                              (micros)          (micros)           (bytes)                 50%             0,00              6,00              0,00               258                 2 75%             0,00              6,00              0,00               535                 5 95%             0,00              8,00              0,00              1916                24 98%             0,00             11,72              0,00              3311                50 99%             0,00             28,46              0,00              5722                72 Min             0,00              2,00              0,00               104                 0 Max             0,00          45359,00              0,00          14530764            182785</p> <p>What this basically means is that 99% percentile of our partitions are small (5KB) while the biggest is 14MB! That's a huge difference and clearly shows that we have a hotspot on a partition somewhere.</p> <p>So now we know for sure that we have an over represented key in our data set, but what key is it and why?</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#the-culprit","title":"The culprit","text":"<p>So we looked at the cardinality of our data set keys which are SHA256 hashes and found out that indeed we had one with more than 1M occurrences while the second highest one was around 100K!...</p> <p>Now that we had the main culprit hash, we turned to our data streaming pipeline to figure out what kind of event was generating the data associated to the given SHA256 hash... and surprise! It was a client's quality assurance bot that was constantly browsing their own website with legitimate behaviour and identity credentials associated to it.</p> <p>So we modified our pipeline to detect this bot and discard its events so that it stops polluting our databases with fake data. Then we cleaned up the million of events worth of mess and traces we stored about the bot.</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#the-aftermath","title":"The aftermath","text":"<p>Finally, we cleared out the data in Scylla and tried again from scratch. Needless to say that the curves got way better and are exactly what we should expect from a well balanced cluster:</p> <p></p> <p>Thanks a lot to the ScyllaDB team for their thorough help and high spirited support!</p> <p>I'll quote them conclude this quick blog post:</p> <p></p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/","title":"Authenticating and connecting to a SSL enabled Scylla cluster using Spark 2","text":"<p>This quick article is a wrap up for reference on how to connect to ScyllaDB using Spark 2 when authentication and SSL are enforced for the clients on the Scylla cluster.</p> <p>We encountered multiple problems, even more since we distribute our workload using a YARN cluster so that our worker nodes should have everything they need to connect properly to Scylla.</p> <p>We found very little help online so I hope it will serve anyone facing similar issues (that's also why I copy/pasted them here).</p> <p>The authentication part is easy going by itself and was not the source of our problems, SSL on the client side was.</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#environment","title":"Environment","text":"<ul> <li>(py)spark: 2.1.0.cloudera2</li> <li>spark-cassandra-connector: datastax:spark-cassandra-connector: 2.0.1-s_2.11</li> <li>python: 3.5.5</li> <li>java: 1.8.0_144</li> <li>scylladb: 2.1.5</li> </ul>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#ssl-cipher-setup","title":"SSL cipher setup","text":"<p>The Datastax spark cassandra driver uses default the TLS_RSA_WITH_AES_256_CBC_SHA cipher that the JVM does not support by default. This raises the following error when connecting to Scylla:</p> <p>18/07/18 13:13:41 WARN channel.ChannelInitializer: Failed to initialize a channel. Closing: [id: 0x8d6f78a7] java.lang.IllegalArgumentException: Cannot support TLS_RSA_WITH_AES_256_CBC_SHA with currently installed providers</p> <p>According to the ssl documentation we have two ciphers available:</p> <ol> <li>TLS_RSA_WITH_AES_256_CBC_SHA</li> <li>TLS_RSA_WITH_AES_128_CBC_SHA</li> </ol> <p>We can get get rid of the error by lowering the cipher to TLS_RSA_WITH_AES_128_CBC_SHA using the following configuration:</p> <p>.config(\"spark.cassandra.connection.ssl.enabledAlgorithms\", \"TLS_RSA_WITH_AES_128_CBC_SHA\")\\</p> <p>However, this is not really a good solution and instead we'd be inclined to use the TLS_RSA_WITH_AES_256_CBC_SHA version. For this we need to follow this Datastax's procedure.</p> <p>Then we need to deploy the JCE security jars on our all client nodes, if using YARN like us this means that you have to deploy these jars to all your NodeManager nodes.</p> <p>For example by hand:</p> <p># unzip jce_policy-8.zip</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#cp-unlimitedjcepolicyjdk8jar-optoracle-jdk-bin-180144jrelibsecurity","title":"cp UnlimitedJCEPolicyJDK8/*.jar /opt/oracle-jdk-bin-1.8.0.144/jre/lib/security/","text":"","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#java-trust-store","title":"Java trust store","text":"<p>When connecting, the clients need to be able to validate the Scylla cluster's self-signed CA. This is done by setting up a trustStore JKS file and providing it to the spark connector configuration (note that you protect this file with a password).</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#keystore-vs-truststore","title":"keyStore vs trustStore","text":"<p>In SSL handshake purpose of trustStore is to verify credentials and purpose of keyStore is to provide credentials. keyStore in Java stores private key and certificates corresponding to the public keys and is required if you are a SSL Server or SSL requires client authentication. TrustStore stores certificates from third parties or your own self-signed certificates, your application identify and validates them using this trustStore.</p> <p>The spark-cassandra-connector documentation has two options to handle keyStore and trustStore.</p> <p>When we did not use the trustStore option, we would get some obscure error when connecting to Scylla:</p> <p>com.datastax.driver.core.exceptions.TransportException: [node/1.1.1.1:9042] Channel has been closed</p> <p>When enabling DEBUG logging, we get a clearer error which indicated a failure in validating the SSL certificate provided by the Scylla server node:</p> <p>Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#setting-up-the-truststore-jks","title":"setting up the trustStore JKS","text":"<p>You need to have the self-signed CA public certificate file, then issue the following command:</p> <p># keytool -importcert -file /usr/local/share/ca-certificates/MY_SELF_SIGNED_CA.crt -keystore COMPANY_TRUSTSTORE.jks -noprompt Enter keystore password: Re-enter new password:  Certificate was added to keystore</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#using-the-truststore","title":"using the trustStore","text":"<p>Now you need to configure spark to use the trustStore like this:</p> <p>.config(\"spark.cassandra.connection.ssl.trustStore.password\", \"PASSWORD\")\\ .config(\"spark.cassandra.connection.ssl.trustStore.path\", \"COMPANY_TRUSTSTORE.jks\")\\</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#spark-ssl-configuration-example","title":"Spark SSL configuration example","text":"<p>This wraps up the SSL connection configuration used for spark.</p> <p>This example uses pyspark2 and reads a table in Scylla from a YARN cluster:</p> <p>$ pyspark2 --packages datastax:spark-cassandra-connector:2.0.1-s_2.11 --files COMPANY_TRUSTSTORE.jks</p> <p>spark = SparkSession.builder.appName(\"scylla_app\")\\ .config(\"spark.cassandra.auth.password\", \"test\")\\ .config(\"spark.cassandra.auth.username\", \"test\")\\ .config(\"spark.cassandra.connection.host\", \"node1,node2,node3\")\\ .config(\"spark.cassandra.connection.ssl.clientAuth.enabled\", True)\\ .config(\"spark.cassandra.connection.ssl.enabled\", True)\\ .config(\"spark.cassandra.connection.ssl.trustStore.password\", \"PASSWORD\")\\ .config(\"spark.cassandra.connection.ssl.trustStore.path\", \"COMPANY_TRUSTSTORE.jks\")\\ .config(\"spark.cassandra.input.split.size_in_mb\", 1)\\ .config(\"spark.yarn.queue\", \"scylla_queue\").getOrCreate()</p> <p>df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_table\", keyspace=\"test\").load() df.show()</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-09-28-py3status-v3-13/","title":"py3status v3.13","text":"<p>I am once again lagging behind the release blog posts but this one is an important one.</p> <p>I'm proud to announce that our long time contributor @lasers has become an official collaborator of the py3status project!</p> <p>Dear @lasers, your amazing energy and overwhelming ideas have served our little community for a while. I'm sure we'll have a great way forward as we learn to work together with @tobes :) Thank you again very much for everything you do!</p> <p>This release is as much dedicated to you as it is yours :)</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-09-28-py3status-v3-13/#important-notice","title":"IMPORTANT notice","text":"<p>After this release, py3status coding style CI will enforce the 'black' formatter style.  </p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-09-28-py3status-v3-13/#highlights","title":"Highlights","text":"<p>Needless to say that the changelog is huge, as usual, here is a very condensed view:</p> <ul> <li> <p>documentation updates, especially on the formatter (thanks @L0ric0)  </p> </li> <li> <p>py3 storage: use $XDG_CACHE_HOME or ~/.cache</p> </li> <li>formatter: multiple variable and feature fixes and enhancements</li> <li>better config parser</li> <li>new modules: lm_sensors, loadavg, mail, nvidia_smi, sql, timewarrior, wanda_the_fish</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-09-28-py3status-v3-13/#thank-you-contributors","title":"Thank you contributors!","text":"<ul> <li>lasers</li> <li>tobes</li> <li>maximbaz</li> <li>cyrinux</li> <li>Lorenz Steinert @L0ric0</li> <li>wojtex</li> <li>horgix</li> <li>su8</li> <li>Maikel Punie</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/","title":"py3status v3.14","text":"<p>I'm happy to announce this release as it contains some very interesting developments in the project. This release was focused on core changes.  </p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/#important-notice","title":"IMPORTANT notice","text":"<p>There are now two optional dependencies to py3status:</p> <ul> <li> <p>gevent</p> <ul> <li>will monkey patch the code to make it concurrent</li> <li>the main benefit is to use an asynchronous loop instead of threads  </li> </ul> </li> <li> <p>pyudev</p> <ul> <li>will enable a udev monitor if a module asks for it (only xrandr so far)</li> <li>the benefit is described below  </li> </ul> </li> </ul> <p>To install them all using pip, simply do:  </p> <p>pip install py3status[all]</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/#modules-can-now-reactrefresh-on-udev-events","title":"Modules can now react/refresh on udev events","text":"<p>When pyudev is available, py3status will allow modules to subscribe and react to udev events!</p> <p>The xrandr module uses this feature by default which allows the module to instantly refresh when you plug in or off a secondary monitor. This also allows to stop running the xrandr command in the background and saves a lot of CPU!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/#highlights","title":"Highlights","text":"<ul> <li>py3status core uses black formatter</li> <li>fix default i3status.conf detection<ul> <li>add ~/.config/i3 as a default config directory, closes #1548</li> <li>add .config/i3/py3status in default user modules include directories</li> </ul> </li> <li>add markup (pango) support for modules (#1408), by @MikaYuoadas</li> <li>py3: notify_user module name in the title (#1556), by @lasers</li> <li>print module information to sdtout instead of stderr (#1565), by @robertnf</li> <li>battery_level module: default to using sys instead of acpi (#1562), by @eddie-dunn</li> <li>imap module: fix output formatting issue (#1559), by @girst</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/#thank-you-contributors","title":"Thank you contributors!","text":"<ul> <li>eddie-dunn</li> <li>girst</li> <li>MikaYuoadas</li> <li>robertnf</li> <li>lasers</li> <li>maximbaz</li> <li>tobes</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/","title":"Scylla Summit 2018 write-up","text":"<p>It's been almost one month since I had the chance to attend and speak at Scylla Summit 2018 so I'm relieved to finally publish a short write-up on the key things I wanted to share about this wonderful event!</p> <p></p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#make-scylla-boring","title":"Make Scylla boring","text":"<p>This statement of Glauber Costa sums up what looked to me to be the main driver of the engineering efforts put into Scylla lately: making it work so consistently well on any kind of workload that it's boring to operate :)  </p> <p>I will follow up on this statement to highlight the things I heard and (hopefully) understood during the summit. I hope you'll find it insightful.  </p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#reduced-operational-efforts","title":"Reduced operational efforts","text":"<p>The thread-per-core and queues design still has a lot of possibilities to be leveraged.  </p> <p>The recent addition of RPC streaming capabilities to seastar allows a drastic reduction in the time it takes the cluster to grow or shrink (data rebalancing / resynchronization).  </p> <p>Incremental compaction is also very promising as this background process is one of the most expensive there is in the database's design.  </p> <p>I was happy to hear that scylla-manager will soon be made available and free to use with basic features while retaining more advanced ones for enterprise version (like backup/restore). I also noticed that the current version was not supporting SSL enabled clusters to store its configuration. So I directly asked\u00a0Micha\u0142 for it and I'm glad that it will be released on version 1.3.1. </p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#performant-multi-tenancy","title":"Performant multi-tenancy","text":"<p>Why choose between real-time OLTP &amp; analytics OLAP workloads?  </p> <p>The goal here is to be able to run both on the same cluster by giving users the ability to assign \"SLA\" shares to ROLES. That's basically like pools on Hadoop at a much finer grain since it will create dedicated queues that will be weighted by their share.  </p> <p>Having one queue per usage and full accounting will allow to limit resources efficiently and users to have their say on their latency SLAs.  </p> <p>But Scylla also has a lot to do in the background to run smoothly. So while this design pattern was already applied to tamper compactions, a lot of work has also been done on automatic flow control and back pressure.  </p> <p>For instance, Materialized Views are updated asynchronously which means that while we can interact and put a lot of pressure on the table its based on (called the Main Table), we could overwhelm the background work that's needed to keep MVs View Tables in sync. To mitigate this, a smart back pressure approach was developed and will throttle the clients to make sure that Scylla can manage to do everything at the best performance the hardware allows!  </p> <p>I was happy to hear that work on tiered storage is also planned to better optimize disk space costs for certain workloads.  </p> <p>Last but not least, columnar storage optimized for time series and analytics workloads are also something the developers are looking at.  </p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#latency-is-expensive","title":"Latency is expensive","text":"<p>If you care for latency, you might be happy to hear that a new polling API (named IOCB_CMD_POLL) has been contributed by Christoph Hellwig and Avi Kivity to the 4.19 Linux kernel which avoids context switching I/O by using a shared ring between kernel and userspace. Scylla will be using it by default if the kernel supports it.  </p> <p>The iotune utility has been upgraded since 2.3 to generate an enhanced I/O configuration.  </p> <p>Also, persistent (disk backed) in-memory tables are getting ready and are very promising for latency sensitive workloads!  </p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#a-word-on-drivers","title":"A word on drivers","text":"<p>ScyllaDB has been relying on the Datastax drivers since the start. While it's a good thing for the whole community, it's important to note that the shard-per-CPU approach on data that Scylla is using is not known and leveraged by the current drivers.  </p> <p>Discussions took place and it seems that Datastax will not allow the protocol to evolve so that drivers could discover if the connected cluster is shard aware or not and then use this information to be more clever in which write/read path to use.  </p> <p>So for now ScyllaDB has been forking and developing their shard aware drivers for Java and Go (no Python yet... I was disappointed).  </p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#kubernetes-containers","title":"Kubernetes &amp; containers","text":"<p>The ScyllaDB guys of course couldn't avoid the Kubernetes frenzy so Moreno Garcia gave a lot of feedback and tips on how to operate Scylla on docker with minimal performance degradation.  </p> <p>Kubernetes has been designed for stateless applications, not stateful ones and Docker does some automatic magic that have rather big performance hits on Scylla. You will basically have to play with affinities to dedicate one Scylla instance to run on one server with a \"retain\" reclaim policy.  </p> <p>Remember that the official Scylla docker image runs with dev-mode enabled by default which turns off all performance checks on start. So start by disabling that and look at all the tips and literature that Moreno has put online!  </p> <p></p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#scylla-30","title":"Scylla 3.0","text":"<p>A lot has been written on it already so I will just be short on things that important to understand in my point of view.</p> <ul> <li> <p>Materialized Views do back fill the whole data set  </p> <ul> <li>this job is done by the view building process</li> <li>you can watch its progress in the system_distributed.view_build_status table  </li> </ul> </li> <li> <p>Secondary Indexes are Materialized Views under the hood  </p> <ul> <li> <p>it's like a reverse pointer to the primary key of the Main Table  </p> </li> <li> <p>so if you read the whole row by selecting on the indexed column, two reads will be issued under the hood: one on the indexed MV view table to get the primary key and one on the main table to get the rest of the columns  </p> </li> <li> <p>so if your workload is mostly interested by the whole row, you're better off creating a complete MV to read from than using a SI</p> </li> <li>this is even more true if you plan to do range scans as this double query could lead you to read from multiple nodes instead of one  </li> </ul> </li> <li> <p>Range scan is way more performant</p> <ul> <li>ALLOW FILTERING finally allows a great flexibility by providing server-side filtering!</li> </ul> </li> </ul> <p></p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#random-notes","title":"Random notes","text":"<p>Support for LWT (lightweight transactions) will be relying on a future implementation of the Raft consensus algorithm inside Scylla. This work will also benefits Materialized Views consistency. Duarte Nunes will be the one working on this and I envy him very much!  </p> <p>Support for search workloads is high in the ScyllaDB devs priorities so we should definitely hear about it in the coming months.</p> <p>Support for \"mc\" sstables (new generation format) is done and will reduce storage requirements thanks to metadata / data compression. Migration will be transparent because Scylla can read previous formats as well so it will upgrade your sstables as it compacts them.  </p> <p>ScyllaDB developers have not settled on how to best implement CDC. I hope they do rather soon because it is crucial in their ability to integrate well with Kafka!  </p> <p>Materialized Views, Secondary Indexes and filtering will benefit from the work on partition key and indexes intersections to avoid server side filtering on the coordinator. That's an important optimization to come!</p> <p>Last but not least, I've had the pleasure to discuss with Takuya Asada who is the packager of Scylla for RedHat/CentOS &amp; Debian/Ubuntu. We discussed Gentoo Linux packaging requirements as well as the recent and promising work on a relocatable package. We will collaborate more closely in the future!</p>","tags":["gentoo","scylla"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/","title":"py3status v3.16","text":"<p>Two py3status versions in less than a month? That's the holidays effect but not only!</p> <p>Our community has been busy discussing our way forward to 4.0 (see below) and organization so it was time I wrote a bit about that.</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#community","title":"Community","text":"","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#a-new-collaborator","title":"A new collaborator","text":"<p>First of all we have the great pleasure and honor to welcome Maxim Baz @maximbaz as a new collaborator on the project!  </p> <p>His engagement, numerous contributions and insightful reviews to py3status has made him a well known community member, not to mention his IRC support :)  </p> <p>Once again, thank you for being there Maxim!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#zen-of-py3status","title":"Zen of py3status","text":"<p>As a result of an interesting discussion, we worked on defining better how to contribute to py3status as well as a set of guidelines we agree on to get the project moving on smoothly.  </p> <p>Here is born the zen of py3status which extends the philosophy from the user point of view to the contributor point of view!  </p> <p>This allowed us to handle the numerous open pull requests and get their number down to 5 at the time of writing this post!  </p> <p>Even our dear @lasers don't have any open PR anymore :)</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#315-316-versions","title":"3.15 + 3.16 versions","text":"<p>Our magic @lasers has worked a lot on general modules options as well as adding support for i3-gaps added features such as border coloring and fine tuning.  </p> <p>Also interesting is the work of Thiago Kenji Okada @m45t3r around NixOS packaging of py3status. Thanks a lot for this work and for sharing Thiago!</p> <p>I also liked the question of Andreas Lundblad @aioobe asking if we could have a feature allowing to display a custom graphical output, such as a small PNG or anything upon clicking on the i3bar, you might be interested in following up the i3 issue he opened.  </p> <p>Make sure to read the amazing changelog for details, a lot of modules have been enhanced!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#highlights","title":"Highlights","text":"<ul> <li>You can now set a background, border colors and their urgent counterparts on a global scale or per module</li> <li>CI now checks for black format on modules, so now all the code base obey the black format style!</li> <li>All HTTP requests based modules now have a standard way to define HTTP timeout as well as the same 10 seconds default timeout</li> <li>py3-cmd now allows sending click events with modifiers</li> <li>The py3status -n / --interval command line argument has been removed as it was obsolete. We will ignore it if you have set it up, but better remove it to be clean</li> <li>You can specify your own i3status binary path using the new -u, --i3status command line argument thanks to @Dettorer and @lasers</li> <li>Since Yahoo! decided to retire its public &amp; free weather API, the weather_yahoo module has been removed</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#new-modules","title":"New modules","text":"<ul> <li>new conky module: display conky system monitoring (#1664), by lasers</li> <li>new module emerge_status: display information about running gentoo emerge (#1275), by AnwariasEu</li> <li>new module hueshift: change your screen color temperature (#1142), by lasers</li> <li>new module mega_sync: to check for MEGA service synchronization (#1458), by Maxim Baz</li> <li>new module speedtest: to check your internet bandwidth (#1435), by cyrinux</li> <li>new module usbguard: control usbguard from your bar (#1376), by cyrinux</li> <li>new module velib_metropole: display velib metropole stations and (e)bikes (#1515), by cyrinux</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#a-word-on-40","title":"A word on 4.0","text":"<p>Do you wonder what's gonna be in the 4.0 release? Do you have ideas that you'd like to share? Do you have dreams that you'd love to become true?  </p> <p>Then make sure to read and participate in the open RFC on 4.0 version!  </p> <p>Development has not started yet; we really want to hear from you.</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#thank-you-contributors","title":"Thank you contributors!","text":"<p>There would be no py3status release without our amazing contributors, so thank you guys!</p> <ul> <li>AnwariasEu</li> <li>cyrinux</li> <li>Dettorer</li> <li>ecks</li> <li>flyingapfopenguin</li> <li>girst</li> <li>Jack Doan</li> <li>justin j lin</li> <li>Keith Hughitt</li> <li>L0ric0</li> <li>lasers</li> <li>Maxim Baz</li> <li>oceyral</li> <li>Simon Legner</li> <li>sridhars</li> <li>Thiago Kenji Okada</li> <li>Thomas F. Duellmann</li> <li>Till Backhaus</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/","title":"Bye bye Google Analytics","text":"<p>A few days ago, I removed Google Analytics from my blog and trashed the associated account.</p> <p></p> <p>I've been part of the Marketing Tech and Advertising Tech industries for over 15 years. I design and operate data processing platforms (including web navigation trackers) for a living. So I thought that maybe sharing the reasons of why I took this decision might be of interest for some people. I'll keep it short.</p>"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/#my-convenience-is-not-a-enough-reason-to-send-your-data-to-google","title":"MY convenience is not a enough reason to send YOUR data to Google","text":"<p>The first and obvious question I asked myself is why did I (and so many people) set up this tracker on my web site?</p> <p>My initial answer was a mix of:</p> <ul> <li>convenience : it's easy to set up, there's a nice interface, you get a lot of details, you don't have to ask yourself how it's done, it just works</li> <li>insight : it sounded somewhat important to know who was visiting what content and somehow know about the interest of people visiting</li> </ul> <p>With also a (hopefully not too much) of:</p> <ul> <li>pride: are some blog posts popular? if so which one and let's try to do more like this!</li> </ul> <p>I don't think those are good enough reasons to add a tracker that sends YOUR data to Google.</p>"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/#convenience-kills-diversity","title":"Convenience kills diversity","text":"<p>I'm old enough to have witnessed the rise of internet and its availability to (almost) everyone. The first things I did when I could connect was create and host my own web site, it was great and looked ugly!</p> <p>But while Internet could have been a catalyst for diversity, it turned out to create an over concentration on services and tools that we think are hard to live without because of their convenience (and a human tendency for mimicry).</p> <p>When your choices are reduced and the mass adoption defines your standards, it's easy to let it go and pretend you don't care that much.</p> <p>I decided to stop pretending that I don't care. I don't want to participate in the concentration of web navigation tracking to Google.</p>"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/#open-internet-is-at-risk","title":"Open Internet is at risk","text":"<p>When diversity is endangered so is Open Internet. This idea that a rich ecosystem can bring their own value and be free to grow by using the data they generate or collect is threatened by the GAFA who are building walled gardens around OUR data.</p> <p>For instance, Google used the GDPR regulation as an excuse to close down the access to the data collected by their (so convenient) services. If a company (or you) wants to access / query this data (YOUR data) then you can only by using their billed tools.</p> <p>What should have been only a clear win for us people turned out to also benefit those super blocks and threaten diversity and Open Internet even more.</p> <p>Adding Google Analytics to your web site helps Google have a larger reach and tracking footprint on the whole web: imagine all those millions of web sites visits added together, that's where the value is for them. No wonder GA is free.</p> <p>So in this regard too, I decided to stop contributing to the empowerment of Google.</p>"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/#this-blog-is-tracking-free","title":"This blog is Tracking Free","text":"<p>So from now on if you want to share your thoughts of just let me know you enjoyed a post on this blog, take the lead on YOUR data and use the comment box.</p> <p>The choice is yours!</p>"},{"location":"Tech%20Blog/2019/2019-03-25-py3status-v3-17/","title":"py3status v3.17","text":"<p>I'm glad to announce a new (awaited) release of py3status featuring support for the sway window manager which allows py3status to enter the wayland environment!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-03-25-py3status-v3-17/#updated-configuration-and-custom-modules-paths-detection","title":"Updated configuration and custom modules paths detection","text":"<p>The configuration section of the documentation explains the updated detection of the py3status configuration file (with respect of XDG_CONFIG environment variables):</p> <ul> <li>~/.config/py3status/config</li> <li>~/.config/i3status/config</li> <li>~/.config/i3/i3status.conf</li> <li>~/.i3status.conf</li> <li>~/.i3/i3status.conf</li> <li>/etc/xdg/i3status/config</li> <li>/etc/i3status.conf</li> </ul> <p>Regarding custom modules paths detection, py3status does as described in the documentation:</p> <ul> <li>~/.config/py3status/modules</li> <li>~/.config/i3status/py3status</li> <li>~/.config/i3/py3status</li> <li>~/.i3/py3status</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-03-25-py3status-v3-17/#highlights","title":"Highlights","text":"<p>Lots of modules improvements and clean ups, see changelog.</p> <ul> <li>we worked on the documentation sections and content which allowed us to fix a bunch of typos</li> <li>our magic @lasers have worked a lot on harmonizing thresholds on modules along with a lot of code clean ups</li> <li>new module: scroll to scroll modules on your bar (#1748)</li> <li>@lasers has worked a lot on a more granular pango support for modules output (still work to do as it breaks some composites)</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-03-25-py3status-v3-17/#thanks-contributors","title":"Thanks contributors","text":"<ul> <li>Ajeet D'Souza</li> <li>@boucman</li> <li>Cody Hiar</li> <li>@cyriunx</li> <li>@duffydack</li> <li>@lasers</li> <li>Maxim Baz</li> <li>Thiago Kenji Okada</li> <li>Yaroslav Dronskii</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/","title":"Scylla: four ways to optimize your disk space consumption","text":"<p>We recently had to face free disk space outages on some of our scylla clusters and we learnt some very interesting things while outlining some improvements that could be made to the ScyllaDB guys.</p> <p></p>","tags":["gentoo","optimization","scylla"]},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#100-disk-space-usage","title":"100% disk space usage?","text":"<p>First of all I wanted to give a bit of a heads up about what happened when some of our scylla nodes reached (almost) 100% disk space usage.</p> <p>Basically they:</p> <ul> <li>stopped listening to client requests</li> <li>complained in the logs</li> <li>wouldn't flush commitlog (expected)</li> <li>abort their compaction work (which actually gave back a few GB of space)</li> <li>stay in a stuck / unable to stop state (unexpected, this has been reported)</li> </ul> <p>After restarting your scylla server, the first and obvious thing you can try to do to get out of this situation is to run the nodetool clearsnapshot command which will remove any data snapshot that could be lying around. That's a handy command to reclaim space usually.</p> <p>Reminder: depending on your compaction strategy, it is usually not advised to allow your data to grow over 50% of disk space...</p> <p>But that's only a patch so let's go down the rabbit hole and look at the optimization options we have.</p>","tags":["gentoo","optimization","scylla"]},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#optimize-your-schemas","title":"Optimize your schemas","text":"<p>Schema design and the types your choose for your columns have a huge impact on disk space usage! And in our case we indeed overlooked some of the optimizations that we could have done from the start and that did cost us a lot of wasted disk space. Fortunately it was easy and fast to change.</p> <p>To illustrate this, I'll take a sample of 100,000 rows of a simple and naive schema associating readings of 50 integers to a user ID:</p> <p>Note: all those operations were done using Scylla 3.0.3 on Gentoo Linux.</p> <p>CREATE TABLE IF NOT EXISTS test.not_optimized (     uid text,     readings list,     PRIMARY KEY(uid) ) WITH compression = {}; <p>Once inserted on disk, this takes about 250MB of disk space:</p> <p>250M    not_optimized-00cf1500520b11e9ae38000000000004</p> <p>Now depending on your use case, if those readings at not meant to be updated for example you could use a frozen list instead, which will allow a huge storage optimization:</p> <p>CREATE TABLE IF NOT EXISTS test.mid_optimized  (      uid text,      readings frozen&gt;,      PRIMARY KEY(uid)  ) WITH compression = {}; <p>With this frozen list we now consume 54MB of disk space for the same data!</p> <p>54M     mid_optimized-011bae60520b11e9ae38000000000004</p> <p>There's another optimization that we could do since our user ID are UUIDs. Let's switch to the uuid type instead of text:</p> <p>CREATE TABLE IF NOT EXISTS test.optimized  (      uid uuid,      readings frozen&gt;,      PRIMARY KEY(uid)  ) WITH compression = {}; <p>By switching to uuid, we now consume 50MB of disk space: that's a 80% reduced disk space consumption compared to the naive schema for the same data!</p> <p>50M     optimized-01f74150520b11e9ae38000000000004</p>","tags":["gentoo","optimization","scylla"]},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#enable-compression","title":"Enable compression","text":"<p>All those examples were not using compression. If your workload latencies allows it, you should probably enable compression on your sstables.</p> <p>Let's see its impact on our tables:</p> <p>ALTER TABLE test.not_optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}; ALTER TABLE test.mid_optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}; ALTER TABLE test.optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'};</p> <p>Then we run a nodetool compact test to force a (re)compaction of all the sstables and we get:</p> <p>63M     not_optimized-00cf1500520b11e9ae38000000000004 28M     mid_optimized-011bae60520b11e9ae38000000000004 24M     optimized-01f74150520b11e9ae38000000000004</p> <p>Compression is really a great gain here allowing another 50% reduced disk space usage reduction on our optimized table!</p>","tags":["gentoo","optimization","scylla"]},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#switch-to-the-new-mc-sstable-format","title":"Switch to the new \"mc\" sstable format","text":"<p>Since the Scylla 3.0 release you can use the latest \"mc\" sstable storage format on your scylla clusters. It promises a greater efficiency for usually a way more reduced disk space consumption!</p> <p>It is not enabled by default, you have to add the enable_sstables_mc_format: true parameter to your scylla.yaml for it to be taken into account.</p> <p>Since it's backward compatible, you have nothing else to do as new compactions will start being made using the \"mc\" storage format and the scylla server will seamlessly read from old sstables as well.</p> <p>But in our case of immediate disk space outage, we switched to the new format one node at a time, dropped the data from it and ran a nodetool rebuild to reconstruct the whole node using the new sstable format.</p> <p>Let's demonstrate its impact on our test tables: we add the option to the scylla.yaml file, restart scylla-server and run nodetool compact test again:</p> <p>49M     not_optimized-00cf1500520b11e9ae38000000000004 26M     mid_optimized-011bae60520b11e9ae38000000000004 22M     optimized-01f74150520b11e9ae38000000000004</p> <p>That's a pretty cool gain of disk space, even more for the not optimized version of our schema!</p> <p>So if you're in great need of disk space or it is hard for you to change your schemas, switching to the new \"mc\" sstable format is a simple and efficient way to free up some space without effort.</p>","tags":["gentoo","optimization","scylla"]},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#consider-using-secondary-indexes","title":"Consider using secondary indexes","text":"<p>While denormalization is the norm (yep.. legitimate pun) in the NoSQL world this does not mean we have to duplicate everything all the time. A good example lies in the internals of secondary indexes if your workload can compromise with its moderate impact on latency.</p> <p>Secondary indexes on scylla are built on top of Materialized Views that basically stores an up to date pointer from your indexed column to your main table partition key. That means that secondary indexes MVs are not duplicating all the columns (and thus the data) from your main table as you would have to do when denormalizing a table to query by another column: this saves disk space!</p> <p>This of course comes with a latency drawback because if your workload is interested in the other columns than the partition key of the main table, the coordinator node will actually issue two queries to get all your data:</p> <ol> <li>query the secondary index MV to get the pointer to the partition key of the main table</li> <li>query the main table with the partition key to get the rest of the columns you asked for</li> </ol> <p>This has been an effective trick to avoid duplicating a table and save disk space for some of our workloads!</p>","tags":["gentoo","optimization","scylla"]},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#not-a-tip-move-the-commitlog-to-another-disk-partition","title":"(not a tip) Move the commitlog to another disk / partition?","text":"<p>This should only be considered as a sort of emergency procedure or for cost efficiency (cheap disk tiering) on non critical clusters.</p> <p>While this is possible even if the disk is not formatted using XFS, it not advised to separate the commitlog from data on modern SSD/NVMe disks but... you technically can do it (as we did) on non production clusters.</p> <p>Switching is simple, you just need to change the commitlog_directory parameter in your scylla.yaml file.</p>","tags":["gentoo","optimization","scylla"]},{"location":"Tech%20Blog/2019/2019-04-17-meet-the-py3status-logo/","title":"Meet the py3status logo","text":"<p>I'm proud and very pleased to introduce the py3status logo that Tobaloidee has created for our beloved project!</p> <p></p> <p>We've been discussing and dreaming about this for a while in the dedicated logo issue. So when Tobaloidee came with his awesome concept and I first saw the logo I was amazed at how he perfectly gave life to the poor brief that I expressed.</p>","tags":["logo","py3status"]},{"location":"Tech%20Blog/2019/2019-04-17-meet-the-py3status-logo/#concept","title":"Concept","text":"<p>Thanks again Tobaloidee and of course all of the others who participated (with a special mention to @cyrinux's girlfriend)!</p>","tags":["logo","py3status"]},{"location":"Tech%20Blog/2019/2019-04-17-meet-the-py3status-logo/#variants","title":"Variants","text":"<p>We have a few other variants that exist, I'm putting some of them here for quick download &amp; use.</p> <p></p> <p></p> <p></p>","tags":["logo","py3status"]},{"location":"Tech%20Blog/2019/2019-09-22-py3status-v3-20-europython-2019-edition/","title":"py3status v3.20 - EuroPython 2019 edition","text":"<p>Shame on me to post this so long after it happened... Still, that's a funny story to tell and a lot of thank you to give so let's go!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-09-22-py3status-v3-20-europython-2019-edition/#the-py3status-europython-2019-sprint","title":"The py3status EuroPython 2019 sprint","text":"<p>I've attended all EuroPython conferences since 2013. It's a great event and I encourage everyone to get there!</p> <p>The last two days of the conference week are meant for Open Source projects collaboration: this is called sprints.</p> <p>I don't know why but this year I decided that I would propose a sprint to welcome anyone willing to work on py3status to come and help...</p> <p>To be honest I was expecting that nobody would be interested so when I sat down at an empty table on saturday I thought that it would remain empty... but hey, I would have worked on py3status anyway so every option was okay!</p> <p>Then two students came. They ran Windows and Mac OS and never heard of i3wm or py3status but were curious so I showed them. They could read C so I asked them if they could understand how i3status was reading its horrible configuration file... and they did!</p> <p>Then Oliver Bestwalter (main maintainer of tox) came and told me he was a long time py3status user... followed by Hubert Bry\u0142kowski and \u00d3lafur Bjarni! Wow..</p> <p>We joined forces to create a py3status module that allows the use of the great PewPew hardware device created by Radomir Dopieralski (which was given to all attendees) to control i3!</p> <p></p> <p>And we did it and had a lot of fun!</p> <p>https://www.youtube.com/watch?v=0Oy2CE2GZ7s</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-09-22-py3status-v3-20-europython-2019-edition/#olivers-major-contribution","title":"Oliver's major contribution","text":"<p>The module itself is awesome okay... but thanks to Oliver's experience with tox he proposed and contributed one of the most significant feature py3status has had: the ability to import modules from other pypi packages!</p> <p>The idea is that you have your module or set of modules. Instead of having to contribute them to py3status you could just publish them to pypi and py3status will automatically be able to detect and load them!</p> <p>The usage of entry points allow custom and more distributed modules creation for our project!</p> <p>Read more about this amazing feature on the docs.</p> <p>All of this happened during EuroPython 2019 and I want to extend once again my gratitude to everyone who participated!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-09-22-py3status-v3-20-europython-2019-edition/#thank-you-contributors","title":"Thank you contributors","text":"<p>Version 3.20 is also the work of cool contributors. See the changelog.</p> <ul> <li>Daniel Peukert</li> <li>Kevin Pulo</li> <li>Maxim Baz</li> <li>Piotr Miller</li> <li>Rodrigo Leite</li> <li>lasers</li> <li>luto</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/","title":"Scylla Summit 2019","text":"<p>I've had the pleasure to attend again and present at the Scylla Summit in San Francisco and the honor to be awarded the Most innovative use case of Scylla.</p> <p>It was a great event, full of friendly people and passionate conversations. Peter did a great full write-up of it already so I wanted to share some of my notes instead...</p> <p>This a curated set of topics that I happened to question or discuss in depth so this post is not meant to be taken as a full coverage of the conference.</p>","tags":["gentoo","scylla","summit"]},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#scylla-manager-version-2","title":"Scylla Manager version 2","text":"<p>The upcoming version of scylla-manager is dropping its dependency on SSH setup which will be replaced by an agent, most likely shipped as a separate package.</p> <p>On the features side, I was a bit puzzled by the fact that ScyllaDB is advertising that its manager will provide a repair scheduling window so that you can control when it's running or not.</p> <p>Why did it struck me you ask?</p> <p>Because MongoDB does the same thing within its balancer process and I always thought of this as a patch to a feature that the database should be able to cope with by itself.</p> <p>And that database-do-it-better-than-you motto is exactly one of the promises of Scylla, the boring database, so smart at handling workload impacts on performance that you shouldn't have to start playing tricks to mitigate them... I don't want this time window feature on scylla-manager to be a trojan horse on the demise of that promise!</p>","tags":["gentoo","scylla","summit"]},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#kubernetes","title":"Kubernetes","text":"<p>They almost got late on this but are working hard to play well with the new toy of every tech around the world. Helm charts are also being worked on!</p> <p>The community developed scylla operator by Yannis is now being worked on and backed by ScyllaDB. It can deploy, scale up and down a cluster.</p> <p>Few things to note:</p> <ul> <li>it's using a configmap to store the scylla config</li> <li>no TLS support yet</li> <li>no RBAC support yet</li> <li>kubernetes networking is lighter on the network performance hit that was seen on Docker</li> <li>use placement strategies to dedicate kubernetes nodes to scylla!</li> </ul>","tags":["gentoo","scylla","summit"]},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#change-data-capture","title":"Change Data Capture","text":"<p>Oh boy this one was awaited... but it's now coming soon!</p> <p>I inquired about it's performance impact since every operation will be written to a table. Clearly my questioning was a bit alpha since CDC is still being worked on.</p> <p>I had the chance to discuss ideas with Kamil, Tzach and Dor: one of the thing that one of my colleague Julien asked for was the ability for the CDC to generate an event when a tombstone is written so we could actually know when a specific data expired!</p> <p>I want to stress a few other things too:</p> <ul> <li>default TTL on CDC table is 24H</li> <li>expect I/O impact (logical)</li> <li>TTL tombstones can have a hidden disk space cost and nobody was able to tell me if the CDC table was going to be configured with a lower gc_grace_period than the default 10 days so that's something we need to keep in mind and check for</li> <li>there was no plan to add user information that would allow us to know who actually did the operation, so that's something I asked for because it could be used as a cheap and open source way to get auditing!</li> </ul>","tags":["gentoo","scylla","summit"]},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#lightweight-transactions","title":"LightWeight Transactions","text":"<p>Another so long awaited feature is also coming from the amazing work and knowledge of Konstantin. We had a great conversation about the differences between the currently worked on Paxos based LWT implementation and the maybe later Raft one.</p> <p>So yes, the first LWT implementation will be using Paxos as a consensus algorithm. This will make the LWT feature very consistent while having it slower that what could be achieved using Raft. That's why ScyllaDB have plans on another implementation that could be faster with less data consistency guarantees.</p>","tags":["gentoo","scylla","summit"]},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#user-defined-functions-aggregations","title":"User Defined Functions / Aggregations","text":"<p>This one is bringing the Lua language inside Scylla!</p> <p>To be precise, it will be a Lua JIT as its footprint is low and Lua can be cooperative enough but the ScyllaDB people made sure to monitor its violations (when it should yield but does not) and act strongly upon them.</p> <p>I got into implementation details with Avi, this is what I noted:</p> <ul> <li>lua function return type is not checked at creation but at execution, so expect runtime errors if your lua code is bad</li> <li>since lua is lightweight, there's no need to assign a core to lua execution</li> <li>I found UDA examples, like top-k rows, to be very similar to the Map/Reduce logic</li> <li>UDF will allow simpler token range full table scans thanks to syntax sugar</li> <li>there will be memory limits applied to result sets from UDA, and they will be tunable</li> </ul>","tags":["gentoo","scylla","summit"]},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#text-search","title":"Text search","text":"<p>Dejan is the text search guy at ScyllaDB and the one who kindly implemented the LIKE feature we asked for and that will be released in the upcoming 3.2 version.</p> <p>We discussed ideas and projected use cases to make sure that what's going to be worked on will be used!</p>","tags":["gentoo","scylla","summit"]},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#redis-api","title":"Redis API","text":"<p>I've always been frustrated about Redis because while I love the technology I never trusted its clustering and scaling capabilities.</p> <p>What if you could scale your Redis like Scylla without giving up on performance? That's what the implementation of the Redis API backed by Scylla will get us!</p> <p>I'm desperately looking forward to see this happen!</p>","tags":["gentoo","scylla","summit"]},{"location":"Tech%20Blog/2020/2020-04-14-py3status-v3-28-goodbye-py2-6-3-4/","title":"py3status v3.28 - goodbye py2.6-3.4","text":"<p>The newest version of py3status starts to enforce the deprecation of Python 2.6 to 3.4 (included) initiated by Thiago Kenji Okada more than a year ago and orchestrated by Hugo van Kemenade via #1904 and #1896.</p> <p>Thanks to Hugo, I discovered a nice tool by @asottile to update your Python code base to recent syntax sugars called pyupgrade!</p> <p>Debian buster users might be interested in the installation war story that @TRS-80 kindly described and the final (and documented) solution found.</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2020/2020-04-14-py3status-v3-28-goodbye-py2-6-3-4/#changelog-since-v326","title":"Changelog since v3.26","text":"<ul> <li>drop support for EOL Python 2.6-3.4 (#1896), by Hugo van Kemenade</li> <li>i3status: support read_file module (#1909), by @lasers thx to @dohseven</li> <li>clock module: add \"locale\" config parameter to change time representation (#1910), by inemajo</li> <li>docs: update debian instructions fix #1916</li> <li>mpd_status module: use currentsong command if possible (#1924), by girst</li> <li>networkmanager module: allow using the currently active AP in formats (#1921), by Beno\u00eet Dardenne</li> <li>volume_status module: change amixer flag ordering fix #1914 (#1920)</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2020/2020-04-14-py3status-v3-28-goodbye-py2-6-3-4/#thank-you-contributors","title":"Thank you contributors","text":"<ul> <li>Thiago Kenji Okada</li> <li>Hugo van Kemenade</li> <li>Beno\u00eet Dardenne</li> <li>@dohseven</li> <li>@inemajo</li> <li>@girst</li> <li>@lasers</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2020/2020-08-24-python-scylla-driver-how-we-unleashed-the-scylla-monsters-performance/","title":"Python scylla-driver: how we unleashed the Scylla monster's performance","text":"<p>At Scylla summit 2019 I had the chance to meet Israel Fruchter and we dreamed of working on adding shard-awareness support to the Python cassandra-driver which would be known as scylla-driver.</p> <p>A few months later, when Israel reached out to me on the Scylla-Users #pythonistas Slack channel with the first draft PR I was so excited that I jumped in the wagon to help!</p> <p>The efforts we put into the scylla-driver paid off and significantly improved the performance of the production applications that I had the chance to switch to using it by 15 to 25%!</p> <p></p> <p>Before we reached those numbers and even released the scylla-driver to PyPi, EuroPython 2020 RFP was open and I submitted a talk proposal which was luckily accepted by the community.</p> <p>So I had the chance to deep-dive into Cassandra vs Scylla architecture differences, explain the rationale behind creating the scylla-driver and give Python code details on how we implemented it and the challenges we faced doing so. Check my talk spage for</p> <p>I also explained that I wrote an RFC on the scylla-dev mailing list which lead the developers of Scylla to implement a new connection-to-shard algorithm that will allow clients connecting to a new listening port to select the actual shard they want a connection to.</p> <p>This is an expected major optimization from the current mostly random way of connecting to all shards and I'm happy to say that it's been implemented and is ready to be put to use by all the scylla drivers.</p> <p>I've recently been contacted by PyCon India and other Python related conferences organizers for a talk so I've submitted one to PyCon India where I hope I'll be able to showcase even better numbers thanks to the new algorithm!</p> <p>After my Europython talk we also had very interesting discussions with Pau Freixes about his work on a fully asynchronous Python driver that wraps the C++ driver to get the best possible performance. First results are absolutely amazing so if you're interested in this, make sure to give it a try and contribute to the driver!</p> <p>Stay tuned for more amazing query latencies ;)</p>","tags":["python","scylla"]},{"location":"Tech%20Blog/2020/2020-09-07-py3status-v3-29/","title":"py3status v3.29","text":"<p>Almost 5 months after the latest release (thank you COVID) I'm pleased and relieved to have finally packaged and pushed py3status v3.29 to PyPi and Gentoo portage!</p> <p>This release comes with a lot of interesting contributions from quite a bunch of first-time contributors so I thought that I'd thank them first for a change!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2020/2020-09-07-py3status-v3-29/#thank-you-contributors","title":"Thank you contributors!","text":"<ul> <li>Jacotsu</li> <li>lasers</li> <li>Marc Poulhi\u00e8s</li> <li>Markus Sommer</li> <li>raphaunix</li> <li>Ricardo P\u00e9rez</li> <li>vmoyankov</li> <li>Wilmer van der Gaast</li> <li>Yaroslav Dronskii</li> </ul>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2020/2020-09-07-py3status-v3-29/#so-whats-new-in-v329","title":"So what's new in v3.29?","text":"<p>Two new exciting modules are in!</p> <ul> <li>prometheus module: to display your promQL queries on your bar</li> <li>watson module: for the watson time-tracking tool</li> </ul> <p>Then some interesting bug fixes and enhancements are to be noted</p> <ul> <li>py3.requests: return empty json on remote server problem fix #1401</li> <li>core modules: remove deprecated function, fix type annotation support (#1942)</li> </ul> <p>Some modules also got improved</p> <ul> <li>battery_level module: add power consumption placeholder (#1939) + support more battery paths detection (#1946)</li> <li>do_not_disturb module: change pause default from False to True</li> <li>mpris module: implement broken chromium mpris interface workaround (#1943)</li> <li>sysdata module: add {mem,swap}_free, {mem,swap}_free_unit, {mem,swap}_free_percent + try to use default intel/amd sensors first</li> <li>google_calendar module: fix imports for newer google-python-client-api versions (#1948)</li> </ul> <p>Next version of py3status will certainly drop support for EOL Python 3.5!</p>","tags":["gentoo","portage","py3status","release"]},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/","title":"Renaming and reshaping Scylla tables using scylla-migrator","text":"<p>We have recently faced a problem where some of the first Scylla tables we created on our main production cluster were not in line any more with the evolved schemas that recent tables are using.</p> <p>This typical engineering problem requires either to keep those legacy tables and data queries or to migrate it to the more optimal model with the bandwagon of applications to be modified to query the data the new way... That's something nobody likes doing but hey, we don't like legacy at Numberly so let's kill that one!</p> <p>To overcome this challenge we used the scylla-migrator project and I thought it could be useful to share this experience.</p>","tags":["gentoo","scylla","spark"]},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#how-and-why-our-schema-evolved","title":"How and why our schema evolved","text":"<p>When we first approached ID matching tables we chose to answer two problems at the same time: query the most recent data and keep the history of the changes per source ID.</p> <p>This means that those tables included a date as part of their PRIMARY KEY while the partition key was obviously the matching table ID we wanted to lookup from:</p> <pre><code>CREATE TABLE IF NOT EXISTS ids_by_partnerid(  \npartnerid text,  \nid text,  \ndate timestamp,  \nPRIMARY KEY ((partnerid), date, id)  \n)  \nWITH CLUSTERING ORDER BY (date DESC)\n</code></pre> <p>Making a table with an ever changing date in the clustering key creates what we call a history table. In the schema above the uniqueness of a row is not only defined by a partner_id / id couple but also by its date!</p> <p>Quick caveat: you have to be careful about the actual date timestamp resolution since you may not want to create a row for every second of the same partner_id / id couple (we use an hour resolution).</p> <p>History tables are good for analytics and we also figured we could use them for batch and real time queries where we would be interested in the \"most recent ids for the given partner_id\" (sometimes flavored with a LIMIT):</p> <pre><code>SELECT id FROM ids_by_partnerid WHERE partner_id = \"AXZAZLKDJ\" ORDER BY date DESC;\n</code></pre> <p>As time passed, real time Kafka pipelines started to query these tables hard and were mostly interested in \"all the ids known for the given partner_id\".</p> <p>A sort of DISTINCT(id) is out of the scope of our table! For this we need a table schema that represents a condensed view of the data. We call them compact tables and the only difference with the history table is that the date timestamp is simply not part of the PRIMARY KEY:</p> <pre><code>CREATE TABLE IF NOT EXISTS ids_by_partnerid(\npartnerid text,\nid text,\nseen_date timestamp,\nPRIMARY KEY ((partnerid), id)\n)\n</code></pre> <p>To make that transition happen we thus wanted to:</p> <ul> <li>rename history tables with an __history_ suffix so that they are clearly identified as such</li> <li>get a compacted version of the tables (by keeping their old name) while renaming the date column name to seen_date</li> <li>do it as fast as possible since we will need to stop our feeding pipeline and most of our applications during the process...</li> </ul> <p>STOP: it's not possible to rename a table in CQL!</p> <p></p>","tags":["gentoo","scylla","spark"]},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#scylla-migrator-to-the-rescue","title":"Scylla-migrator to the rescue","text":"<p>We decided to abuse the scylla-migrator to perform this perilous migration.</p> <p>As it was originally designed to help users migrate from Cassandra to Scylla by leveraging Spark it seemed like a good fit for the task since we happen to own Spark clusters powered by Hadoop YARN.</p>","tags":["gentoo","scylla","spark"]},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#building-scylla-migrator-for-spark-24","title":"Building scylla-migrator for Spark &lt; 2.4","text":"<p>Recent scylla-migrator does not support older Spark versions. The trick is to look at the README.md git log and checkout the hopefully right commit that supports your Spark cluster version.</p> <p>In our case for Spark 2.3 we used git commit bc82a57e4134452f19a11cd127bd4c6a25f75020.</p> <p>On Gentoo, make sure to use dev-java/sbt-bin since the non binary version is vastly out of date and won't build the project. You need at least version 1.3.</p>","tags":["gentoo","scylla","spark"]},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#the-scylla-migrator-plan","title":"The scylla-migrator plan","text":"<p>The documentation explains that we need a config file that points to a source cluster+table and a destination cluster+table as long as they have the same schema structure...</p> <p>Renaming is then as simple as duplicating the schema using CQLSH and running the migrator!</p> <p>But what about our compacted version of our original table? The schema is different from the source table!...</p> <p>Good news is that as long as all your columns remain present, you can also change the PRIMARY KEY of your destination table and it will still work!</p> <p>This make the scylla-migrator an amazing tool to reshape or pivot tables!</p> <ul> <li>the column date is renamed to seen_date: that's okay, scylla-migrator supports column renaming (it's a Spark dataframe after all)!</li> <li>the PRIMARY KEY is different in the compacted table since we removed the 'date' from the clustering columns: we'll get a compacted table for free!</li> </ul>","tags":["gentoo","scylla","spark"]},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#using-scylla-migrator","title":"Using scylla-migrator","text":"<p>The documentation is a bit poor on how to submit your application to a Hadoop YARN cluster but that's kind of expected.</p> <p>It also did not mention how to connect to a SSL enabled cluster (are there people really not using SSL on the wire in their production environment?)... anyway let's not start a flame war :)</p> <p>The trick that will save you is to know that you can append all the usual Spark options that are available in the spark-cassandra-connector!</p> <p>Submitting to a Kerberos protected Hadoop YARN cluster targeting a SSL enabled Scylla cluster then looks like this:</p> <pre><code>export JAR_NAME=target/scala-2.11/scylla-migrator-assembly-0.0.1.jar\nexport KRB_PRINCIPAL=USERNAME\n\nspark2-submit \\\n --name ScyllaMigratorApplication \\\n --class com.scylladb.migrator.Migrator  \\\n --conf spark.cassandra.connection.ssl.clientAuth.enabled=True  \\\n --conf spark.cassandra.connection.ssl.enabled=True  \\\n --conf spark.cassandra.connection.ssl.trustStore.path=jssecacerts  \\\n --conf spark.cassandra.connection.ssl.trustStore.password=JKS_PASSWORD  \\\n --conf spark.cassandra.input.consistency.level=LOCAL_QUORUM \\\n --conf spark.cassandra.output.consistency.level=LOCAL_QUORUM \\\n --conf spark.scylla.config=config.yaml \\\n --conf spark.yarn.executor.memoryOverhead=1g \\\n --conf spark.blacklist.enabled=true  \\\n --conf spark.blacklist.task.maxTaskAttemptsPerExecutor=1  \\\n --conf spark.blacklist.task.maxTaskAttemptsPerNode=1  \\\n --conf spark.blacklist.stage.maxFailedTasksPerExecutor=1  \\\n --conf spark.blacklist.stage.maxFailedExecutorsPerNode=1  \\\n --conf spark.executor.cores=16 \\\n --deploy-mode client \\\n --files jssecacerts \\\n --jars ${JAR_NAME}  \\\n --keytab ${KRB_PRINCIPAL}.keytab  \\\n --master yarn \\\n --principal ${KRB_PRINCIPAL}  \\\n ${JAR_NAME}\n</code></pre> <p>Note that we chose to apply a higher consistency level to our reads using a LOCAL_QUORUM instead of the default LOCAL_ONE. I strongly encourage you to do the same since it's appropriate when you're using this kind of tool!</p> <p>Column renaming is simply expressed in the configuration file like this:</p> <pre><code># Column renaming configuration.\nrenames:\n  - from: date\n    to: seen_date\n</code></pre>","tags":["gentoo","scylla","spark"]},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#tuning-scylla-migrator","title":"Tuning scylla-migrator","text":"<p>While easy to use, tuning scylla-migrator to operate those migrations as fast as possible turned out to be a real challenge (remember we have some production applications shut down during the process).</p> <p>Even using 300+ Spark executors I couldn't get my Scylla cluster utilization to more than 50% and migrating a single table with a bit more than 1B rows took almost 2 hours...</p> <p></p> <p>We found the best knobs to play with thanks to the help of Lubos Kosco and this blog post from ScyllaDB:</p> <ul> <li>Increase the splitCount setting: more splits means more Spark executors will be spawned and more tasks out of it. While it might be magic on a pure Spark deployment it's not that amazing on a Hadoop YARN one where executors are scheduled in containers with 1 core by default. We simply moved it from 256 to 384.</li> <li>Disable compaction on destination tables schemas. This gave us a big boost and saved the day since it avoids adding the overhead of compacting while you're pushing down data hard!</li> </ul> <p>To disable compaction on a table simply:</p> <pre><code>ALTER TABLE ids_by_partnerid_history WITH compaction = {'class': 'NullCompactionStrategy'};\n</code></pre> <p>Remember to run a manual compaction using <code>nodetool compact &lt;keyspace&gt; &lt;table&gt;</code> and to enable compaction back on your tables once you're done!</p> <p>Happy Scylla tables mangling!</p>","tags":["gentoo","scylla","spark"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/","title":"From Wordpress to MkDocs","text":"<p>The idea of a statically built website has been in my mind for a while but I never found a satisfying stack to make it happen until I discovered and got intimate with MkDocs.</p> <p>Building a static website out of any kind of formatted text file neither new nor hard to do. But when I was interested in the subject a while ago, the ecosystem to support it and make it useful was not as mature as it is today.</p>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#my-website-stack-wish-list","title":"My website stack wish list","text":"<p>What I wanted was the ability to:</p> <ul> <li>build a responsive website out of simple text formatted files</li> <li>have a versioned, historized and Open Source view of my website sources on git</li> <li>work on different kind of content, from simple pages to blog posts to tutorials</li> <li>test it locally</li> <li>deploy and host it seamlessly (and at no cost if possible)</li> <li>have everything automated (except the actual writing, ok!)</li> </ul> <p>Bonus was to be able to:</p> <ul> <li>have the possibility to add dynamic content in the build process</li> </ul>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#mkdocs","title":"MkDocs?","text":"<p>Yep that means that I got the crazy idea of using a project initially designed to ease the creation of technical documentation of projects for my website.</p> <p>Quote</p> <p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introduction below, then check the User Guide for more info.</p> <p>After you swallow the possible shock or fear, let's reconsider my stack wish list above... After all, a git based website made out of text files that is built by a CI and hosted through CD looks exactly the same to me as a technical documentation build and hosting process!</p> <p>All it needs is some nice sugar to accommodate with specific needs.</p>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#converting-my-wordpress-content-to-markdown","title":"Converting my Wordpress content to Markdown","text":"<p>I used the nice wordpress export to markdown project which did the work perfectly for me as I could run the export to output a file structure fitting my mkdocs file hierarchy needs.</p>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#the-ultrabugfr-stack","title":"The ultrabug.fr stack","text":"","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#git","title":"Git","text":"<p>Let's start with the obvious git to get revision control over the sources of the website. Be it text files, media files and configuration files: everything is on git!</p>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#mkdocs_1","title":"MkDocs","text":"<p>Now MkDocs comes into play as it offers a straightforward way to structurate and configure the resulting website.</p> <p>By itself it does not offer all the features I needed. For example, a blog section needs to sort articles by the \"most recent\" first while some other sections of the website simply need alphabetical ordering. Wait, what if this particular sub-section you wanted first? What about emojis or nice thumbnails?</p> <p>Here is a list of what I'm using to enrich MkDocs and accommodate my needs:</p> <ul> <li> mkdocs-material: this is a responsive and good looking theme for MkDocs that offers some nice features to present your content in a lean way.</li> <li> mkdocs-redirects: I did not want to break my old links so I'm using this mkdocs plugin to make sure that my old Wordpress content is still redirected to the new mkdocs structure URLs of this website.</li> <li> python markdown extensions: to have nice markdown extensions like these checkboxes and of course emojis </li> <li> mkdocs-awesome-pages-plugin: this one is the ultimate plugin if like me you need to control the ordering of your navigation!</li> </ul>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#multi-language-support","title":"Multi-language support","text":"<p>MkDocs does not support language localization and I wanted to be able to propose some of my website sections and content in my mother tongue (French) and in English.</p> <p>My first attempt was to simply add a  and  flag followed by the localized version of my content on a single page. It was okay, not great but okay enough so I could live with it at start.</p> <p>It did not last long since my friend @Lujeni immediately told me that it would be better without all these flags flying around a same page...</p> <p>So I ended up writing a mkdocs plugin to support pages localization easily!</p> <p>This work took me a bit (too much?) further and I am now also working on adding theme localization support to the whole mkdocs project!</p>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#github","title":"Github","text":"<p>Last but not least, I use Github workflow Actions + Pages to build and host my website now!</p>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#take-a-leap-go-static","title":"Take a leap, go static!","text":"<p>I hope this article inspires you to try and use these cool projects </p>","tags":["mkdocs","wordpress"]},{"location":"Tech%20Blog/2021/2021-05-06-on-scylla-manager-suspend-resume/","title":"On Scylla Manager Suspend &amp; Resume feature","text":"<p>Disclaimer</p> <p>This blog post is neither a rant nor intended to undermine the great work that Michal and his team are doing on scylla-manager.</p> <p>I felt the need to write about it because I took a public stance on this topic during Scylla Summit 2019.</p>","tags":["gentoo","scylla","manager"]},{"location":"Tech%20Blog/2021/2021-05-06-on-scylla-manager-suspend-resume/#scylla-manager-23-suspend-resume","title":"Scylla Manager 2.3 Suspend &amp; Resume","text":"<p>Scylla Manager 2.3 released a way to suspend maintenance operations such as backups and repairs and resume them using <code>sctool</code> commands or API calls. This allows Scylla users to make sure that they can preserve their precious CPU resources for their needs during high demand hours (usually during the day).</p> <p>It is interesting to note that the feature is actually not a schedule per se but rather offers the possibility to manually suspend and resume scheduled tasks in scylla-manager. You'll have a setup a contrab to use it!</p> <p>On paper this is great as it comes as a nice addition to the 2.1 and 2.2 releases featuring repair parallelism and intensity controls. There surely are plenty of users out there that were expecting this news so thank you for this but...</p> <p>Will I use it? yes. Am I happy that I have to? no.</p>","tags":["gentoo","scylla","manager"]},{"location":"Tech%20Blog/2021/2021-05-06-on-scylla-manager-suspend-resume/#i-made-fun-of-mongodb-for-doing-it","title":"I made fun of MongoDB for doing it","text":"<p>Taking a step back, scylla-manager just implemented what is called a balancing window schedule in MongoDB sharded clusters.</p> <p>Back in Scylla Summit 2019 I said on stage that this kind of feature felt like a patch in MongoDB because such kind of database should be smart enough to cope with the background operations that it generated itself.</p> <p> </p> <p>Since MongoDB is not built on an architectural design fit for scheduling and optimizing certain operations per CPU, the impact of its internal shard balancer is bad on performance and the balancing window strategy allows users to reduce their cluster suffering during the most busy part of the day.</p> <p>But Scylla was built on the architectural thread-per-core promise that its smarter per-core IO scheduler could handle OLTP and OLAP workloads at the same time. I was living by it and believed this could be applied to repairs as well.</p>","tags":["gentoo","scylla","manager"]},{"location":"Tech%20Blog/2021/2021-05-06-on-scylla-manager-suspend-resume/#now-scylla-is-doing-it","title":"Now Scylla is doing it","text":"<p>So today I'm unsure how to welcome this news.</p> <p></p> <p>On one hand there are valid use cases where you want to make sure that you get all the performance that you can from your hardware and I'm sure most users will gladly welcome this because they know it will help their cluster on a daily basis. (and I am one of them).</p> <p>On the other hand I can't get rid of the deceptive feeling that Scylla is still not smart enough to handle the impacts of background maintenance operations and deliver to be the promised boring database.</p> <p>For some time I even hoped that workload prioritization could be used better to diminish repair impacts on performance but it's never been released to us Open Source users anyway so I never had the chance to build a really opinion on that (and I like to remind that we are also paying customers at Numberly: we do not run enterprise because we choose not to).</p> <p>I also want to acknowledge that a lot of efforts have been and still are invested in optimizing Scylla maintenance operations: Asias He is notoriously smart and prolific on that topic and this is not an easy task indeed!</p>","tags":["gentoo","scylla","manager"]},{"location":"Tech%20Blog/2021/2021-05-06-on-scylla-manager-suspend-resume/#keep-faith-on-a-brighter-and-boring-future","title":"Keep faith on a brighter and boring future","text":"<p>In conclusion I have the feeling that these are just necessary steps on a path to greater victories.</p> <p>The new 4.4 IO scheduler and project Circe open up interesting perspectives thanks to more collaborative CPU queues, switching to tablet based data storage and the whole concept of repairs being voided.</p> <p>Let's hope these promises will materialize soon (pun intended) so we can actually ditch scylla-manager suspend &amp; resume crontabs (sorry Michal)!</p>","tags":["gentoo","scylla","manager"]},{"location":"Tech%20Blog/2021/2021-05-06-on-scylla-manager-suspend-resume/#my-scylla-manager-wish","title":"My scylla-manager wish","text":"<p>I would love to be able to prevent that a repair and backup task run at the same time so that they queue instead of overlapping.</p> <p>No? what if I say please? </p>","tags":["gentoo","scylla","manager"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/","title":"Create beautiful and localized documentations and websites using MkDocs + Github","text":"<p>This is the web version of the talk I gave at EuroPython 2021. If you prefer the video format, it is available here.</p> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#introductions","title":"Introductions","text":"","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#mkdocs","title":"MkDocs","text":"<p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation.</p> <p>Documentation source files are written in Markdown, and configured with a single YAML configuration file.</p> <p>MkDocs is written in Python, its source code is easy to understand and we are open to contributions!</p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#mkdocs-material","title":"mkdocs-material","text":"<p>MkDocs sites can be themed, and while it comes with 2 built in themes, their look and feel is a bit dated and their customization is limited.</p> <p>Here comes mkdocs-material, a Material Design theme for MkDocs that allows you to create a branded static site from a set of Markdown files to host the documentation of your Open Source or commercial project.</p> <p>The mkdocs-material theme is by far the most popular theme of MkDocs as it is based on applying the Material Design principles to MkDocs generated sites.</p> <p>It is customizable, searchable, mobile-friendly and supports 40+ languages.</p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#i18n-or-l10n","title":"i18n or l10n?!","text":"<p>Warning</p> <p>Defining i18n vs l10n is controversial so this is the rationale we adopted at MkDocs.</p> <p>A benevolent debate on i18n vs l10n took place on mkdocs #774 where we settled that internationalization (i18n) will refer to the MkDocs core feature used to allow users to localize (l10n) their documentation.</p> <p>Therefore MkDocs use the i18n term to refer to the fact that it supports theme text and dialogs localization (just like Jinja).</p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#mkdocs-static-i18n","title":"mkdocs-static-i18n","text":"<p>The mkdocs-static-i18n plugin allows you to support multiple languages of your documentation by adding localized versions of your files to your existing documentation pages.</p> <p>It also allows you to build and serve localized versions of any file extension and automatically display localized images, medias and assets from your Markdown sources.</p> <p>Check who's already using mkdocs-static-i18n here!</p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#github-pages","title":"Github Pages","text":"<p>Warning</p> <p>GitHub Pages is not intended for or allowed to be used as a free web hosting service to run your online business, e-commerce site, or any other website that is primarily directed at either facilitating commercial transactions or providing commercial software as a service (SaaS).</p> <p>Websites for you and your projects hosted directly from your GitHub repository. Just edit, push, and your changes are live.</p> <p>You get one site per GitHub account and organization, and unlimited project sites.</p> <p>Free HTTPS with support for custom domain names.</p> <p>Soft limitations:</p> <ul> <li>1GB website</li> <li>100GB traffic bandwidth per month</li> <li>10 builds (deploy) per hour</li> </ul>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#creating-a-multi-language-mkdocs-site","title":"Creating a multi-language MkDocs site","text":"","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#what-we-need","title":"What we need","text":"<ul> <li>A fork of https://github.com/ultrabug/ep2021</li> <li>A Python3 virtual environment:</li> </ul>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#requirementstxt","title":"requirements.txt","text":"<p>Let's see what's inside our <code>requirements.txt</code> file and why.</p> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#mkdocs-122","title":"mkdocs &gt;= 1.2.2","text":"<p>We do not need <code>mkdocs[i18n]</code> which adds support for built-in themes localization since mkdocs-material supports localization on its own.</p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#mkdocs-material-7111","title":"mkdocs-material &gt;= 7.1.11","text":"<p>We want a modern looking, responsive and highly customizable theme with built-in support for a language switcher.</p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#mkdocs-static-i18n-018","title":"mkdocs-static-i18n &gt;= 0.18","text":"<p>Allows to localize our content pages by creating a <code>.&lt;language&gt;</code> prefixed version of any file to localize it automatically.</p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#localized-mkdocs-to-github-workflow","title":"Localized MkDocs to Github workflow","text":"<ol> <li> <p>Initialize our project</p> <pre><code>mkdocs new .\n</code></pre> </li> <li> <p>Build and serve locally, open in browser</p> <pre><code>mkdocs serve\n</code></pre> </li> <li> <p>Modify content, add assets and localize (translate) our website</p> </li> <li>Local website is auto-refreshed on browser, we preview every modification we make live</li> <li>When done, commit and push changes</li> <li> <p>Deploy to Github Pages</p> <pre><code>mkdocs gh-deploy\n</code></pre> </li> <li> <p>It's online!</p> </li> </ol>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#initialize-our-mkdocs-project","title":"Initialize our MkDocs project","text":"<pre><code>mkdocs new .\n</code></pre> <p>Will create:</p> <ul> <li>an initial <code>mkdocs.yml</code> configuration file</li> <li>a <code>docs/index.md</code> documentation home page</li> </ul> <p></p> <p>Tip</p> <p>If you cloned my repository or use your own fork, you can get to this step by checking out the <code>new</code> branch. <pre><code>git checkout new\n</code></pre></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#preview-our-website-locally","title":"Preview our website locally","text":"<pre><code>mkdocs serve\n</code></pre> <p>The above command will build our website and run a local web server so we can preview it directly on our browser.</p> <p>Open this URL on your browser: http://127.0.0.1:8000/</p> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#changes-trigger-a-refresh","title":"Changes trigger a refresh","text":"<pre><code>mkdocs serve\n</code></pre> <p>Any change we make will trigger a rebuild and refresh our page in our browser!</p> <p></p> <p>Tip</p> <p>If you cloned my repository or use your own fork, you can get to this step by checking out the <code>change</code> branch. <pre><code>git checkout change\n</code></pre></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#switch-to-mkdocs-material","title":"Switch to mkdocs-material","text":"<pre><code>theme:\nname: material\n</code></pre> <p>Instantaneous switch from the default <code>mkdocs</code> theme to the beautiful and feature rich <code>material</code> theme.</p> <p></p> <p>Tip</p> <p>If you cloned my repository or use your own fork, you can get to this step by checking out the <code>material</code> branch. <pre><code>git checkout material\n</code></pre></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#switch-to-europython-colors","title":"Switch to EuroPython colors","text":"<pre><code>theme:\nname: material\npalette:\nprimary: green\n</code></pre> <p>The material theme is highly customizable, let's use EuroPython's green color palette.</p> <p></p> <p>Tip</p> <p>If you cloned my repository or use your own fork, you can get to this step by checking out the <code>palette</code> branch. <pre><code>git checkout palette\n</code></pre></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#add-imagesassets","title":"Add images/assets","text":"<p>The <code>docs/</code> folder is where we organize our content pages and assets. Let's modify the home page and add the EuroPython logo.</p> <p></p> <p>Tip</p> <p>If you cloned my repository or use your own fork, you can get to this step by checking out the <code>logo</code> branch. <pre><code>git checkout logo\n</code></pre></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#mkdocs-static-i18n_1","title":"mkdocs-static-i18n","text":"<p>Will create <code>/&lt;language&gt;/</code> versions of our website!</p> <pre><code>plugins:\n- search\n- i18n:\ndefault_language: en\nlanguages:\nen: English\nfr: Fran\u00e7ais\n</code></pre> <p>Will generate those URLs:</p> <ol> <li> <p>https://ultrabug.github.io/ep2021/</p> <pre><code>default URL is the English version\n</code></pre> </li> <li> <p>https://ultrabug.github.io/ep2021/en/</p> <pre><code>/en/ URL is also the English version, this is the same site as the / URL\n</code></pre> </li> <li> <p>https://ultrabug.github.io/ep2021/fr/</p> <pre><code>/fr/ URL is the French version of our site\n</code></pre> </li> </ol> <p>The plugin automatically configures:</p> <ul> <li>search plugin language and localized content indexation</li> <li>material theme language</li> <li>material theme language switcher in the header</li> </ul>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#localizing-our-content","title":"Localizing our content","text":"<p>Let's translate and localize our website!</p> <p>We localize the versions of our pages and assets by suffixing them with `..`` <p></p> <p>Tip</p> <p>If you cloned my repository or use your own fork, you can get to this step by checking out the <code>translation</code> branch. <pre><code>git checkout translation\n</code></pre></p> <p>Let's see the content of the translated files:</p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#indexenmd","title":"index.en.md","text":"","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#indexfrmd","title":"index.fr.md","text":"","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#mkdocs-static-i18n-demo","title":"mkdocs-static-i18n demo","text":"","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#automatic-markdown-localization","title":"Automatic Markdown localization","text":"<p>Focus on translating, not referencing localized versions of your pages and assets!</p> <p>Both index.en.md and index.fr.md refer to <code>ep2021-logo.png</code> in their Markdown source:</p> <pre><code>![europython 2021 logo](assets/ep2021-logo.png)\n</code></pre> <p></p> <p>Tip</p> <p>If you cloned my repository or use your own fork, you can get to this step by checking out the <code>localized</code> branch. <pre><code>git checkout localized\n</code></pre></p> <p>This means that the Markdown reference to the image will be localized automatically by the plugin!</p> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#push-our-work-to-github","title":"Push our work to Github","text":"<pre><code>git add\ngit commit\ngit push\n</code></pre> <p>We're done, let's commit and push our content to our Github repository.</p> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#build-and-deploy-to-github","title":"Build and deploy to Github","text":"<pre><code>mkdocs gh-deploy\n</code></pre> <p>The above command will:</p> <ol> <li>Build our multi-language website</li> <li>Copy <code>site/</code> dir to the <code>gh-pages</code> branch</li> <li>Push the <code>gh-pages</code> branch to Github</li> </ol> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#github-pages-is-automatically-configured","title":"Github Pages is automatically configured","text":"<p>No configuration needed on your Github repository, it's done automatically.</p> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#our-multi-language-mkdocs-website-is-online","title":"Our multi-language MkDocs website is online!","text":"<p>Check out your own URL or the https://ultrabug.github.io/ep2021/ website of this repository.</p> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#automatic-mkdocs-gh-deploy","title":"Automatic mkdocs gh-deploy","text":"<p>Let\u2019s use Github Actions to run mkdocs <code>gh-deploy</code> for us automatically when we push new commits!</p> <p>Create the file <code>.github/workflows/gh-deploy.yml</code>:</p> <pre><code>name: gh-deploy\n\non:\npush:\nbranches:\n- main\n\njobs:\nbuild:\nname: MkDocs Github Pages automatic deployment\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout main\nuses: actions/checkout@v2\n\n- name: Set up Python 3.9\nuses: actions/setup-python@v2\nwith:\npython-version: '3.9'\n\n- name: Install requirements\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n\n- name: MkDocs gh-deploy\nrun: |\ngit pull\nmkdocs gh-deploy\n</code></pre> <p>Now every time we push new commits to the <code>main</code> branch, our website will be automatically deployed and refreshed on Github Pages!</p> <p>Tip</p> <p>If you cloned my repository or use your own fork, you can get to this step by checking out the <code>actions</code> branch. <pre><code>git checkout actions\n</code></pre></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#resources","title":"Resources","text":"","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#some-useful-and-awesome-mkdocs-plugins","title":"Some useful and awesome MkDocs plugins","text":"<ul> <li>MkDocs Wiki's Plugins list (community based)</li> <li>Plugin to handle URL redirections</li> <li>Plugin to handle navigation pages ordering in a very flexible way</li> <li>Plugin to hook your own functions without having to create a MkDocs plugin</li> </ul>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#some-useful-and-awesome-markdown-extensions","title":"Some useful and awesome Markdown extensions","text":"<p>Check out the sample <code>mkdocs.yml</code> available on the <code>extensions</code> branch!</p> <p></p> <p></p>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/#special-thanks","title":"Special thanks","text":"<ul> <li>mkdocs: <code>@waylan</code> <code>@oprypin</code></li> <li>mkdocs-material: <code>@squidfunk</code></li> <li>mkdocs-static-i18n: <code>@Caetan95</code> <code>@Stanzilla</code> <code>@adamkusmirek</code></li> <li>pymdown-extensions: <code>@facelessuser</code></li> </ul>","tags":["mkdocs","mkdocs-static-i18n","localization","internationalization","europython"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/","title":"Learning Rust the hard way for a production Kafka+ScyllaDB pipeline","text":"<p>This is the web version of the talk I gave at Scylla Summit 2022.</p> <p></p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#project-context-at-numberly","title":"Project context at Numberly","text":"<p>You may remember that Numberly is a digital data marketing expert helping brands connect with their customers using all digital channels available.</p> <p>As a data company, we operate on a lot of data that is fast moving. This event driven approach drives our technological choices towards platforms that allow us to process and react to stimulus as close to real time as possible.</p> <p>As such we combine Kafka and Scylla superpowers extensively thanks to streams and specialized pipeline applications that we'll call data processors here.</p> <p></p> <p>Each of those pipeline data processor applications prepares and enriches the incoming data so that it is useful to the downstream business / partner or client applications.</p> <p>Relevance of a data driven decision is at its best when it's close to the event's time of occurrence which means that availability and latency are business critical to us.</p> <p>Latency and resilience are the pillars upon which we have to build our platforms to make our business reliable in the face of our clients and partners.</p> <p>Those data processor apps, kafka, and of course Scylla can't fail, if they do we get angry partners and clients... And nobody wants to deal with an angry client!</p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#the-rusted-opportunity","title":"The (rusted) opportunity","text":"<p>The data industry and ecosystems are always changing. Last fall, we had to adapt three of the most demanding data processors written in Python.</p> <p>Those processor applications were doing the job for more than 5 years, they were battle tested and trustworthy.</p> <p>But I was following Rust maturation for a while, I was curious and it always felt less intimidating than C or C++ to me. So when this opportunity came, I went to my colleagues and told them:</p> <p>Question</p> <p>Hey why not rewrite those 3 python applications that we know work very well with one rust application which we don't even know the language?</p> <p>Ok, I must admit that I lost my CTO badge for a few seconds when I saw their faces...</p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#the-never-tried-before-rust-promises","title":"The (never tried before) Rust promises","text":"<p>Rust makes promises that some people seem to agree with, it's intriguing!</p> <p>It is supposed to...</p> <ul> <li>Be secure<ul> <li>Memory and thread safety as first class citizens</li> <li>No runtime or garbage collector</li> </ul> </li> <li>Be easy to deploy<ul> <li>Compiled binaries are self-sufficient</li> </ul> </li> <li>Make no compromises<ul> <li>Strongly and statically typed</li> <li>Exhaustivity is mandatory</li> <li>Built-in error management syntax and primitives</li> </ul> </li> <li>Play well with Python<ul> <li>PyO3 can be used to run Rust from Python (or the contrary)</li> </ul> </li> </ul> <p>But furthermore, their marketing motto speaks to the marketer inside me:</p> <p>Quote</p> <p>A language empowering everyone to build reliable and efficient software.</p> <p>That being said, careful readers will notice that I did not mention speed in the Rust promises.</p> <p>Isn't Rust supposed to be OMG fast? No, they talk about efficiency and it's not the same!</p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#efficient-software-faster-software","title":"Efficient software != Faster software","text":"<p>Efficient software does not always mean faster software.</p> <p>Brett Cannon, a Python core developer, advocates that selecting a programming language for being faster on paper is a form or premature optimization.</p> <p></p> <p>I agree with him in the sense that the word Fast have different meanings depending on your objectives.</p> <p>To me, Rust can be said to be faster as a consequence of being Efficient, which does not cover all the items on the list here.</p> <p>Let's apply them to my context:</p> <ul> <li>Fast to develop?<ul> <li>No. Python is way faster, did that for 15 years</li> </ul> </li> <li>Fast to maintain?<ul> <li>No. Nobody at Numberly does Rust yet</li> </ul> </li> <li>Fast to prototype?<ul> <li>No. Code must be complete to compile and run</li> </ul> </li> <li>Fast to process data?<ul> <li>Sure. To prove it, measure it first</li> </ul> </li> <li>Fast to cover all failure cases?<ul> <li>Definitely. Mandatory exhaustivity + error handling primitives</li> </ul> </li> </ul> <p>As we can see in my case, choosing Rust over Python will mean that I will definitely lose time!</p> <p>Quote</p> <p>I did not choose Rust to be \u201cfaster\u201d. Our Python code was fast enough to deliver their pipeline processing.</p> <p>So why would I want to lose time? The short answer is Innovation!</p> <p></p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#the-reliable-software-paradigms","title":"The reliable software paradigms","text":"<p>Now what will I gain from losing time other than the pain of using semicolons and brackets?</p> <p>Supposedly a more reliable software thanks to Rust unique design and paradigms.</p> <p>In other words, what makes me slow is also an opportunity to make my software stronger!</p> <ul> <li>Low level paradigms (ownership, borrowing, lifetimes)<ul> <li>If it compiles, it's safe</li> </ul> </li> <li>Strong type safety<ul> <li>Predictable, readable, maintainable</li> </ul> </li> <li>Compilation (debug, release)<ul> <li>Rust compiler is very helpful vs a random Python exception</li> </ul> </li> <li>Dependency management<ul> <li>Finally something looking sane vs Python mess</li> </ul> </li> <li>Exhaustive pattern matching<ul> <li>Confidence that you're not forgetting something</li> </ul> </li> <li>Error management primitives (Result)<ul> <li>Handle failure right from the language syntax</li> </ul> </li> </ul> <p>Quote</p> <p>I chose Rust because it provided me with the programming paradigms at the right abstraction level that I needed to finally understand and better explain the reliability and performance of an application.</p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#learning-rust-the-hard-way","title":"Learning Rust the hard way","text":"<p>Here is an overview of all the aspects and all the technological stacks that I had to deal with. I'll detail the most insightful parts below.</p> <p></p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#confluent-kafka-schema-registry","title":"Confluent Kafka Schema Registry","text":"<p>We use Confluent Kafka Community edition with a Schema Registry to handle our Avro encoded messages.</p> <p>Confluent Schema Registry adds a magic byte to kafka message payloads, breaking vanilla Apache Avro schema deserialization.</p> <p>Luckily for me, Gerard Klij had done the heavy lifting in his crate which helped me a lot before I discovered performance problems. We are working on improving that and I hope to switch back to his helpful project once we're done.</p> <p>Until then I decided to use the manual approach shown here and decode Avro messages myself with respect of their schema:</p> <pre><code>/// Deserialize the given kafka raw `message` using the provided\n/// Avro `schema` and return a Navigation struct message to be\n/// used by the processors.\npub fn get_decoded_message(schema: &amp;Schema, message: &amp;BorrowedMessage) -&gt; Result&lt;Navigation&gt; {\n/*\n    DO NOT pass the reader schema as last argument to from_avro_datum\n    this implies a resolve() on each value and impacts performances badly\n    &gt; Ok(from_avro_datum(&amp;schema, &amp;mut reader, Some(&amp;schema)).unwrap()) // decode + resolve\n    */\nlet mut reader = Cursor::new(&amp;message.payload().unwrap()[5..]);\nlet val = match from_avro_datum(&amp;schema, &amp;mut reader, None) {\nOk(inner) =&gt; inner,\nErr(err) =&gt; {\nreturn Err(anyhow!(err));\n}\n};\nlet navigation: Navigation = match &amp;val {\nValue::Record(_) =&gt; from_value::&lt;Navigation&gt;(&amp;val).unwrap(),\n_ =&gt; {\nreturn Err(anyhow!(\"could not map avro data to struct\"));\n}\n};\nOk(navigation)\n}\n</code></pre>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#apache-avro-rust-was-broken","title":"Apache Avro Rust was broken","text":"<p>Then I hit the second wall when even if my reading of the Avro payload was done right I could not deserialize them...</p> <p>As a total Rustian newbie, I blamed myself for days before even daring to open Apache Avro source code and look at it. I eventually found out that Apache Avro was broken for complex schemas like us!</p> <p>It made me wonder if anyone in the whole world was actually using Avro with Rust in production...even more knowing that the project had been given to the Apache foundation without a committer able to merge PRs at the time.</p> <p>So here I am contributing fixes to Apache Avro Rust which eventually got merged three months later in Jan. 2022: AVRO-3232 and AVRO-3240.</p> <p>Anyway, another unexpected fact that Rust allowed me to prove is that deserializing Avro is faster than deserializing JSON in our case of rich and complex data structures. My colleague Othmane was sure of it, I could finally prove it!</p> <p>Once Avro was fixed, I could start experimenting with the <code>--release</code> flag and run my code optimized which I was not expecting such a speed boost from!</p> <p></p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#asynchronous-patterns-to-optimize-latency","title":"Asynchronous patterns to optimize latency","text":"<p>Once I was able to consume messages from Kafka, I started looking at the best pattern to process them.</p> <p>I found the tokio asynchronous runtime very intuitive coming from Python asyncio.</p> <p>I played a lot with various code patterns to optimize and make consuming messages from Kafka latency stable and reliable.</p> <p>One of the interesting findings was to not defer the decoding of Avro messages to a green-thread but do it right in the consumer loop. Deserialization is a CPU bound operation which will benefit from not being cooperative with other green-thread tasks!</p> <p>Similar, allowing and controlling your parallelism will help stabilize your I/O bound operations, let's see a real example of that:</p> <p></p> <p>Deferring the rest of my processing logic which is I/O bound to green-threads helped absorb tail latencies without affecting my kafka consuming speed.</p> <p>The Grafana dashboard above shows that around 9:00 something made Scylla slower than usual, scylla select and insert P95 latencies went up by 16.</p> <p>That's where parallelism load (the little bump on the first graph) also started to increase as I had more active green-threads processing messages.</p> <p>But it only hit my kafka consuming latency by a factor of 2 at P95, effectively absorbing tail latencies due to this ephemeral overload!</p> <p>This is the typical example of something that was harder to pinpoint and demonstrate in Python but became clear with Rust.</p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#scylla-rust-shard-aware-driver","title":"Scylla Rust (shard-aware) driver","text":"<p>I found the Scylla Rust driver to be intuitive and well featured, congratulations to the team which is also very helpful on their dedicated channel on the Scylla Slack service, join us there!</p> <p>The new <code>CachingSession</code> is very handy to cache your prepared statements so you don't have to do it yourself like I did at first...</p> <p>Warning</p> <p>Prepared queries are NOT paged by default, use paged queries with <code>execute_iter()</code> instead!</p> <p>I'm showcasing a code example of a production connection function to scylla, using SSL, multi-datacenter awareness and a caching session:</p> <pre><code>/// Connect to Scylla and return a `CachingSession` that will\n/// be used by the processors.\npub async fn get_scylla_session(\nscylla_nodes: Vec&lt;&amp;str&gt;,\nscylla_user: &amp;str,\nscylla_password: &amp;str,\nssl_ca_path: &amp;str,\n) -&gt; Result&lt;CachingSession&gt; {\nlet mut context_builder = SslContextBuilder::new(SslMethod::tls())?;\nlet ca_dir = fs::canonicalize(PathBuf::from(ssl_ca_path))?;\ncontext_builder.set_ca_file(ca_dir.as_path())?;\ncontext_builder.set_verify(SslVerifyMode::PEER);\n// load balancing policy\nlet local_dc = &amp;*DATACENTER;\nlet dc_robin = Box::new(DcAwareRoundRobinPolicy::new(local_dc.to_string()));\nlet policy = Arc::new(TokenAwarePolicy::new(dc_robin));\n// the size of the prepared statement cache has no impact on the underlying\n// DashMap memory consumption as it is only used by the CachingSession\n// struct logic itself.\ninfo!(\n\"scylla connecting to {:?} as user {} on datacenter {}\",\n&amp;scylla_nodes, scylla_user, local_dc\n);\nlet caching_session = CachingSession::from(\nSessionBuilder::new()\n.known_nodes(&amp;scylla_nodes)\n.connection_timeout(Duration::from_secs(15))\n.ssl_context(Some(context_builder.build()))\n.user(scylla_user, scylla_password)\n.load_balancing(policy)\n.build()\n.await?,\n1000,\n);\nOk(caching_session)\n}\n</code></pre>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#exporting-metrics-properly-for-prometheus","title":"Exporting metrics properly for Prometheus","text":"<p>Now to Prometheus which even if it comes late on this presentation is actually one of the first things I did setup on my application.</p> <p>For all the experiments I did, I measured their latency and throughput impacts thanks to Prometheus.</p> <p>For a test to be meaningful, those measurements must be made right and then graphed right. Scylla people know this by heart but it's usually harder for mortals like the rest of us...</p> <p>So here is an example of how I measure scylla query insertion latency.</p> <p>The first and important gotcha is to setup your histogram bucket correctly with your expected graphing finesse:</p> <pre><code>pub static ref SCYLLA_INSERT_QUERIES_LATENCY_HIST_SEC: Histogram = register_histogram!(\n\"scylla_insert_queries_latency_seconds\",\n\"Scylla INSERT query latency histogram in seconds\",\nvec![0.0005, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 5.0, 15.0],\n)\n.expect(\"failed to create prometheus metric\");\n</code></pre> <p>Here I expect scylla latency to vary between 50\u00b5s and 15s which is the maximal server timeout I'm allowing for writes.</p> <p>Then I use it like this: I start a timer on the histogram and record its duration on success with <code>timer.observe_duration();</code> and drop it on failure with <code>drop(timer);</code> so that my metrics are not polluted by possible errors.</p> <pre><code>let timer = SCYLLA_INSERT_QUERIES_LATENCY_HIST_SEC.start_timer();\nmatch scylla_session\n.execute(scylla_statement, &amp;scylla_parameters)\n.await\n{\nOk(_) =&gt; {\ntimer.observe_duration();\nOk(())\n}\nErr(err) =&gt; {\ndrop(timer);\nPROCESSING_ERRORS_TOTAL\n.with_label_values(&amp;[\"scylla_insert\"])\n.inc();\nerror!(\"insert_in_scylla: {:?}\", err);\nErr(anyhow!(err))\n}\n}\n</code></pre>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#grafana-dashboarding","title":"Grafana dashboarding","text":"<p>One of my best time investment in this project was to create a detailed and meaningful Grafana dashboard so I could see and compare the results of my Rust application experimentations.</p> <p></p> <p>Make sure you graph as much things as possible!</p> <ul> <li>cache sizes (statements)</li> <li>query and throughput rates and occurrence of everything, discerning the difference between the two</li> <li>make errors metrics meaningful by using labels (by type)</li> <li>kubernetes pod memory</li> </ul> <p>Also read the great article that the folks at Grafana wrote on how to visualize prometheus histograms right in Grafana, it's not as obvious as one might think!</p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#did-i-really-lose-time-because-of-rust","title":"Did I really lose time because of Rust","text":"<p>The real question is: do I have the feeling to have lost time at all? Short answer: hell no!</p> <p>The syntax was surprisingly simple and intuitive to adopt even coming from Python.</p> <p>In the end I have to confess that Rust made me want to test and analyze everything at a lower level and I absolutely failed to resist the temptation!</p> <p>So most of my time was spent on testing, graphing, analyzing and trying to come up with a decent and insightful explanation: this surely does not look like wasted time to me!</p> <p>For the number hungry of you in the audience, here are some numbers taken from the application:</p> <ul> <li>Kafka consumer max throughput with processing? 200K msg/s on 20 partitions</li> <li>Avro deserialization P50 latency? 75\u00b5s</li> <li>Scylla SELECT P50 latency on 1.5B+ rows tables? 250\u00b5s</li> <li>Scylla INSERT P50 latency on 1.5B+ rows tables? 2ms</li> </ul>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-02-21-learning-rust-the-hard-way/#conclusion-it-went-better-than-expected","title":"Conclusion: it went better than expected","text":"<p>It went way better than expected!</p> <ul> <li>Rust crates ecosystem is mature, similar to Python Package Index</li> <li>The scylla-rust-driver is stable and efficient!</li> <li>It took me a while to accept that Apache Avro was broken, not me...</li> <li>3 Python apps totalling 54 pods replaced by 1 Rust app totalling 20 pods</li> <li>This feels like the most reliable and efficient software I ever wrote!</li> </ul> <p>Even if it was my first Rust application, I felt confident during the development process which transformed into confidence in a predictable and resilient software.</p> <p>After weeks of production, the new Rust pipeline processor proves to be very stable and resilient.</p> <p>So now I can fairly say that yes, Rust promises are living up to expectations!</p>","tags":["rust","python","kafka","scylladb","scylla summit","avro"]},{"location":"Tech%20Blog/2022/2022-07-09-yubikey-full-reset/","title":"How to reset your Yubikey when you locked your PIN","text":"<p>This article is a short reminder of the procedure to follow to reset a Yubikey to its factory defaults and thus reset the user and admin PIN when you locked them after too many unsuccessful tries using GPG/SSH.</p> <pre><code>gpg --card-status\nApplication ID ...: D2760001240102010006090289860000\nVersion ..........: 2.1\nManufacturer .....: Yubico\nSerial number ....: 09992652\nName of cardholder: [not set]\nLanguage prefs ...: [not set]\nSex ..............: unspecified\nURL of public key : [not set]\nLogin data .......: [not set]\nSignature PIN ....: not forced\nKey attributes ...: 2048R 2048R 2048R\nMax. PIN lengths .: 127 127 127\nPIN retry counter : 0 0 3\nSignature counter : 0\nSignature key ....: [none]\nEncryption key....: [none]\nAuthentication key: [none]\nGeneral key info..: [none]\n</code></pre> <p>This procedure should be used only if you don't have a PUK code to use, which is the simple and non-destructive way to recover from a locked PIN situation.</p>","tags":["yubikey","security","gpg","ssh"]},{"location":"Tech%20Blog/2022/2022-07-09-yubikey-full-reset/#using-a-puk-code-non-destructive","title":"Using a PUK code (non-destructive)","text":"<p>Using a PUK code, you can use the <code>passwd</code> command in gpg to clear the PIN lock counters.</p> <pre><code>$ gpg --card-edit --expert\n\ngpg/card&gt; help\nquit           quit this menu\nadmin          show admin commands\nhelp           show this help\nlist           list all available data\nfetch          fetch the key specified in the card URL\npasswd         menu to change or unblock the PIN\nverify         verify the PIN and list all data\nunblock        unblock the PIN using a Reset Code\n\ngpg/card&gt; passwd\ngpg: OpenPGP card no. XXXXXXXXXXXXXXXXXX detected\n\n1 - change PIN\n2 - unblock PIN\n3 - change Admin PIN\n4 - set the Reset Code\nQ - quit\n\nYour selection? 2\n</code></pre>","tags":["yubikey","security","gpg","ssh"]},{"location":"Tech%20Blog/2022/2022-07-09-yubikey-full-reset/#reset-your-yubikey-to-factory-defaults-destructive","title":"Reset your Yubikey to factory defaults (destructive)","text":"<p>So you locked your GPG (user/admin) PIN counters?</p> <pre><code>PIN retry counter : 0 0 3\n                    ^   ^\n     user PIN counter   admin PIN counter\n</code></pre> <p>Warning</p> <p>Reseting your Yubikey to its factory defaults means that you will loose everything stored on it, so you'll have to setup your GPG/SSH again!</p> <p>The procedure is about sending hexadecimal commands through the gpg-agent:</p> <pre><code>$ gpg-connect-agent --hex\n\n&gt; scd apdu 00 20 00 81 08 40 40 40 40 40 40 40 40\nD[0000] 69 82 i.\nOK\n&gt; scd apdu 00 20 00 81 08 40 40 40 40 40 40 40 40\nD[0000] 69 82 i.\nOK\n&gt; scd apdu 00 20 00 81 08 40 40 40 40 40 40 40 40\nD[0000] 69 82 i.\nOK\n&gt; scd apdu 00 20 00 81 08 40 40 40 40 40 40 40 40\nD[0000] 69 83 i.\nOK\n&gt; scd apdu 00 20 00 83 08 40 40 40 40 40 40 40 40\nD[0000] 69 82 i.\nOK\n&gt; scd apdu 00 20 00 83 08 40 40 40 40 40 40 40 40\nD[0000] 69 82 i.\nOK\n&gt; scd apdu 00 20 00 83 08 40 40 40 40 40 40 40 40\nD[0000] 69 82 i.\nOK\n&gt; scd apdu 00 20 00 83 08 40 40 40 40 40 40 40 40\nD[0000] 69 83 i.\nOK\n&gt; scd apdu 00 e6 00 00\nD[0000] 90 00 ..\nOK\n&gt; scd apdu 00 44 00 00\nD[0000] 90 00 ..\nOK\n&gt;\n</code></pre> <p>Then unplug/replug your Yubikey + restart your gpg-agent daemon:</p> <pre><code>$ pkill gpg-agent &amp;&amp; pkill scdaemon\n$ gpg-agent --card-status\n\n[...]\nPIN retry counter : 3 0 3\n[...]\n</code></pre>","tags":["yubikey","security","gpg","ssh"]},{"location":"Tech%20Blog/2023/2023-02-15-building-a-shard-aware-application/","title":"Building a 100% ScyllaDB Shard-Aware Application Using Rust","text":"<p>I wrote a web transcript of the talk I gave with my colleagues Joseph and Yassir at Scylla Summit 2023.</p> <p></p> <p>You can find it here on the numberly.tech blog: Building a 100% ScyllaDB Shard-Aware Application Using Rust.</p>","tags":["rust","python","kafka","scylladb","scylla summit"]},{"location":"Tech%20Talks/","title":"Community talks and awards","text":"<p> I've been fortunate since some of my talk proposals have been accepted over the last years at tech conferences where I got to participate as a speaker.</p> <p>I've also been very humbled to be interviewed and awarded for my community work and professional experience.</p>"},{"location":"Tech%20Talks/2013/","title":"EuroPython 2013","text":"<p>I gave an informal but still recorded talk about uWSGI + gevent to build a fault tolerant application (my first one ever).</p> <ul> <li>Watch\u00a0the video</li> </ul>"},{"location":"Tech%20Talks/2014/","title":"Paris.py 5","text":"<p>A talk about fault tolerance in application design (in French).</p> <ul> <li>Watch\u00a0the video</li> </ul>"},{"location":"Tech%20Talks/2015/","title":"EuroPython 2015","text":""},{"location":"Tech%20Talks/2015/#designing-a-scalable-and-distributed-application","title":"Designing a scalable and distributed application","text":"<ul> <li>Download the slides</li> <li>Watch\u00a0the video</li> </ul> <p>One of the key aspect to keep in mind when developing a scalable application is its faculty to grow easily. But while we're used to take advantage of scalable backend technologies such as mongodb or couchbase, scaling automatically our own application core is usually another story.</p> <p>In this talk I will explain and showcase a distributed web application design based on consul and uWSGI and its consul plugin. This design will cover the key components of a distributed and scalable application:</p> <ul> <li>Automatic service registration and discovery will allow your application to grow itself automatically.</li> <li>Health checking and service unregistration will allow your application to be fault tolerant, highly available and to shrink itself automatically.</li> <li>A distributed Key/Value storage will allow you to (re)configure your distributed application nodes at once.</li> <li>Multi-Datacenter awareness will allow your application to scale around the world easily.</li> </ul>"},{"location":"Tech%20Talks/2016/","title":"EuroPython 2016","text":""},{"location":"Tech%20Talks/2016/#using-service-discovery-to-build-dynamic-python-applications","title":"Using Service Discovery to build dynamic python applications","text":"<ul> <li>Download the slides</li> <li>Watch the video</li> </ul> <p>This talk will showcase and compare three Service Discovery technologies and their usage to build a dynamic and distributed python application:</p> <ul> <li>consul</li> <li>etcd</li> <li>zookeeper</li> </ul> <p>After a short introduction to service discovery, we will iterate and compare how we can address the concrete and somewhat complex design of our python application using each technology.</p> <p>We'll then be able to discuss their strengths, weaknesses and python bindings and finally showcase the application in a demo.</p> <p>All the source code will of course be made available for the audience to benefit and start from for their own use !</p>"},{"location":"Tech%20Talks/2016/#planning-for-the-worst-interactive-talk-with-guillaume-gelin","title":"Planning for the worst (interactive talk, with Guillaume Gelin)","text":"<ul> <li>Download the slides</li> <li>Watch the video</li> </ul> <p>This talk is about sharing our experience about how we handled production problems on all levels of our applications.</p> <p>We'll begin with common problems, errors and failures and dig on to more obscure ones while sharing concrete tips, good practices and code to address them !</p> <p>This talk will make you feel the warmth of not being alone facing a problem :)</p>"},{"location":"Tech%20Talks/2017/europython2017/","title":"EuroPython 2017","text":""},{"location":"Tech%20Talks/2017/europython2017/#leveraging-consistent-hashing-in-your-python-applications","title":"Leveraging consistent hashing in your python applications","text":"<ul> <li>Download the slides</li> <li>Watch the video</li> </ul> <p>While consistent hashing is largely known and adopted in the NoSQL database clusters to solve data distribution and data access reliability, it is less known and used by the typical developers.</p> <p>This talk will introduce you to consistent hashing and the problems it solves while going through a practical use case in a python application.</p>"},{"location":"Tech%20Talks/2017/scaling-python/","title":"Scaling Python interview (book)","text":"<p>I've had the pleasure to be interviewed by Julien Danjou for his latest book called \"scaling python\".</p> <p>I answered the following questions:</p> <ul> <li>Could you introduce yourself and explain how you came to Python?</li> <li>What do you think make Python great (or not) when building distributed systems? What are the things you consider being advantages or drawbacks?</li> <li>What would be your the top N mistakes to avoid or advice/best practice     to follow when building scalable, distributed systems using Python?</li> </ul> <p></p>"},{"location":"Tech%20Talks/2018/europython-2018/","title":"EuroPython 2018","text":""},{"location":"Tech%20Talks/2018/europython-2018/#the-rise-of-python-in-the-data-communities","title":"The rise of Python in the Data communities","text":"<ul> <li>Download the slides</li> <li>Watch the video</li> </ul> <p>A retrospective and prospective of Python\u2019s adoption in the data-driven industries and how it has and should influence its ecosystem and communities.</p> <p>Thanks to its versatility, Python\u2019s usage and adoption has changed a lot over the last decade to go beyond the very act of software programming.</p> <p>From Developers to SysOps, closely followed by Scientists and Data analysts, Python has spread to become a common tongue for a wide range of people.</p> <p>We will start by looking at how this increased adoption impacted Python ecosystem and is still shaping it today. While this talk is not walk through all the Python technologies around data, some of them will be outlined so you will hear words like Numpy, Pandas or Jupyter.</p> <p>Then we will try to project ourselves in the future and by highlighting the pitfalls Python has to overcome to keep up with its pace and mature in its ability to scale!</p>"},{"location":"Tech%20Talks/2018/pyconfr-2018/","title":"PyConFR 2018","text":"<p>I had the chance to give three talks there, one of them brand new about my experience in evaluating then getting ScyllaDB in production.</p>"},{"location":"Tech%20Talks/2018/pyconfr-2018/#my-journey-into-joining-billions-of-rows-in-seconds-using-scylladb","title":"My journey into joining billions of rows in seconds using ScyllaDB","text":"<ul> <li>Download the slides</li> <li>Watch the video</li> </ul>"},{"location":"Tech%20Talks/2018/pyconfr-2018/#the-rise-of-python-in-the-data-communities","title":"The rise of Python in the Data communities","text":"<ul> <li>Download the slides</li> <li>Watch the video</li> </ul>"},{"location":"Tech%20Talks/2018/pyconfr-2018/#leveraging-consistent-hashing-in-your-python-applications","title":"Leveraging consistent hashing in your python applications","text":"<p>I updated it from my 2017 talk with a digression on Python 3.6 new dict implementation.</p> <ul> <li>Download the slides</li> <li>Watch the video</li> </ul> <p></p>"},{"location":"Tech%20Talks/2018/scylla-summit-2018/","title":"Scylla Summit 2018","text":""},{"location":"Tech%20Talks/2018/scylla-summit-2018/#scylla-user-award-contribution-to-scylla-open-source","title":"Scylla User Award: Contribution to Scylla Open Source","text":"<p>I'm proud to have been chosen for the Contribution to Scylla Open Source award during the summit.</p> <p>I've been contributing to Scylla through my Gentoo Linux packaging as well as to the Python code base, writing quite a bunch of blog posts and helping around in the general Slack channel.</p>"},{"location":"Tech%20Talks/2018/scylla-summit-2018/#replacing-mongodb-and-hive-with-scylla","title":"Replacing MongoDB and Hive with Scylla","text":"<ul> <li>See the slides</li> <li>Watch the video</li> </ul>"},{"location":"Tech%20Talks/2019/europython-2019/","title":"EuroPython 2019","text":""},{"location":"Tech%20Talks/2019/europython-2019/#how-we-run-graphql-apis-in-production-on-our-kubernetes-cluster","title":"How we run GraphQL APIs in production on our Kubernetes cluster","text":"<ul> <li>See the slides</li> <li>Watch the video</li> </ul>"},{"location":"Tech%20Talks/2019/scylla-summit-2019/","title":"Scylla Summit 2019","text":""},{"location":"Tech%20Talks/2019/scylla-summit-2019/#scylla-user-award-most-innovative-use-of-scylla","title":"Scylla User Award : Most innovative use of Scylla","text":"<p>Quote</p> <p>AdTech pioneer Numberly has combined Scylla with Kafka Connect, Kafka Streams, Apache Spark and Python Faust, built on Gentoo Linux and deployed on bare-metal across multiple datacenters, all managed with Kubernetes. All of that resulted in reengineering a calculation process that used to take 72 hours but can now be delivered in just 10 seconds.</p>"},{"location":"Tech%20Talks/2019/scylla-summit-2019/#mongodb-vs-scylla-production-experience-from-both-dev-ops-standpoint","title":"MongoDB vs Scylla: Production Experience from Both Dev &amp; Ops Standpoint","text":"<ul> <li>See the slides</li> <li>Watch the video</li> </ul>"},{"location":"Tech%20Talks/2020/europython-2020/","title":"EuroPython 2020","text":""},{"location":"Tech%20Talks/2020/europython-2020/#a-deep-dive-and-comparison-of-python-drivers-for-cassandra-and-scylla","title":"A deep dive and comparison of Python drivers for Cassandra and Scylla","text":"<p>Learn about Cassandra &amp; Scylla architectural differences and how we modified the cassandra Python driver to create the scylla-driver that is capable of routing CQL queries down to nodes CPUs!</p> <ul> <li>See the slides</li> <li>Watch the video</li> </ul>"},{"location":"Tech%20Talks/2020/pycon-india-2020/","title":"PyCon India 2020","text":""},{"location":"Tech%20Talks/2020/pycon-india-2020/#a-deep-dive-and-comparison-of-python-drivers-for-cassandra-and-scylla","title":"A deep dive and comparison of Python drivers for Cassandra and Scylla","text":"<p>I gave an updated talk from the EuroPython 2020 one sharing the recent developments of the Scylla Python driver.</p> <p></p> <ul> <li>See the slides</li> <li>Watch the video</li> </ul>"},{"location":"Tech%20Talks/2020/scylla-university/","title":"Scylla University","text":"<p>My presentation of the Scylla Token Ring Architecture and introduction to the Scylla specific drivers are now a part of the Scylla University courses!</p> <p>ScyllaDB has also used my 2020 talks material to write two blog posts about how we wrote the Scylla Python driver.</p> <ul> <li>Making a Shard-Aware Python Driver for Scylla, Part 1</li> <li>Making a Shard-Aware Python Driver for Scylla, Part 2</li> </ul>"},{"location":"Tech%20Talks/2021/europython-2021/","title":"EuroPython 2021","text":""},{"location":"Tech%20Talks/2021/europython-2021/#create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github","title":"Create beautiful and localized documentations and websites using MkDocs + Github","text":"<p>In this talk I walk you through all the steps needed to create and host feature rich and localized documentations (and even websites) using MkDocs + Github Pages.</p> <ul> <li>Click here to access the live version of this talk on my blog!</li> <li>Github repository for the talk</li> </ul>"},{"location":"Tech%20Talks/2021/europython-2021/#tech-for-good-build-the-world-you-want-to-live-in","title":"Tech for Good, build the world you want to live in","text":"<p>As a EuroPython 2021 Gold sponsor, Numberly had a 30 minutes talk slot reserved on the conference agenda. Instead of using it to promote ourselves, we decided to hand over the stage to some of the non profit organization we support and work with through our 1000mercis Impacts fundation.</p>"},{"location":"Tech%20Talks/2021/scylla-confluent-webinar-2021/","title":"Scylla + Confluent Kafka (webinar)","text":""},{"location":"Tech%20Talks/2021/scylla-confluent-webinar-2021/#building-event-streaming-architectures-on-scylla-and-confluent-with-kafka","title":"Building Event Streaming Architectures on Scylla and Confluent with Kafka","text":"<p>I had the privilege to share the stage with Confluent's Tim Berglund and my colleague Othmane El Metioui for this great webinar.</p> <p>This was a particular day as Tim's daughter happened to be in the middle of her labor and was about to give birth to his first grandson while we were live on stage. And at the same time Confluent announced they were going public!</p> <p>Peter Corless of ScyllaDB did a great write-up of the webinar where we shared on how we combine the powers of Scylla and Confluent Kafka at Numberly.</p>"},{"location":"Tech%20Talks/2021/scylla-summit-2021/","title":"Scylla Summit 2021","text":""},{"location":"Tech%20Talks/2021/scylla-summit-2021/#scylla-user-award-community-member-of-the-year","title":"Scylla User Award : Community Member of the Year","text":"<p>I'm very proud to have been awared by the Scylla community for the third time!</p> <p>Quote</p> <p>Scylla Community Member of the Year: Alexys Jacob, Numberly</p> <p>Alexys Jacob, CTO of Numberly, is a familiar name for those who track the Scylla User Awards. This year he is recognized for partnering with our engineers for the development of a shard-aware driver written in Python. This new driver provides significantly better database performance for users of the popular Python language. You can follow the course of this development in part one and part two of our blog series on the Python shard-aware driver, and also watch Alexys\u2019 Scylla Summit presentation on Getting the Scylla Shard-Aware Drivers Faster.</p> <p>Through my work and contributions to the Python Scylla shard-aware driver I found some caveats that could benefit all the others Scylla shard-aware drivers. This inspired a new advanced shard-aware CQL port that Scylla 4.3+ now offers to such newer drivers. The benefit? Faster drivers, faster queries!</p>"},{"location":"Tech%20Talks/2021/scylla-summit-2021/#getting-the-scylla-shard-aware-drivers-faster","title":"Getting the Scylla Shard-Aware drivers faster","text":"<p>This talk is a 10min talk on newer shard-aware drivers!</p>"},{"location":"Tech%20Talks/2022/scylla-rust-webinar/","title":"Scylla Webinar May 2022","text":"<p>The ScyllaDB folks and I hosted a webinar on May 5th 2022 that allowed me to update my Scylla Summit 2022 talk and share my experience of learning Rust the hard way in the migration of Python pipelines to Rust.</p> <p>Abstract</p> <p>Numberly operates business-critical data pipelines and applications where failure and latency means \"lost money\" in the best-case scenario. Most of their data pipelines and applications are deployed on Kubernetes and rely on Kafka and ScyllaDB, with Kafka acting as the message bus and ScyllaDB as the source of data for enrichment. The availability and latency of both systems are thus very important for data pipelines. While most of Numberly\u2019s applications are developed using Python, they found a need to move high-performance applications to Rust in order to benefit from a lower-level programming language.</p> <p>Learn the lessons from Numberly\u2019s experience, including:</p> <ul> <li>Rationale in selecting a lower-level language</li> <li>Developing using a lower-level Rust code base</li> <li>Observability and analyzing latency impacts with Rust</li> <li>Tuning everything from Apache Avro to driver client settings</li> <li>How to build a mission-critical system combining Apache Kafka and ScyllaDB</li> <li>Half a year Rust in production feedback</li> </ul>"},{"location":"Tech%20Talks/2022/scylla-summit-2022/","title":"Scylla Summit 2022","text":"<p>The 2022 edition of Scylla Summit was full of great presentations and I was fortunate to be part of it.</p> <p>I shared my experience of the journey that led me rewrite three Python based data pipeline applications into one written in Rust!</p> <p></p> <p>Abstract</p> <p>Numberly operates business-critical data pipelines and applications where failure and latency means \"lost money\" in the best case scenario.</p> <p>Most of those data pipelines and applications are deployed on Kubernetes and rely on Kafka and ScyllaDB, where Kafka acts as the message bus and ScyllaDB as the source of some data enrichment. The availability and latency of both systems are thus very important to us because they mix and match data in the early stage of our pipelines to be consumed by our platforms. Most of our applications are developed using Python. But we always felt that we could benefit from a lower-level programming language to squeeze the performance of our hardware even further for some of the most demanding applications.</p> <p>So, when an important part of our data pipeline was to be adjusted to reflect some important changes in our platforms, we thought it was a great opportunity to rewrite it in Rust! Moving to Rust was hard, not only because of the language itself, but because being at a lower level allowed us to see, test, and demonstrate things that we could not pinpoint or explain that well using Python.</p> <p>We spent a lot of time analyzing the latency impacts of code patterns and client driver settings and ended up contributing to Apache Avro as we went down the rabbit hole. This session will share our experience transitioning from Python to Rust while meeting the expectations of a business-critical application mixing data from Confluent Kafka and ScyllaDB.</p> <p>There will be code snippets, graphs, numbers, tears, pull requests, grins, latency results, smiles, rants of frustration, and a lot of fun!</p>"},{"location":"Tech%20Talks/2023/europython-2023/","title":"EuroPython 2023","text":""},{"location":"Tech%20Talks/2023/europython-2023/#would-rust-make-you-a-better-pythonista","title":"Would Rust Make You A Better Pythonista?","text":"<p>What would a Pythonista gain from becoming a Rustacean other than semicolons and brackets?</p> <p>In this talk I'll share the learnings and achievements I got by adding the Rust programming language into my Python life. Illustrating a real story now in production at scale, I'll walk you through all the pains and joys of this unexpected journey which changed me more than I anticipated.</p> <ul> <li>See the slides</li> </ul>"},{"location":"Tech%20Talks/2023/scylla-infoq-webinar/","title":"Scylla + InfoQ Webinar","text":"<p>The InfoQ community hosted a webinar on March 2nd 2023 where Peter Coreless from ScyllaDB and I shared the stage. This opportunity allowed me to once again update my Scylla Summit 2022 talk and share my experience of learning Rust the hard way in the migration of Python pipelines to Rust.</p> <p>This topic does have a lot of traction it seems, that's for good!</p> <p>Abstract</p> <p>Numberly operates business-critical data pipelines and applications where failure and latency means \"lost money\" in the best-case scenario. Most of their data pipelines and applications are deployed on Kubernetes and rely on Kafka and ScyllaDB, with Kafka acting as the message bus and ScyllaDB as the source of data for enrichment. The availability and latency of both systems are thus very important for data pipelines. While most of Numberly\u2019s applications are developed using Python, they found a need to move high-performance applications to Rust in order to benefit from a lower-level programming language.</p> <p>Learn the lessons from Numberly\u2019s experience, including:</p> <ul> <li>Rationale in selecting a lower-level language</li> <li>Developing using a lower-level Rust code base</li> <li>Observability and analyzing latency impacts with Rust</li> <li>Tuning everything from Apache Avro to driver client settings</li> <li>How to build a mission-critical system combining Apache Kafka and ScyllaDB</li> <li>Half a year Rust in production feedback</li> </ul>"},{"location":"Tech%20Talks/2023/scylla-summit-2023/","title":"Scylla Summit 2023","text":""},{"location":"Tech%20Talks/2023/scylla-summit-2023/#building-a-100-scylladb-shard-aware-application-using-rust","title":"Building a 100% ScyllaDB Shard-Aware Application Using Rust","text":"<p>Yet another edition of the Scylla Summit with exciting presentations which I was fortunate to be part of. But this year I'm proud to have shared the stage with two of my colleagues: Joseph and Yassir!</p> <p></p> <p>We shared our experience and technical achievements in building a 100% shard-aware application using Rust that allowed us to leverage the thread-per-core architecture of ScyllaDB inside our own application!</p> <p>You can find a written transcript of the presentation on the numberly.tech blog.</p> <p>Abstract</p> <p>At Numberly we designed an entire data processing application on ScyllaDB's low-level internal sharding using Rust. Starting from what seemed like a crazy idea, our application design actually delivers amazing strengths like idempotence, distributed and predictable data processing with infinite scalability thanks to ScyllaDB. Having ScyllaDB as our only backend, we managed to reduce operational costs while benefiting from core architectural paradigms like:</p> <ul> <li>predictable data distribution and processing capacity</li> <li>idempotence by leveraging deterministic data sharding</li> <li>optimized data manipulation using consistent shard-aware partition keys</li> <li>virtually infinite scaling along ScyllaDB</li> </ul> <p>This talk will walk you through this amazing experience. We will share our thought process, the roadblocks we overcame, and the numerous contributions we made to ScyllaDB to reach our goal in production. Guaranteed 100% made with love in Paris using ScyllaDB and Rust! </p>"},{"location":"Tech%20Talks/2023/scylla-summit-2023/#scylla-award-technical-achievement","title":"Scylla Award: Technical Achievement","text":"<p>This work and the numerous Open-Source contributions that we brought to the ScyllaDB community has been rewarded by the Technical Achievement Award from ScyllaDB.</p> <p>I feel very honored by this award as it demonstrates that any innovative idea can be transformed into breakthrough technical achievements and contributions. Now live in production at Numberly!</p>"},{"location":"fr/","title":"About","text":"<p> Bienvenue et merci de votre visite !</p>"},{"location":"fr/#gentoo-linux-developer","title":"Gentoo Linux developer","text":"<p>Cela fait plus de 10 ans que je suis d\u00e9veloppeur Gentoo Linux. Je maintiens principalement des packages li\u00e9s \u00e0 l'\u00e9cosyst\u00e8me des bases de donn\u00e9es distribu\u00e9es et du clustering. Je fais aussi partie de l'\u00e9quipe qui rend disponible les containers docker gentoo linux.</p>"},{"location":"fr/#open-source-author-and-contributor","title":"Open Source author and contributor","text":"<p>Je suis engag\u00e9 dans plusieurs communaut\u00e9s Open Source et je consacre une part non n\u00e9gligeable de mon temps \u00e0 contribuer \u00e0 des projets Open Source. GitHub m'a remerci\u00e9 d'\u00eatre l'un des 900 maintainers Open Source dont l'entreprise d\u00e9pend pour op\u00e9rer sa plateforme.</p> <p>Je suis l'auteur de plusieurs projets Open Source comme par exemple :</p> <ul> <li>py3status: un wrapper \u00e0 i3status sous i3wm \u00e9crit en Python</li> <li>uhashring: une librairie compl\u00e8te qui impl\u00e9mente du consistent hashing en Python compatible avec ketama, ce projet a \u00e9t\u00e9 d\u00e9sign\u00e9 comme critique sur PyPI</li> <li>mkdocs-static-i18n: un plugin pour MkDocs vous permettant de cr\u00e9er une documentation (ou un site web comme le mien) multi-langue</li> <li>matterhook: une librairie Python permettant d'interagir avec les Webhooks Mattermost</li> </ul> <p>Voici une petite liste de projets auxquels je contribue :</p> <ul> <li>MkDocs</li> <li>Apache Airflow</li> <li>Apache Avro (Rust)</li> <li>MongoDB / PyMongo</li> <li>Python asyncio Kafka library</li> <li>Scylla / Scylla Python Driver</li> </ul>"},{"location":"fr/#tech-speaker-writer","title":"Tech speaker &amp; writer","text":"<p>Depuis de nombreuses ann\u00e9es j'ai la chance que certaines de mes propositions de sujets soient accept\u00e9es lors de conf\u00e9rences techniques et d'y prendre la parole.</p> <p>C'est important pour moi d'au moins proposer des sujets car je crois que l'exp\u00e9rience n'a de sens que si elle est partag\u00e9e.</p> <p>C'est pourquoi j'\u00e9cris aussi des blog posts \u00e0 propos de certaines nouvelles de projets Open Source ou \u00e0 propos de probl\u00e8mes techniques auxquels j'ai \u00e9t\u00e9 confront\u00e9.</p>"},{"location":"fr/#awarded-community-member","title":"Awarded community member","text":"<p>Je suis tr\u00e8s honor\u00e9 d'avoir donn\u00e9 des interviews et avoir re\u00e7u des prix pour mon engagement communautaire et mon exp\u00e9rience professionnelle.</p>"},{"location":"fr/#fpv-hobby","title":"FPV Hobby","text":"<p>Lorsque je ne bidouille pas du code sur mon ordinateur j'aime bidouiller mes quadcopters et voler en FPV !</p> <p>Je partage aussi ma modeste exp\u00e9rience de cette discipline dans mon FPV Handbook et mes guides de Builds FPV.</p>"},{"location":"fr/#support-me","title":"Support me","text":"<p>Si mon travail vous impacte positivement, vous pouvez dire merci en donnant une \u00e9toile  \u00e0 l'un de mes projets:</p> <p></p> <p>Vous pouvez aussi me remercier avec via un don libre sur Paypal ou Github Sponsor.</p>"},{"location":"fr/#contact-me","title":"Contact me","text":"<ul> <li>Sur Twitter</li> <li>Sur Mastodon</li> <li>Sur IRC Libera.Chat #gentoo-containers ou IRC OFTC #py3status</li> <li>Sur Discord ultrabug</li> </ul>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#a-digital-hd-build-on-the-apex-base-frame-kit","title":"A digital HD build on the Apex Base Frame Kit","text":"<p> Pour mon premier build DIY je voulais un 5\" solide.</p> <p>J'ai choisi le ImpulseRC Apex Base Frame Kit car je ne voulais pas d\u00e9pendre d'une frame sp\u00e9cifique HD et perdre la place faite sp\u00e9cifiquement pour le DJI Air Unit.</p> <p>Ce fut un challenge de tout faire tenir proprement mais j'adore le r\u00e9sultat et j'esp\u00e8re que vous appr\u00e9cierez la densit\u00e9 et l'\u00e9quilibre du build.</p> <p>Cela donne vie \u00e0 un build compact (mais pas trop) sans sacrifier la place pour l'\u00e9lectronique. C'est propre (en tout cas pour moi) et tr\u00e8s puissant.</p> <p>J'ai pris \u00e9norm\u00e9ment de plaisir \u00e0 travailler dessus et j'esp\u00e8re que vous le sentirez au long de votre lecture. Amusez-vous bien !</p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#hardware-list","title":"Hardware list","text":"<p>Note</p> <p>Les liens ci-dessous ne sont PAS des liens d'affiliation. Ce sont tout simplement les liens o\u00f9 j'ai moi-m\u00eame command\u00e9 le mat\u00e9riel n\u00e9cessaire \u00e0 ce build.</p> <ul> <li> ImpulseRC Apex Base Frame Kit<ul> <li> T-Motor Combo F7 HD FC + F55A PRO II</li> <li> TBS Motor Steele Ethix Stout V3</li> <li> TBS Tracer Nano RX</li> <li> Caddx Vista HD System (see improvement notes below)</li> <li> Apex HD CAMERA SIDE PLATE KIT (see improvement notes below)</li> <li> 20cm Coaxial Cable for Caddx Vista (20cm !)</li> <li> Vifly Finder Mini - Buzzer (lightweight, does a good job)</li> <li> XT60 14AWG 10cm cable (longer)</li> <li> Apex DJI antenna TPU holder (will fit tracer immortal-t)</li> </ul> </li> <li> HQProp ETHIX P3 Peanut Butter &amp; Jelly 5.1x3x3 - PC (2x CW + 2xCCW)</li> <li> TrueRC Singularity U.FL Lite 112mm 5.8GHz - LHCP</li> <li> Strap Lipo KEVLAR 240x16mm - DFR</li> </ul>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#notes-pour-ameliorations-futures","title":"Notes pour am\u00e9liorations futures","text":"<ul> <li> Passer sur le Caddx Nebula Pro Vista Kit pour ne plus d\u00e9pendre du Apex HD CAMERA SIDE PLATE KIT et gagner 3 grammes et quelques euros ?</li> </ul>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#frame-assembly","title":"Frame assembly","text":"<p>Suivre le guide officiel de ImpulseRC pour assembler la frame.</p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#caddx-vista-on-the-20x20-rear-mounting-holes","title":"Caddx Vista on the 20x20 rear mounting holes","text":"<p>Note</p> <p>Vous aurez besoin de 4 longues vis M2 non incluses dans le kit. Les \u00e9crous sont cependant ceux fournis avec le kit.</p> <p></p> <p></p> <p>Le c\u00e2ble de 20cm pour Caddx Vista est mieux prot\u00e9g\u00e9 que celui d'origine.</p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#caddx-vista-vtx-antenna-mounting","title":"Caddx Vista VTX antenna mounting","text":"<p>Utiliser le TPU pour antennes DJI fix\u00e9 \u00e0 l'arri\u00e8re pour l'antenne U.FL en la zippant sur une entretoise pour prot\u00e9ger le vista.</p> <p>Les antennes Tracer Immortal T tiennent bien : une est zipp\u00e9e sur le sabot de la plate du bas fourni et l'autre est enfich\u00e9e dans la partie verticale du TPU.</p> <p></p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#esc-and-xt60-mounting","title":"ESC and XT60 mounting","text":"<p>Dans ma premi\u00e8re version de ce build j'ai coup\u00e9 le XT60 trop court et les h\u00e9lices finissent par le toucher. Depuis j'utilise un XT60 de 10cm zipp\u00e9 \u00e0 l'entretoise avant.</p> <p>Warning</p> <p>Notez bien que l'ESC est mont\u00e9 \u00e0 l'envers afin d'utiliser l'espace disponible \u00e0 l'avant pour placer le capacitor. Vous devrez r\u00e9allouer les ressources moteurs dans la CLI (voir la section betaflight ci-apr\u00e8s).</p> <p></p> <p></p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#rx-wiring","title":"RX wiring","text":"<p>Aucune surprise dans le branchement du TBS Tracer Nano RX !</p> <p></p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#fc-mounting","title":"FC mounting","text":"<p>Utilisez le c\u00e2ble FC-vers-ESC fourni le plus long (cela passe parfaitement, attention il y a un sens) et connectez la Caddx Vista.</p> <p></p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#rx-placement","title":"RX placement","text":"<p>Une fois prot\u00e9g\u00e9, le RX tient sur le Caddx Vista gr\u00e2ce \u00e0 du scotch double face.</p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#buzzer","title":"Buzzer","text":"<p>Le Vifly finder mini est l\u00e9ger \u00e0 l'avant et occupe parfaitement l'espace disponible, c'est un super compromis.</p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#finishing-the-build","title":"Finishing the build","text":"<p>Finissez le build en pla\u00e7ant la top plate et les caches en plastiques sur les bras fournis.</p> <p></p> <p></p>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#motor-resource-reallocation","title":"Motor resource reallocation","text":"<p>Comme l'ESC a \u00e9t\u00e9 mont\u00e9 \u00e0 l'envers il est n\u00e9cessaire de repositionner les moteurs dans la CLI. Selon votre montage, les ID peuvent changer.</p> <pre><code># resources\nresource MOTOR 1 B01\nresource MOTOR 2 B00\nresource MOTOR 3 C07\nresource MOTOR 4 C06\nsave\n</code></pre>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#minimal-cli-diff","title":"Minimal CLI diff","text":"<p>CLI diff de la configuration minimale sans tuning approfondi. Voir la section d'apr\u00e8s pour avoir ma derni\u00e8re version tun\u00e9e.</p> <pre><code>#\n# Building AutoComplete Cache ... Done!\n#\n# diff all\n\n# version\n# Betaflight / STM32F7X2 (S7X2) 4.2.6 Jan  5 2021 / 19:08:42 (a4b6db1e7) MSP API: 1.43\n# config: manufacturer_id: TMTR, board_name: TMOTORF7, version: e02dd6f2, date: 2020-11-04T11:31:26Z\n\n# start the command batch\nbatch start\n\n# reset configuration to default settings\ndefaults nosave\n\nboard_name TMOTORF7\nmanufacturer_id TMTR\nmcu_id 002f00263338510639393832\nsignature\n\n# resources\nresource MOTOR 1 B01\nresource MOTOR 2 B00\nresource MOTOR 3 C07\nresource MOTOR 4 C06\n\n# feature\nfeature -RX_PARALLEL_PWM\nfeature RX_SERIAL\nfeature TELEMETRY\n\n# beacon\nbeacon RX_LOST\nbeacon RX_SET\n\n# serial\nserial 1 1 115200 57600 0 115200\nserial 4 64 115200 57600 0 115200\n\n# aux\naux 0 0 0 1800 2100 0 0\naux 1 1 2 1300 1700 0 0\naux 2 13 1 1300 2100 0 0\naux 3 35 2 1800 2100 0 0\n\n# rxfail\nrxfail 7 s 750\n\n# master\nset gyro_lowpass2_hz = 375\nset dyn_notch_width_percent = 0\nset dyn_notch_q = 250\nset dyn_notch_min_hz = 90\nset dyn_notch_max_hz = 350\nset dyn_lpf_gyro_min_hz = 300\nset dyn_lpf_gyro_max_hz = 750\nset acc_calibration = 26,3,238,1\nset min_check = 1000\nset rssi_channel = 8\nset serialrx_provider = CRSF\nset dshot_bidir = ON\nset motor_pwm_protocol = DSHOT600\nset small_angle = 180\nset osd_warn_rssi = ON\nset osd_rssi_alarm = 40\nset osd_vbat_pos = 257\nset osd_rssi_pos = 2486\nset osd_rssi_dbm_pos = 161\nset osd_tim_1_pos = 353\nset osd_tim_2_pos = 321\nset osd_flymode_pos = 2241\nset osd_throttle_pos = 313\nset osd_vtx_channel_pos = 193\nset osd_craft_name_pos = 33\nset osd_gps_speed_pos = 161\nset osd_gps_lon_pos = 33\nset osd_gps_lat_pos = 1\nset osd_gps_sats_pos = 65\nset osd_home_dir_pos = 2275\nset osd_home_dist_pos = 2145\nset osd_flight_dist_pos = 184\nset osd_altitude_pos = 2177\nset osd_warnings_pos = 2441\nset osd_avg_cell_voltage_pos = 2516\nset osd_disarmed_pos = 2411\nset osd_flip_arrow_pos = 65\nset osd_core_temp_pos = 248\nset osd_log_status_pos = 97\nset osd_gps_sats_show_hdop = OFF\n\nprofile 0\n\n# profile 0\nset dyn_lpf_dterm_min_hz = 105\nset dyn_lpf_dterm_max_hz = 255\nset dterm_lowpass2_hz = 225\nset d_pitch = 32\nset d_roll = 30\nset d_min_roll = 0\nset d_min_pitch = 0\n\nprofile 1\n\nprofile 2\n\n# restore original profile selection\nprofile 0\n\nrateprofile 0\n\nrateprofile 1\n\nrateprofile 2\n\nrateprofile 3\n\nrateprofile 4\n\nrateprofile 5\n\n# restore original rateprofile selection\nrateprofile 2\n\n# save configuration\nsave\n</code></pre>"},{"location":"fr/FPV%20Builds/ImpulseRC%20Apex/#cli-diff-actuel","title":"CLI diff actuel","text":"<p>Ce CLI diff est mis \u00e0 jour au fur et \u00e0 mesure de l'avancement de mon tune.</p> <p>J'ai commenc\u00e9 par me baser sur les presets freestyle de UAV Tech que je tente modestement d'am\u00e9liorer avec le temps.</p> <pre><code># \n# Building AutoComplete Cache ... Done!\n# \n# diff all\n\n# version\n# Betaflight / STM32F7X2 (S7X2) 4.2.6 Jan  5 2021 / 19:08:42 (a4b6db1e7) MSP API: 1.43\n# config: manufacturer_id: TMTR, board_name: TMOTORF7, version: e02dd6f2, date: 2020-11-04T11:31:26Z\n\n# start the command batch\nbatch start\n\n# reset configuration to default settings\ndefaults nosave\n\nboard_name TMOTORF7\nmanufacturer_id TMTR\nmcu_id 002f00263338510639393832\nsignature \n\n# resources\nresource MOTOR 1 B01\nresource MOTOR 2 B00\nresource MOTOR 3 C07\nresource MOTOR 4 C06\n\n# feature\nfeature -RX_PARALLEL_PWM\nfeature RX_SERIAL\nfeature TELEMETRY\n\n# serial\nserial 1 1 115200 57600 0 115200\nserial 4 64 115200 57600 0 115200\n\n# aux\naux 0 0 0 1800 2100 0 0\naux 1 1 2 1300 1700 0 0\naux 2 13 1 1300 2100 0 0\naux 3 35 2 1800 2100 0 0\n\n# rxfail\nrxfail 7 s 750\n\n# master\nset gyro_lowpass2_hz = 375\nset dyn_notch_width_percent = 0\nset dyn_notch_q = 250\nset dyn_notch_max_hz = 350\nset dyn_lpf_gyro_min_hz = 300\nset dyn_lpf_gyro_max_hz = 750\nset acc_calibration = 26,3,238,1\nset min_check = 1000\nset rssi_channel = 8\nset serialrx_provider = CRSF\nset sbus_baud_fast = ON\nset dshot_bidir = ON\nset motor_pwm_protocol = DSHOT600\nset small_angle = 180\nset osd_warn_rssi = ON\nset osd_rssi_alarm = 40\nset osd_vbat_pos = 257\nset osd_rssi_pos = 2486\nset osd_rssi_dbm_pos = 161\nset osd_tim_1_pos = 353\nset osd_tim_2_pos = 321\nset osd_flymode_pos = 2241\nset osd_throttle_pos = 313\nset osd_vtx_channel_pos = 193\nset osd_craft_name_pos = 33\nset osd_gps_speed_pos = 161\nset osd_gps_lon_pos = 33\nset osd_gps_lat_pos = 1\nset osd_gps_sats_pos = 65\nset osd_home_dir_pos = 227\nset osd_home_dist_pos = 97\nset osd_flight_dist_pos = 184\nset osd_altitude_pos = 129\nset osd_warnings_pos = 2441\nset osd_avg_cell_voltage_pos = 2516\nset osd_disarmed_pos = 2411\nset osd_flip_arrow_pos = 65\nset osd_core_temp_pos = 248\nset osd_log_status_pos = 97\nset osd_gps_sats_show_hdop = OFF\nset gyro_rpm_notch_harmonics = 2\n\nprofile 0\n\n# profile 0\nset dyn_lpf_dterm_min_hz = 105\nset dyn_lpf_dterm_max_hz = 255\nset dterm_lowpass2_hz = 225\nset pidsum_limit = 1000\nset pidsum_limit_yaw = 1000\nset p_pitch = 69\nset d_pitch = 44\nset f_pitch = 171\nset p_roll = 63\nset d_roll = 40\nset f_roll = 162\nset p_yaw = 68\nset f_yaw = 162\nset d_min_roll = 0\nset d_min_pitch = 0\n\nprofile 1\n\nprofile 2\n\n# restore original profile selection\nprofile 0\n\nrateprofile 0\n\nrateprofile 1\n\nrateprofile 2\n\n# rateprofile 2\nset rates_type = ACTUAL\nset roll_rc_rate = 1\nset pitch_rc_rate = 1\nset yaw_rc_rate = 1\nset roll_expo = 54\nset pitch_expo = 54\nset yaw_expo = 54\nset roll_srate = 100\nset pitch_srate = 100\nset tpa_breakpoint = 1750\n\nrateprofile 3\n\nrateprofile 4\n\nrateprofile 5\n\n# restore original rateprofile selection\nrateprofile 2\n\n# save configuration\nsave\n</code></pre>"},{"location":"fr/FPV%20Handbook/","title":"Introduction","text":"<p> Salut \u00e0 toi, pilote FPV !</p> <p>J'ai cr\u00e9\u00e9 ce manuel afin de m'aider \u00e0 avoir sous la main et r\u00e9f\u00e9rencer tout ce qui m'est utile dans ma pratique de cette discipline. Je le consigne publiquement au cas o\u00f9 cela puisse \u00eatre utile \u00e0 quelqu'un d'autre !</p> <p>Ceci est un project Open Source alors n'h\u00e9sites pas \u00e0 y contribuer en ouvrant une issue Github pour discuter ou une Pull Request pour proposer un changement.</p> <p>Bon vol!</p>"},{"location":"fr/Tech%20Blog/","title":"Articles techniques","text":"<p> J'aime partager mes d\u00e9couvertes et mon exp\u00e9rience \u00e0 travers des blog posts afin qu'ils puissent r\u00e9f\u00e9renc\u00e9s et trouv\u00e9s facilement.</p> <p>N'h\u00e9sitez pas \u00e0 me contacter si vous pensez que des \u00e9l\u00e9ments n\u00e9cessitent une clarification !</p>"},{"location":"fr/Tech%20Talks/","title":"Community talks and awards","text":"<p> Depuis de nombreuses ann\u00e9es j'ai la chance que certaines de mes propositions de sujets soient accept\u00e9es lors de conf\u00e9rences techniques et d'y prendre la parole.</p> <p>Je suis aussi tr\u00e8s honor\u00e9 d'avoir donn\u00e9 des interviews et avoir re\u00e7u des prix pour mon engagement communautaire et mon exp\u00e9rience professionnelle.</p>"}]}