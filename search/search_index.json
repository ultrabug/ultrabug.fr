{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About \u00b6 Welcome around and thank you for visiting! Open Source community service \u00b6 Gentoo Linux developer \u00b6 I've been a Gentoo Linux developer for more than 10 years now. I mainly focus on packaging distributed databases ecosystems and cluster related projects. I'm also part of the team who provides gentoo linux docker containers . Open Source author and contributor \u00b6 I'm strongly involved in various Open Source communities and am spending a fair amount of my time contributing to Open Source projects. I am the author of several Open Source projects, for instance: py3status : an i3wm i3status wrapper written in Python uhashring : a full featured consistent hashing Python library compatible with ketama matterhook : a Python library to interact with Mattermost Webhooks This is a quick list of projects I have contributed to: Apache Airflow MongoDB / PyMongo Python asyncio Kafka library Scylla / Scylla Python Driver Tech speaker & writer \u00b6 I've been fortunate since some of my talk proposals have been accepted over the last years at tech conferences where I got to participate as a speaker. This is very important for me because I believe that real world experience is never as useful as when it's shared with your peers and community. I also write blog posts about some of the technical challenges I face and some Open Source news in the hope it is of interest or help to someone. Awarded community member \u00b6 I've been very humbled to be interviewed and awarded for my community work and professional experience. FPV Hobby \u00b6 When I'm not hacking computers and code I like to hack quadcopters and to fly them in FPV ! I share my modest experience on this hobby through my FPV Handbook and FPV Builds guides. Contact me \u00b6 On Twitter On IRC freenode #gentoo-dev or #py3status","title":"About"},{"location":"#about","text":"Welcome around and thank you for visiting!","title":"About"},{"location":"#open-source-community-service","text":"","title":"Open Source community service"},{"location":"#gentoo-linux-developer","text":"I've been a Gentoo Linux developer for more than 10 years now. I mainly focus on packaging distributed databases ecosystems and cluster related projects. I'm also part of the team who provides gentoo linux docker containers .","title":"Gentoo Linux developer"},{"location":"#open-source-author-and-contributor","text":"I'm strongly involved in various Open Source communities and am spending a fair amount of my time contributing to Open Source projects. I am the author of several Open Source projects, for instance: py3status : an i3wm i3status wrapper written in Python uhashring : a full featured consistent hashing Python library compatible with ketama matterhook : a Python library to interact with Mattermost Webhooks This is a quick list of projects I have contributed to: Apache Airflow MongoDB / PyMongo Python asyncio Kafka library Scylla / Scylla Python Driver","title":"Open Source author and contributor"},{"location":"#tech-speaker-writer","text":"I've been fortunate since some of my talk proposals have been accepted over the last years at tech conferences where I got to participate as a speaker. This is very important for me because I believe that real world experience is never as useful as when it's shared with your peers and community. I also write blog posts about some of the technical challenges I face and some Open Source news in the hope it is of interest or help to someone.","title":"Tech speaker &amp; writer"},{"location":"#awarded-community-member","text":"I've been very humbled to be interviewed and awarded for my community work and professional experience.","title":"Awarded community member"},{"location":"#fpv-hobby","text":"When I'm not hacking computers and code I like to hack quadcopters and to fly them in FPV ! I share my modest experience on this hobby through my FPV Handbook and FPV Builds guides.","title":"FPV Hobby"},{"location":"#contact-me","text":"On Twitter On IRC freenode #gentoo-dev or #py3status","title":"Contact me"},{"location":"FPV%20Builds/ImpulseRC%20Apex/","text":"Apex 5\" HD on base frame kit \u00b6 A digital HD build on the Apex Base Frame Kit \u00b6 For my first DIY build ever I wanted a strong 5\" frame. I chose the ImpulseRC Apex Base Frame Kit because I did not want to rely on a HD specific frame and loose the space made for the DJI Air Unit. It was quite a challenge to get everything to fit properly but I love the result and I hope you'll appreciate the density and great balance of this build. This ends up as a compact (but not too much) build without sacrifying the space for the electronics. It is clean (to my standards at least) and very powerful. I enjoyed working on it very much and I hope you'll get a sense of it reading this build guide. Enjoy! Hardware list \u00b6 Note Those links are NOT affiliate links. I just share the links where I actually bought the stuff needed for this build. ImpulseRC Apex Base Frame Kit T-Motor Combo F7 HD FC + F55A PRO II TBS Motor Steele Ethix Stout V3 TBS Tracer Nano RX Caddx Vista HD System (see improvement notes below) Apex HD CAMERA SIDE PLATE KIT (see improvement notes below) 20cm Coaxial Cable for Caddx Vista (20cm !) Vifly Finder Mini - Buzzer (lightweight) XT60 14AWG 10cm cable (longer) Apex DJI antenna TPU holder (will fit tracer immortal-t) HQProp ETHIX P3 Peanut Butter & Jelly 5.1x3x3 - PC (2x CW + 2xCCW) TrueRC Singularity U.FL Lite 112mm 5.8GHz - LHCP Strap Lipo KEVLAR 240x16mm - DFR Notes to self for future improvements \u00b6 Switch to the Caddx Nebula Pro Vista Kit so we do not need the Apex HD CAMERA SIDE PLATE KIT any more, save 3 grams and money? Build steps \u00b6 Frame assembly \u00b6 Follow the official ImpulseRC Apex guide to assemble the frame. Caddx Vista on the 20x20 rear mounting holes \u00b6 Note You will need 4 long M2 screws that are not provided with the kit. Nuts are the ones provided with the kit. The 20cm cable for Caddx Vista is better protected than the standard one. Caddx Vista VTX antenna mounting \u00b6 Use the Apex DJI antenna TPU holder on the rear and make sure to zip tie the U.FL antenna to the standoff. The Tracer Immortal T antennas fit well: one is zip tied in the lower plate by the provided clamp and the other is sticked in the vertical section of the TPU. ESC and XT60 mounting \u00b6 In my first version I cut short the XT60 leads but it held the XT60 too close to the motors. Instead I'm now using a 10cm XT60 cable zip tied to the front standoff. Warning Be mindful of the fact that the ESC mounted in reverse position to leverage the space at the front for the capacitor. You will need to reallocate the motor resources on the CLI (see the betaflight section below). RX wiring \u00b6 Nothing surprising here, wiring the TBS Tracer Nano RX is straightforward! FC mounting \u00b6 Use the longest of the provided FC-to-ESC cables (it connects perfectly, mind the wire direction) and connect the Caddx Vista. RX placement \u00b6 Once protected, the RX is stuck on top of the Caddx Vista using double sided tape. Buzzer \u00b6 I chose the Vifly finder mini to get a lightweight yet powerful buzzer that fits perfectly at the front. Finishing the build \u00b6 Finish the build by placing the top plate and the plastic provided covers on the arms. Betaflight configuration \u00b6 Betaflight target: TMTR/TMOTORF7(STM32F7X2) Ports: Bi-directional DShot is enabled and supported natively by the BLHeli32 ESC: Filter settings to accomodate the bi-directional DShot: Modes: Joshua Bardwell's OSD for DJI FPV Google settings: Motor resource reallocation \u00b6 Since the ESC is mounted in reverse position, the motors need to be reassigned on the CLI. Depending on your wiring, IDs may change. # resources resource MOTOR 1 B01 resource MOTOR 2 B00 resource MOTOR 3 C07 resource MOTOR 4 C06 save Minimal CLI diff \u00b6 This is the minimal CLI diff that I initially set up without advanced tuning. To get my current tune, see the next section. # # Building AutoComplete Cache ... Done! # # diff all # version # Betaflight / STM32F7X2 (S7X2) 4.2.6 Jan 5 2021 / 19:08:42 (a4b6db1e7) MSP API: 1.43 # config: manufacturer_id: TMTR, board_name: TMOTORF7, version: e02dd6f2, date: 2020-11-04T11:31:26Z # start the command batch batch start # reset configuration to default settings defaults nosave board_name TMOTORF7 manufacturer_id TMTR mcu_id 002f00263338510639393832 signature # resources resource MOTOR 1 B01 resource MOTOR 2 B00 resource MOTOR 3 C07 resource MOTOR 4 C06 # feature feature -RX_PARALLEL_PWM feature RX_SERIAL feature TELEMETRY # beacon beacon RX_LOST beacon RX_SET # serial serial 1 1 115200 57600 0 115200 serial 4 64 115200 57600 0 115200 # aux aux 0 0 0 1800 2100 0 0 aux 1 1 2 1300 1700 0 0 aux 2 13 1 1300 2100 0 0 aux 3 35 2 1800 2100 0 0 # rxfail rxfail 7 s 750 # master set gyro_lowpass2_hz = 375 set dyn_notch_width_percent = 0 set dyn_notch_q = 250 set dyn_notch_min_hz = 90 set dyn_notch_max_hz = 350 set dyn_lpf_gyro_min_hz = 300 set dyn_lpf_gyro_max_hz = 750 set acc_calibration = 26,3,238,1 set min_check = 1000 set rssi_channel = 8 set serialrx_provider = CRSF set dshot_bidir = ON set motor_pwm_protocol = DSHOT600 set small_angle = 180 set osd_warn_rssi = ON set osd_rssi_alarm = 40 set osd_vbat_pos = 257 set osd_rssi_pos = 2486 set osd_rssi_dbm_pos = 161 set osd_tim_1_pos = 353 set osd_tim_2_pos = 321 set osd_flymode_pos = 2241 set osd_throttle_pos = 313 set osd_vtx_channel_pos = 193 set osd_craft_name_pos = 33 set osd_gps_speed_pos = 161 set osd_gps_lon_pos = 33 set osd_gps_lat_pos = 1 set osd_gps_sats_pos = 65 set osd_home_dir_pos = 2275 set osd_home_dist_pos = 2145 set osd_flight_dist_pos = 184 set osd_altitude_pos = 2177 set osd_warnings_pos = 2441 set osd_avg_cell_voltage_pos = 2516 set osd_disarmed_pos = 2411 set osd_flip_arrow_pos = 65 set osd_core_temp_pos = 248 set osd_log_status_pos = 97 set osd_gps_sats_show_hdop = OFF profile 0 # profile 0 set dyn_lpf_dterm_min_hz = 105 set dyn_lpf_dterm_max_hz = 255 set dterm_lowpass2_hz = 225 set d_pitch = 32 set d_roll = 30 set d_min_roll = 0 set d_min_pitch = 0 profile 1 profile 2 # restore original profile selection profile 0 rateprofile 0 rateprofile 1 rateprofile 2 rateprofile 3 rateprofile 4 rateprofile 5 # restore original rateprofile selection rateprofile 2 # save configuration save Tuned CLI diff \u00b6 This CLI diff is updated as my tuning is evolving. I started off UAV Tech's freestyle presets which I'm modestly trying to improve over time. # # Building AutoComplete Cache ... Done! # # diff all # version # Betaflight / STM32F7X2 (S7X2) 4.2.6 Jan 5 2021 / 19:08:42 (a4b6db1e7) MSP API: 1.43 # config: manufacturer_id: TMTR, board_name: TMOTORF7, version: e02dd6f2, date: 2020-11-04T11:31:26Z # start the command batch batch start # reset configuration to default settings defaults nosave board_name TMOTORF7 manufacturer_id TMTR mcu_id 002f00263338510639393832 signature # resources resource MOTOR 1 B01 resource MOTOR 2 B00 resource MOTOR 3 C07 resource MOTOR 4 C06 # feature feature -RX_PARALLEL_PWM feature RX_SERIAL feature TELEMETRY # serial serial 1 1 115200 57600 0 115200 serial 4 64 115200 57600 0 115200 # aux aux 0 0 0 1800 2100 0 0 aux 1 1 2 1300 1700 0 0 aux 2 13 1 1300 2100 0 0 aux 3 35 2 1800 2100 0 0 # rxfail rxfail 7 s 750 # master set gyro_lowpass2_hz = 375 set dyn_notch_width_percent = 0 set dyn_notch_q = 250 set dyn_notch_max_hz = 350 set dyn_lpf_gyro_min_hz = 300 set dyn_lpf_gyro_max_hz = 750 set acc_calibration = 26,3,238,1 set min_check = 1000 set rssi_channel = 8 set serialrx_provider = CRSF set sbus_baud_fast = ON set dshot_bidir = ON set motor_pwm_protocol = DSHOT600 set small_angle = 180 set osd_warn_rssi = ON set osd_rssi_alarm = 40 set osd_vbat_pos = 257 set osd_rssi_pos = 2486 set osd_rssi_dbm_pos = 161 set osd_tim_1_pos = 353 set osd_tim_2_pos = 321 set osd_flymode_pos = 2241 set osd_throttle_pos = 313 set osd_vtx_channel_pos = 193 set osd_craft_name_pos = 33 set osd_gps_speed_pos = 161 set osd_gps_lon_pos = 33 set osd_gps_lat_pos = 1 set osd_gps_sats_pos = 65 set osd_home_dir_pos = 227 set osd_home_dist_pos = 97 set osd_flight_dist_pos = 184 set osd_altitude_pos = 129 set osd_warnings_pos = 2441 set osd_avg_cell_voltage_pos = 2516 set osd_disarmed_pos = 2411 set osd_flip_arrow_pos = 65 set osd_core_temp_pos = 248 set osd_log_status_pos = 97 set osd_gps_sats_show_hdop = OFF set gyro_rpm_notch_harmonics = 2 profile 0 # profile 0 set dyn_lpf_dterm_min_hz = 105 set dyn_lpf_dterm_max_hz = 255 set dterm_lowpass2_hz = 225 set pidsum_limit = 1000 set pidsum_limit_yaw = 1000 set p_pitch = 69 set d_pitch = 44 set f_pitch = 171 set p_roll = 63 set d_roll = 40 set f_roll = 162 set p_yaw = 68 set f_yaw = 162 set d_min_roll = 0 set d_min_pitch = 0 profile 1 profile 2 # restore original profile selection profile 0 rateprofile 0 rateprofile 1 rateprofile 2 # rateprofile 2 set rates_type = ACTUAL set roll_rc_rate = 1 set pitch_rc_rate = 1 set yaw_rc_rate = 1 set roll_expo = 54 set pitch_expo = 54 set yaw_expo = 54 set roll_srate = 100 set pitch_srate = 100 set tpa_breakpoint = 1750 rateprofile 3 rateprofile 4 rateprofile 5 # restore original rateprofile selection rateprofile 2 # save configuration save","title":"Apex 5\" HD on base frame kit"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#apex-5-hd-on-base-frame-kit","text":"","title":"Apex 5\" HD on base frame kit"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#a-digital-hd-build-on-the-apex-base-frame-kit","text":"For my first DIY build ever I wanted a strong 5\" frame. I chose the ImpulseRC Apex Base Frame Kit because I did not want to rely on a HD specific frame and loose the space made for the DJI Air Unit. It was quite a challenge to get everything to fit properly but I love the result and I hope you'll appreciate the density and great balance of this build. This ends up as a compact (but not too much) build without sacrifying the space for the electronics. It is clean (to my standards at least) and very powerful. I enjoyed working on it very much and I hope you'll get a sense of it reading this build guide. Enjoy!","title":"A digital HD build on the Apex Base Frame Kit"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#hardware-list","text":"Note Those links are NOT affiliate links. I just share the links where I actually bought the stuff needed for this build. ImpulseRC Apex Base Frame Kit T-Motor Combo F7 HD FC + F55A PRO II TBS Motor Steele Ethix Stout V3 TBS Tracer Nano RX Caddx Vista HD System (see improvement notes below) Apex HD CAMERA SIDE PLATE KIT (see improvement notes below) 20cm Coaxial Cable for Caddx Vista (20cm !) Vifly Finder Mini - Buzzer (lightweight) XT60 14AWG 10cm cable (longer) Apex DJI antenna TPU holder (will fit tracer immortal-t) HQProp ETHIX P3 Peanut Butter & Jelly 5.1x3x3 - PC (2x CW + 2xCCW) TrueRC Singularity U.FL Lite 112mm 5.8GHz - LHCP Strap Lipo KEVLAR 240x16mm - DFR","title":"Hardware list"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#notes-to-self-for-future-improvements","text":"Switch to the Caddx Nebula Pro Vista Kit so we do not need the Apex HD CAMERA SIDE PLATE KIT any more, save 3 grams and money?","title":"Notes to self for future improvements"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#build-steps","text":"","title":"Build steps"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#frame-assembly","text":"Follow the official ImpulseRC Apex guide to assemble the frame.","title":"Frame assembly"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#caddx-vista-on-the-20x20-rear-mounting-holes","text":"Note You will need 4 long M2 screws that are not provided with the kit. Nuts are the ones provided with the kit. The 20cm cable for Caddx Vista is better protected than the standard one.","title":"Caddx Vista on the 20x20 rear mounting holes"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#caddx-vista-vtx-antenna-mounting","text":"Use the Apex DJI antenna TPU holder on the rear and make sure to zip tie the U.FL antenna to the standoff. The Tracer Immortal T antennas fit well: one is zip tied in the lower plate by the provided clamp and the other is sticked in the vertical section of the TPU.","title":"Caddx Vista VTX antenna mounting"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#esc-and-xt60-mounting","text":"In my first version I cut short the XT60 leads but it held the XT60 too close to the motors. Instead I'm now using a 10cm XT60 cable zip tied to the front standoff. Warning Be mindful of the fact that the ESC mounted in reverse position to leverage the space at the front for the capacitor. You will need to reallocate the motor resources on the CLI (see the betaflight section below).","title":"ESC and XT60 mounting"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#rx-wiring","text":"Nothing surprising here, wiring the TBS Tracer Nano RX is straightforward!","title":"RX wiring"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#fc-mounting","text":"Use the longest of the provided FC-to-ESC cables (it connects perfectly, mind the wire direction) and connect the Caddx Vista.","title":"FC mounting"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#rx-placement","text":"Once protected, the RX is stuck on top of the Caddx Vista using double sided tape.","title":"RX placement"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#buzzer","text":"I chose the Vifly finder mini to get a lightweight yet powerful buzzer that fits perfectly at the front.","title":"Buzzer"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#finishing-the-build","text":"Finish the build by placing the top plate and the plastic provided covers on the arms.","title":"Finishing the build"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#betaflight-configuration","text":"Betaflight target: TMTR/TMOTORF7(STM32F7X2) Ports: Bi-directional DShot is enabled and supported natively by the BLHeli32 ESC: Filter settings to accomodate the bi-directional DShot: Modes: Joshua Bardwell's OSD for DJI FPV Google settings:","title":"Betaflight configuration"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#motor-resource-reallocation","text":"Since the ESC is mounted in reverse position, the motors need to be reassigned on the CLI. Depending on your wiring, IDs may change. # resources resource MOTOR 1 B01 resource MOTOR 2 B00 resource MOTOR 3 C07 resource MOTOR 4 C06 save","title":"Motor resource reallocation"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#minimal-cli-diff","text":"This is the minimal CLI diff that I initially set up without advanced tuning. To get my current tune, see the next section. # # Building AutoComplete Cache ... Done! # # diff all # version # Betaflight / STM32F7X2 (S7X2) 4.2.6 Jan 5 2021 / 19:08:42 (a4b6db1e7) MSP API: 1.43 # config: manufacturer_id: TMTR, board_name: TMOTORF7, version: e02dd6f2, date: 2020-11-04T11:31:26Z # start the command batch batch start # reset configuration to default settings defaults nosave board_name TMOTORF7 manufacturer_id TMTR mcu_id 002f00263338510639393832 signature # resources resource MOTOR 1 B01 resource MOTOR 2 B00 resource MOTOR 3 C07 resource MOTOR 4 C06 # feature feature -RX_PARALLEL_PWM feature RX_SERIAL feature TELEMETRY # beacon beacon RX_LOST beacon RX_SET # serial serial 1 1 115200 57600 0 115200 serial 4 64 115200 57600 0 115200 # aux aux 0 0 0 1800 2100 0 0 aux 1 1 2 1300 1700 0 0 aux 2 13 1 1300 2100 0 0 aux 3 35 2 1800 2100 0 0 # rxfail rxfail 7 s 750 # master set gyro_lowpass2_hz = 375 set dyn_notch_width_percent = 0 set dyn_notch_q = 250 set dyn_notch_min_hz = 90 set dyn_notch_max_hz = 350 set dyn_lpf_gyro_min_hz = 300 set dyn_lpf_gyro_max_hz = 750 set acc_calibration = 26,3,238,1 set min_check = 1000 set rssi_channel = 8 set serialrx_provider = CRSF set dshot_bidir = ON set motor_pwm_protocol = DSHOT600 set small_angle = 180 set osd_warn_rssi = ON set osd_rssi_alarm = 40 set osd_vbat_pos = 257 set osd_rssi_pos = 2486 set osd_rssi_dbm_pos = 161 set osd_tim_1_pos = 353 set osd_tim_2_pos = 321 set osd_flymode_pos = 2241 set osd_throttle_pos = 313 set osd_vtx_channel_pos = 193 set osd_craft_name_pos = 33 set osd_gps_speed_pos = 161 set osd_gps_lon_pos = 33 set osd_gps_lat_pos = 1 set osd_gps_sats_pos = 65 set osd_home_dir_pos = 2275 set osd_home_dist_pos = 2145 set osd_flight_dist_pos = 184 set osd_altitude_pos = 2177 set osd_warnings_pos = 2441 set osd_avg_cell_voltage_pos = 2516 set osd_disarmed_pos = 2411 set osd_flip_arrow_pos = 65 set osd_core_temp_pos = 248 set osd_log_status_pos = 97 set osd_gps_sats_show_hdop = OFF profile 0 # profile 0 set dyn_lpf_dterm_min_hz = 105 set dyn_lpf_dterm_max_hz = 255 set dterm_lowpass2_hz = 225 set d_pitch = 32 set d_roll = 30 set d_min_roll = 0 set d_min_pitch = 0 profile 1 profile 2 # restore original profile selection profile 0 rateprofile 0 rateprofile 1 rateprofile 2 rateprofile 3 rateprofile 4 rateprofile 5 # restore original rateprofile selection rateprofile 2 # save configuration save","title":"Minimal CLI diff"},{"location":"FPV%20Builds/ImpulseRC%20Apex/#tuned-cli-diff","text":"This CLI diff is updated as my tuning is evolving. I started off UAV Tech's freestyle presets which I'm modestly trying to improve over time. # # Building AutoComplete Cache ... Done! # # diff all # version # Betaflight / STM32F7X2 (S7X2) 4.2.6 Jan 5 2021 / 19:08:42 (a4b6db1e7) MSP API: 1.43 # config: manufacturer_id: TMTR, board_name: TMOTORF7, version: e02dd6f2, date: 2020-11-04T11:31:26Z # start the command batch batch start # reset configuration to default settings defaults nosave board_name TMOTORF7 manufacturer_id TMTR mcu_id 002f00263338510639393832 signature # resources resource MOTOR 1 B01 resource MOTOR 2 B00 resource MOTOR 3 C07 resource MOTOR 4 C06 # feature feature -RX_PARALLEL_PWM feature RX_SERIAL feature TELEMETRY # serial serial 1 1 115200 57600 0 115200 serial 4 64 115200 57600 0 115200 # aux aux 0 0 0 1800 2100 0 0 aux 1 1 2 1300 1700 0 0 aux 2 13 1 1300 2100 0 0 aux 3 35 2 1800 2100 0 0 # rxfail rxfail 7 s 750 # master set gyro_lowpass2_hz = 375 set dyn_notch_width_percent = 0 set dyn_notch_q = 250 set dyn_notch_max_hz = 350 set dyn_lpf_gyro_min_hz = 300 set dyn_lpf_gyro_max_hz = 750 set acc_calibration = 26,3,238,1 set min_check = 1000 set rssi_channel = 8 set serialrx_provider = CRSF set sbus_baud_fast = ON set dshot_bidir = ON set motor_pwm_protocol = DSHOT600 set small_angle = 180 set osd_warn_rssi = ON set osd_rssi_alarm = 40 set osd_vbat_pos = 257 set osd_rssi_pos = 2486 set osd_rssi_dbm_pos = 161 set osd_tim_1_pos = 353 set osd_tim_2_pos = 321 set osd_flymode_pos = 2241 set osd_throttle_pos = 313 set osd_vtx_channel_pos = 193 set osd_craft_name_pos = 33 set osd_gps_speed_pos = 161 set osd_gps_lon_pos = 33 set osd_gps_lat_pos = 1 set osd_gps_sats_pos = 65 set osd_home_dir_pos = 227 set osd_home_dist_pos = 97 set osd_flight_dist_pos = 184 set osd_altitude_pos = 129 set osd_warnings_pos = 2441 set osd_avg_cell_voltage_pos = 2516 set osd_disarmed_pos = 2411 set osd_flip_arrow_pos = 65 set osd_core_temp_pos = 248 set osd_log_status_pos = 97 set osd_gps_sats_show_hdop = OFF set gyro_rpm_notch_harmonics = 2 profile 0 # profile 0 set dyn_lpf_dterm_min_hz = 105 set dyn_lpf_dterm_max_hz = 255 set dterm_lowpass2_hz = 225 set pidsum_limit = 1000 set pidsum_limit_yaw = 1000 set p_pitch = 69 set d_pitch = 44 set f_pitch = 171 set p_roll = 63 set d_roll = 40 set f_roll = 162 set p_yaw = 68 set f_yaw = 162 set d_min_roll = 0 set d_min_pitch = 0 profile 1 profile 2 # restore original profile selection profile 0 rateprofile 0 rateprofile 1 rateprofile 2 # rateprofile 2 set rates_type = ACTUAL set roll_rc_rate = 1 set pitch_rc_rate = 1 set yaw_rc_rate = 1 set roll_expo = 54 set pitch_expo = 54 set yaw_expo = 54 set roll_srate = 100 set pitch_srate = 100 set tpa_breakpoint = 1750 rateprofile 3 rateprofile 4 rateprofile 5 # restore original rateprofile selection rateprofile 2 # save configuration save","title":"Tuned CLI diff"},{"location":"FPV%20Handbook/","text":"Introduction \u00b6 Hello fellow FPV pilot! This handbook serves as a way for me to keep and reference all the links and assets that have been helpful to me in this hobby. I share it publicly in case it could help someone in the community! This is an Open Source project so feel free to contribute by opening a discussion in a Github issue or proposing a change using a Pull Request. Happy flying!","title":"Introduction"},{"location":"FPV%20Handbook/#introduction","text":"Hello fellow FPV pilot! This handbook serves as a way for me to keep and reference all the links and assets that have been helpful to me in this hobby. I share it publicly in case it could help someone in the community! This is an Open Source project so feel free to contribute by opening a discussion in a Github issue or proposing a change using a Pull Request. Happy flying!","title":"Introduction"},{"location":"FPV%20Handbook/Batteries/","text":"Batteries notes \u00b6 Danger LiPo batteries are dangerous and can catch fire when they are damaged or badly maintained. What is written here is meant as a quick reminder and does not replace a fully comprehensive guide on battery management. Calculating charging amperage \u00b6 The safest amperage to charge a battery pack is 1C that we calculate by taking its mAh rating divided by 1000. Quick examples of 1C calculation: 1500 mAh battery = 1500 / 1000 = 1.5A charge 850 mAh battery = 850 / 1000 = 0.8A or 0.9A charge If you're in a hurry, you can charge at 2C which is basically twice the amperage you calculated above but do not do it too often. Parallel charging reminder \u00b6 Warning All batteries charged in parallel should have the same cell count ! Preferably parallel charge batteries of the same brand and model. Batteries cells voltage should not diverge from more than 0.1V per cell. This means that the sum of the cell voltage difference should not exceed 0.1 x (cell count) . Plug in the XT60 plug first on the parallel charging board , then the balance lead. After you plug your batteries in the parallel charging board, allow a few minutes for them to settle as some current will naturally distribute over them. The calculation of charging amperage should be multiplied by the number of batteries being charged in parallel but should not exceed 1C per battery.","title":"Batteries notes"},{"location":"FPV%20Handbook/Batteries/#batteries-notes","text":"Danger LiPo batteries are dangerous and can catch fire when they are damaged or badly maintained. What is written here is meant as a quick reminder and does not replace a fully comprehensive guide on battery management.","title":"Batteries notes"},{"location":"FPV%20Handbook/Batteries/#calculating-charging-amperage","text":"The safest amperage to charge a battery pack is 1C that we calculate by taking its mAh rating divided by 1000. Quick examples of 1C calculation: 1500 mAh battery = 1500 / 1000 = 1.5A charge 850 mAh battery = 850 / 1000 = 0.8A or 0.9A charge If you're in a hurry, you can charge at 2C which is basically twice the amperage you calculated above but do not do it too often.","title":"Calculating charging amperage"},{"location":"FPV%20Handbook/Batteries/#parallel-charging-reminder","text":"Warning All batteries charged in parallel should have the same cell count ! Preferably parallel charge batteries of the same brand and model. Batteries cells voltage should not diverge from more than 0.1V per cell. This means that the sum of the cell voltage difference should not exceed 0.1 x (cell count) . Plug in the XT60 plug first on the parallel charging board , then the balance lead. After you plug your batteries in the parallel charging board, allow a few minutes for them to settle as some current will naturally distribute over them. The calculation of charging amperage should be multiplied by the number of batteries being charged in parallel but should not exceed 1C per battery.","title":"Parallel charging reminder"},{"location":"FPV%20Handbook/Batteries/lipo_performance_database/","text":"LiPo database \u00b6 Joshua Bardwell's Lipo Performance Database","title":"LiPo database"},{"location":"FPV%20Handbook/Batteries/lipo_performance_database/#lipo-database","text":"Joshua Bardwell's Lipo Performance Database","title":"LiPo database"},{"location":"FPV%20Handbook/Batteries/skyrc-q200/","text":"SkyRC Q200 Charger \u00b6 My version is the SK-100104. Website \u00b6 SkyRC Q200 Charger Website Manual \u00b6 SkyRC Q200 Charger Manual","title":"SkyRC Q200 Charger"},{"location":"FPV%20Handbook/Batteries/skyrc-q200/#skyrc-q200-charger","text":"My version is the SK-100104.","title":"SkyRC Q200 Charger"},{"location":"FPV%20Handbook/Batteries/skyrc-q200/#website","text":"SkyRC Q200 Charger Website","title":"Website"},{"location":"FPV%20Handbook/Batteries/skyrc-q200/#manual","text":"SkyRC Q200 Charger Manual","title":"Manual"},{"location":"FPV%20Handbook/Betaflight/osd-dji-fpv/","text":"DJI FPV Goggles OSD setup \u00b6 CLI dump credits to Joshua Bardwell: set osd_units = METRIC set osd_warn_arming_disable = ON set osd_warn_batt_not_full = ON set osd_warn_batt_warning = ON set osd_warn_batt_critical = ON set osd_warn_visual_beeper = ON set osd_warn_crash_flip = ON set osd_warn_esc_fail = ON set osd_warn_core_temp = ON set osd_warn_rc_smoothing = ON set osd_warn_fail_safe = ON set osd_warn_launch_control = ON set osd_warn_no_gps_rescue = ON set osd_warn_gps_rescue_disabled = ON set osd_warn_rssi = OFF set osd_warn_link_quality = OFF set osd_warn_over_cap = OFF set osd_rssi_alarm = 20 set osd_link_quality_alarm = 80 set osd_rssi_dbm_alarm = -60 set osd_cap_alarm = 2200 set osd_alt_alarm = 100 set osd_distance_alarm = 0 set osd_esc_temp_alarm = -128 set osd_esc_rpm_alarm = -1 set osd_esc_current_alarm = -1 set osd_core_temp_alarm = 70 set osd_ah_max_pit = 20 set osd_ah_max_rol = 40 set osd_ah_invert = OFF set osd_logo_on_arming = OFF set osd_logo_on_arming_duration = 5 set osd_tim1 = 2560 set osd_tim2 = 2561 set osd_vbat_pos = 257 set osd_rssi_pos = 2486 set osd_link_quality_pos = 234 set osd_rssi_dbm_pos = 161 set osd_tim_1_pos = 353 set osd_tim_2_pos = 321 set osd_remaining_time_estimate_pos = 234 set osd_flymode_pos = 2241 set osd_anti_gravity_pos = 234 set osd_g_force_pos = 234 set osd_throttle_pos = 313 set osd_vtx_channel_pos = 193 set osd_crosshairs_pos = 205 set osd_ah_sbar_pos = 206 set osd_ah_pos = 78 set osd_current_pos = 234 set osd_mah_drawn_pos = 234 set osd_motor_diag_pos = 234 set osd_craft_name_pos = 33 set osd_display_name_pos = 234 set osd_gps_speed_pos = 2209 set osd_gps_lon_pos = 2081 set osd_gps_lat_pos = 2049 set osd_gps_sats_pos = 2113 set osd_home_dir_pos = 2275 set osd_home_dist_pos = 2145 set osd_flight_dist_pos = 184 set osd_compass_bar_pos = 234 set osd_altitude_pos = 2177 set osd_pid_roll_pos = 234 set osd_pid_pitch_pos = 234 set osd_pid_yaw_pos = 234 set osd_debug_pos = 234 set osd_power_pos = 234 set osd_pidrate_profile_pos = 234 set osd_warnings_pos = 2441 set osd_avg_cell_voltage_pos = 2516 set osd_pit_ang_pos = 234 set osd_rol_ang_pos = 234 set osd_battery_usage_pos = 234 set osd_disarmed_pos = 2411 set osd_nheading_pos = 234 set osd_nvario_pos = 234 set osd_esc_tmp_pos = 234 set osd_esc_rpm_pos = 234 set osd_esc_rpm_freq_pos = 234 set osd_rtc_date_time_pos = 234 set osd_adjustment_range_pos = 234 set osd_flip_arrow_pos = 65 set osd_core_temp_pos = 248 set osd_log_status_pos = 97 set osd_stick_overlay_left_pos = 234 set osd_stick_overlay_right_pos = 234 set osd_stick_overlay_radio_mode = 2 set osd_rate_profile_name_pos = 234 set osd_pid_profile_name_pos = 234 set osd_profile_name_pos = 234 set osd_rcchannels_pos = 234 set osd_camera_frame_pos = 35 set osd_efficiency_pos = 234 set osd_stat_rtc_date_time = OFF set osd_stat_tim_1 = OFF set osd_stat_tim_2 = ON set osd_stat_max_spd = ON set osd_stat_max_dist = OFF set osd_stat_min_batt = ON set osd_stat_endbatt = OFF set osd_stat_battery = OFF set osd_stat_min_rssi = ON set osd_stat_max_curr = ON set osd_stat_used_mah = ON set osd_stat_max_alt = OFF set osd_stat_bbox = ON set osd_stat_bb_no = ON set osd_stat_max_g_force = OFF set osd_stat_max_esc_temp = OFF set osd_stat_max_esc_rpm = OFF set osd_stat_min_link_quality = OFF set osd_stat_flight_dist = OFF set osd_stat_max_fft = OFF set osd_stat_total_flights = OFF set osd_stat_total_time = OFF set osd_stat_total_dist = OFF set osd_stat_min_rssi_dbm = OFF set osd_profile = 1 set osd_profile_1_name = - set osd_profile_2_name = - set osd_profile_3_name = - set osd_gps_sats_show_hdop = OFF set osd_displayport_device = AUTO set osd_rcchannels = -1,-1,-1,-1 set osd_camera_frame_width = 24 set osd_camera_frame_height = 11","title":"DJI FPV Goggles OSD setup"},{"location":"FPV%20Handbook/Betaflight/osd-dji-fpv/#dji-fpv-goggles-osd-setup","text":"CLI dump credits to Joshua Bardwell: set osd_units = METRIC set osd_warn_arming_disable = ON set osd_warn_batt_not_full = ON set osd_warn_batt_warning = ON set osd_warn_batt_critical = ON set osd_warn_visual_beeper = ON set osd_warn_crash_flip = ON set osd_warn_esc_fail = ON set osd_warn_core_temp = ON set osd_warn_rc_smoothing = ON set osd_warn_fail_safe = ON set osd_warn_launch_control = ON set osd_warn_no_gps_rescue = ON set osd_warn_gps_rescue_disabled = ON set osd_warn_rssi = OFF set osd_warn_link_quality = OFF set osd_warn_over_cap = OFF set osd_rssi_alarm = 20 set osd_link_quality_alarm = 80 set osd_rssi_dbm_alarm = -60 set osd_cap_alarm = 2200 set osd_alt_alarm = 100 set osd_distance_alarm = 0 set osd_esc_temp_alarm = -128 set osd_esc_rpm_alarm = -1 set osd_esc_current_alarm = -1 set osd_core_temp_alarm = 70 set osd_ah_max_pit = 20 set osd_ah_max_rol = 40 set osd_ah_invert = OFF set osd_logo_on_arming = OFF set osd_logo_on_arming_duration = 5 set osd_tim1 = 2560 set osd_tim2 = 2561 set osd_vbat_pos = 257 set osd_rssi_pos = 2486 set osd_link_quality_pos = 234 set osd_rssi_dbm_pos = 161 set osd_tim_1_pos = 353 set osd_tim_2_pos = 321 set osd_remaining_time_estimate_pos = 234 set osd_flymode_pos = 2241 set osd_anti_gravity_pos = 234 set osd_g_force_pos = 234 set osd_throttle_pos = 313 set osd_vtx_channel_pos = 193 set osd_crosshairs_pos = 205 set osd_ah_sbar_pos = 206 set osd_ah_pos = 78 set osd_current_pos = 234 set osd_mah_drawn_pos = 234 set osd_motor_diag_pos = 234 set osd_craft_name_pos = 33 set osd_display_name_pos = 234 set osd_gps_speed_pos = 2209 set osd_gps_lon_pos = 2081 set osd_gps_lat_pos = 2049 set osd_gps_sats_pos = 2113 set osd_home_dir_pos = 2275 set osd_home_dist_pos = 2145 set osd_flight_dist_pos = 184 set osd_compass_bar_pos = 234 set osd_altitude_pos = 2177 set osd_pid_roll_pos = 234 set osd_pid_pitch_pos = 234 set osd_pid_yaw_pos = 234 set osd_debug_pos = 234 set osd_power_pos = 234 set osd_pidrate_profile_pos = 234 set osd_warnings_pos = 2441 set osd_avg_cell_voltage_pos = 2516 set osd_pit_ang_pos = 234 set osd_rol_ang_pos = 234 set osd_battery_usage_pos = 234 set osd_disarmed_pos = 2411 set osd_nheading_pos = 234 set osd_nvario_pos = 234 set osd_esc_tmp_pos = 234 set osd_esc_rpm_pos = 234 set osd_esc_rpm_freq_pos = 234 set osd_rtc_date_time_pos = 234 set osd_adjustment_range_pos = 234 set osd_flip_arrow_pos = 65 set osd_core_temp_pos = 248 set osd_log_status_pos = 97 set osd_stick_overlay_left_pos = 234 set osd_stick_overlay_right_pos = 234 set osd_stick_overlay_radio_mode = 2 set osd_rate_profile_name_pos = 234 set osd_pid_profile_name_pos = 234 set osd_profile_name_pos = 234 set osd_rcchannels_pos = 234 set osd_camera_frame_pos = 35 set osd_efficiency_pos = 234 set osd_stat_rtc_date_time = OFF set osd_stat_tim_1 = OFF set osd_stat_tim_2 = ON set osd_stat_max_spd = ON set osd_stat_max_dist = OFF set osd_stat_min_batt = ON set osd_stat_endbatt = OFF set osd_stat_battery = OFF set osd_stat_min_rssi = ON set osd_stat_max_curr = ON set osd_stat_used_mah = ON set osd_stat_max_alt = OFF set osd_stat_bbox = ON set osd_stat_bb_no = ON set osd_stat_max_g_force = OFF set osd_stat_max_esc_temp = OFF set osd_stat_max_esc_rpm = OFF set osd_stat_min_link_quality = OFF set osd_stat_flight_dist = OFF set osd_stat_max_fft = OFF set osd_stat_total_flights = OFF set osd_stat_total_time = OFF set osd_stat_total_dist = OFF set osd_stat_min_rssi_dbm = OFF set osd_profile = 1 set osd_profile_1_name = - set osd_profile_2_name = - set osd_profile_3_name = - set osd_gps_sats_show_hdop = OFF set osd_displayport_device = AUTO set osd_rcchannels = -1,-1,-1,-1 set osd_camera_frame_width = 24 set osd_camera_frame_height = 11","title":"DJI FPV Goggles OSD setup"},{"location":"FPV%20Handbook/Betaflight/uav-tech-presets/","text":"UAV Tech presets \u00b6 Link to the UAV Tech presets","title":"UAV Tech presets"},{"location":"FPV%20Handbook/Betaflight/uav-tech-presets/#uav-tech-presets","text":"Link to the UAV Tech presets","title":"UAV Tech presets"},{"location":"FPV%20Handbook/Build%20Tips/antennas/","text":"Antenna mounting tips \u00b6 I found Joshua Bardwell's tips on antenna mounting quite useful:","title":"Antenna mounting tips"},{"location":"FPV%20Handbook/Build%20Tips/antennas/#antenna-mounting-tips","text":"I found Joshua Bardwell's tips on antenna mounting quite useful:","title":"Antenna mounting tips"},{"location":"FPV%20Handbook/FC%20%26%20ESC/blheli32/","text":"BLHeliSuite32 \u00b6 BLHeliSuite32 download Google Drive","title":"BLHeliSuite32"},{"location":"FPV%20Handbook/FC%20%26%20ESC/blheli32/#blhelisuite32","text":"BLHeliSuite32 download Google Drive","title":"BLHeliSuite32"},{"location":"FPV%20Handbook/FC%20%26%20ESC/mamba-f405-mini-3/","text":"Mamba F405+F35 Mini Mk3 DJI \u00b6 Website \u00b6 Diatone Mamba F405+F35 Mini Mk3 DJI Website General wiring \u00b6","title":"Mamba F405+F35 Mini Mk3 DJI"},{"location":"FPV%20Handbook/FC%20%26%20ESC/mamba-f405-mini-3/#mamba-f405f35-mini-mk3-dji","text":"","title":"Mamba F405+F35 Mini Mk3 DJI"},{"location":"FPV%20Handbook/FC%20%26%20ESC/mamba-f405-mini-3/#website","text":"Diatone Mamba F405+F35 Mini Mk3 DJI Website","title":"Website"},{"location":"FPV%20Handbook/FC%20%26%20ESC/mamba-f405-mini-3/#general-wiring","text":"","title":"General wiring"},{"location":"FPV%20Handbook/FC%20%26%20ESC/t-motor-f7/","text":"T-Motor F7+F55A PROII HD \u00b6 Website \u00b6 T-Motor F7+F55A PROII HD Website General wiring \u00b6","title":"T-Motor F7+F55A PROII HD"},{"location":"FPV%20Handbook/FC%20%26%20ESC/t-motor-f7/#t-motor-f7f55a-proii-hd","text":"","title":"T-Motor F7+F55A PROII HD"},{"location":"FPV%20Handbook/FC%20%26%20ESC/t-motor-f7/#website","text":"T-Motor F7+F55A PROII HD Website","title":"Website"},{"location":"FPV%20Handbook/FC%20%26%20ESC/t-motor-f7/#general-wiring","text":"","title":"General wiring"},{"location":"FPV%20Handbook/Frames/","text":"ImpulseRC Apex \u00b6 ImpulseRC Apex Build Guide","title":"ImpulseRC Apex"},{"location":"FPV%20Handbook/Frames/#impulserc-apex","text":"ImpulseRC Apex Build Guide","title":"ImpulseRC Apex"},{"location":"FPV%20Handbook/Legal%20documents/","text":"DGAC \u00b6 These are links to the French and Europeans regulation guides (in French). Liens utiles vers les guides de r\u00e9glementation drones. Guide de l'a\u00e9romod\u00e9lisme Guide Cat\u00e9gorie Ouverte : drones op\u00e9rants avec un faible risque (principalement de loisirs) Guide Cat\u00e9gorie Sp\u00e9cifique","title":"DGAC"},{"location":"FPV%20Handbook/Legal%20documents/#dgac","text":"These are links to the French and Europeans regulation guides (in French). Liens utiles vers les guides de r\u00e9glementation drones. Guide de l'a\u00e9romod\u00e9lisme Guide Cat\u00e9gorie Ouverte : drones op\u00e9rants avec un faible risque (principalement de loisirs) Guide Cat\u00e9gorie Sp\u00e9cifique","title":"DGAC"},{"location":"FPV%20Handbook/Prints/","text":"3D printing useful list \u00b6 Crossfire Nano TPU holder Mini Immortal T holder Vertical Tracer Antenna Mount DJI FPV goggles cable lock","title":"3D printing useful list"},{"location":"FPV%20Handbook/Prints/#3d-printing-useful-list","text":"Crossfire Nano TPU holder Mini Immortal T holder Vertical Tracer Antenna Mount DJI FPV goggles cable lock","title":"3D printing useful list"},{"location":"FPV%20Handbook/RX/","text":"TBS Tracer \u00b6 Manual \u00b6 TBS Tracer Manual Joshua Bardwell's complete setup guide: Nano receiver \u00b6 Nano receiver general wiring \u00b6 TBS Cloud XF WiFi firmware update \u00b6 Updating the TBS Cloud WiFi firmware is done by using the TBS Agent X software and the TBS Cloud WiFi: You need to be logged into TBS Agent X Download the TBS Cloud activation ZIP file Unzip the file, you should get something like: \u251c\u2500\u2500 V1.15 \u2502 \u251c\u2500\u2500 CROSSFIRE \u2502 \u2502 \u2514\u2500\u2500 firmware.bin \u2502 \u251c\u2500\u2500 FUSION \u2502 \u2502 \u2514\u2500\u2500 firmware.bin \u2502 \u2514\u2500\u2500 TANGO2 \u2502 \u2514\u2500\u2500 firmware.bin \u251c\u2500\u2500 V1.17 \u2502 \u251c\u2500\u2500 CROSSFIRE \u2502 \u2502 \u2514\u2500\u2500 firmware.bin \u2502 \u251c\u2500\u2500 FUSION \u2502 \u2502 \u2514\u2500\u2500 firmware.bin \u2502 \u2514\u2500\u2500 TANGO2 \u2502 \u2514\u2500\u2500 firmware.bin Connect your TBS RX module using USB Make sure the Cloud XF WiFi module is green On your PC, look for and connect to a SSID named tbs_crossfire_xxxx Once connected to your TBS XF WiFi, go to http://192.168.4.1 Select Upgrade on the XF WiFi interface, choose the latest CROSSFIRE/firmware.bin Push the Upgrade button and wait for the upgrade to be completed. Note After the upgrade is done, your module will reboot but the version displayed in TBS Agent X will remain the same. To refresh it, just unplug/replug your RX module.","title":"TBS Tracer"},{"location":"FPV%20Handbook/RX/#tbs-tracer","text":"","title":"TBS Tracer"},{"location":"FPV%20Handbook/RX/#manual","text":"TBS Tracer Manual Joshua Bardwell's complete setup guide:","title":"Manual"},{"location":"FPV%20Handbook/RX/#nano-receiver","text":"","title":"Nano receiver"},{"location":"FPV%20Handbook/RX/#nano-receiver-general-wiring","text":"","title":"Nano receiver general wiring"},{"location":"FPV%20Handbook/RX/#tbs-cloud-xf-wifi-firmware-update","text":"Updating the TBS Cloud WiFi firmware is done by using the TBS Agent X software and the TBS Cloud WiFi: You need to be logged into TBS Agent X Download the TBS Cloud activation ZIP file Unzip the file, you should get something like: \u251c\u2500\u2500 V1.15 \u2502 \u251c\u2500\u2500 CROSSFIRE \u2502 \u2502 \u2514\u2500\u2500 firmware.bin \u2502 \u251c\u2500\u2500 FUSION \u2502 \u2502 \u2514\u2500\u2500 firmware.bin \u2502 \u2514\u2500\u2500 TANGO2 \u2502 \u2514\u2500\u2500 firmware.bin \u251c\u2500\u2500 V1.17 \u2502 \u251c\u2500\u2500 CROSSFIRE \u2502 \u2502 \u2514\u2500\u2500 firmware.bin \u2502 \u251c\u2500\u2500 FUSION \u2502 \u2502 \u2514\u2500\u2500 firmware.bin \u2502 \u2514\u2500\u2500 TANGO2 \u2502 \u2514\u2500\u2500 firmware.bin Connect your TBS RX module using USB Make sure the Cloud XF WiFi module is green On your PC, look for and connect to a SSID named tbs_crossfire_xxxx Once connected to your TBS XF WiFi, go to http://192.168.4.1 Select Upgrade on the XF WiFi interface, choose the latest CROSSFIRE/firmware.bin Push the Upgrade button and wait for the upgrade to be completed. Note After the upgrade is done, your module will reboot but the version displayed in TBS Agent X will remain the same. To refresh it, just unplug/replug your RX module.","title":"TBS Cloud XF WiFi firmware update"},{"location":"FPV%20Handbook/TX/tbs_agent_lite/","text":"TBS Agent Lite \u00b6 The TBS Agent Lite is a modern replacement for the default Crossfire LUA scripts. TBS Agent Lite download link","title":"TBS Agent Lite"},{"location":"FPV%20Handbook/TX/tbs_agent_lite/#tbs-agent-lite","text":"The TBS Agent Lite is a modern replacement for the default Crossfire LUA scripts. TBS Agent Lite download link","title":"TBS Agent Lite"},{"location":"FPV%20Handbook/TX/tx16s/","text":"TX16S \u00b6 User manual \u00b6 Radiomaster TX16S English User Guide Radiomaster TX16S French User Guide Initial setup \u00b6 The initial setup is important especially the battery and gimbals calibration parts. Oscar Liang's Radiomaster TX16S setup","title":"TX16S"},{"location":"FPV%20Handbook/TX/tx16s/#tx16s","text":"","title":"TX16S"},{"location":"FPV%20Handbook/TX/tx16s/#user-manual","text":"Radiomaster TX16S English User Guide Radiomaster TX16S French User Guide","title":"User manual"},{"location":"FPV%20Handbook/TX/tx16s/#initial-setup","text":"The initial setup is important especially the battery and gimbals calibration parts. Oscar Liang's Radiomaster TX16S setup","title":"Initial setup"},{"location":"FPV%20Handbook/VTX/","text":"Caddx Vista \u00b6 How to switch to FCC 700mW output \u00b6 Oscar Liang's guide on FCC mode How to switch to 1200mW output \u00b6 Oscar Liang's guide on 1200mW output SBUS Baud Fast low latency performance with DJI FPV \u00b6 On the DJI FPV goggles go to Settings > Device > Protocol and make sure to select Sbus Baud Fast On Betaflight CLI, type set sbus_baud_fast=ON","title":"Caddx Vista"},{"location":"FPV%20Handbook/VTX/#caddx-vista","text":"","title":"Caddx Vista"},{"location":"FPV%20Handbook/VTX/#how-to-switch-to-fcc-700mw-output","text":"Oscar Liang's guide on FCC mode","title":"How to switch to FCC 700mW output"},{"location":"FPV%20Handbook/VTX/#how-to-switch-to-1200mw-output","text":"Oscar Liang's guide on 1200mW output","title":"How to switch to 1200mW output"},{"location":"FPV%20Handbook/VTX/#sbus-baud-fast-low-latency-performance-with-dji-fpv","text":"On the DJI FPV goggles go to Settings > Device > Protocol and make sure to select Sbus Baud Fast On Betaflight CLI, type set sbus_baud_fast=ON","title":"SBUS Baud Fast low latency performance with DJI FPV"},{"location":"Tech%20Blog/2012/2012-01-11-portage-basics/","text":"Les distributions Linux disposent toutes de ce qu'on appelle un gestionnaire de paquet dit package management system ou plus simplement package manager . Un package manager est un ensemble de programmes et d'outils permettant l'automatisation de l'installation / mise \u00e0 jour / configuration / d\u00e9sinstallation de logiciels sur un syst\u00e8me. Sous Gentoo Linux, le gestionnaire de paquet s'appelle portage . Il permet de manipuler les packages disponible sur notre syst\u00e8me Gentoo. Un package repr\u00e9sente un logiciel disponible \u00e0 travers le package manager. Selon les distributions il peut prendre diff\u00e9rentes formes comme par exemple une archive compress\u00e9e. Portage \u00b6 Portage est \u00e9crit en python et en bash . C'est sans conteste l'un des package manager les plus flexibles et performants car il offre des possibilit\u00e9s de personnalisation tr\u00e8s fines des packages que l'on souhaite installer sur son syst\u00e8me. La liste des packages disponibles \u00e0 l'installation est organis\u00e9e dans une arborescence de dossiers, c'est ce qu'on appelle le portage tree . Le nom \"arbre portage\" fait r\u00e9f\u00e9rence \u00e0 l'arborescence organis\u00e9e par cat\u00e9gorie des packages. Cette arborescence est stock\u00e9e par d\u00e9faut dans le dossier /usr/portage/ dont voici un exemple : /usr/portage/www-apache /usr/portage/www-apps /usr/portage/www-client /usr/portage/www-misc Dans chaque cat\u00e9gorie, on retrouve un dossier par package disponible : /usr/portage/www-client/chromium /usr/portage/www-client/firefox /usr/portage/www-client/opera On voit que firefox et opera font partie de la cat\u00e9gorie www-client , leur nom complet de package sous Gentoo est : www-client/firefox www-client/opera Bien s\u00fbr, on pourra aussi les appeler par leur petit nom mais il est important de noter qu'il est possible que deux packages aient le m\u00eame nom s'ils font partie d'une arborescence diff\u00e9rente. Mieux vaut donc toujours les appeler par leur nom complet. Les commandes \u00b6 Toutes les commandes suivantes font partie de portage et permettent de le manipuler. La plus connue est sans aucun doute emerge. /usr/bin/ebuild /usr/bin/egencache /usr/bin/emerge /usr/bin/portageq /usr/bin/quickpkg /usr/bin/repoman /usr/sbin/archive-conf /usr/sbin/dispatch-conf /usr/sbin/emaint /usr/sbin/emerge-webrsync /usr/sbin/env-update /usr/sbin/etc-update /usr/sbin/fixpackages /usr/sbin/regenworld /usr/sbin/update-env /usr/sbin/update-etc Dans un prochain post, je parlerai de l'utilisation des principales commandes de portage et de leur configuration.","title":"Portage basics"},{"location":"Tech%20Blog/2012/2012-01-11-portage-basics/#portage","text":"Portage est \u00e9crit en python et en bash . C'est sans conteste l'un des package manager les plus flexibles et performants car il offre des possibilit\u00e9s de personnalisation tr\u00e8s fines des packages que l'on souhaite installer sur son syst\u00e8me. La liste des packages disponibles \u00e0 l'installation est organis\u00e9e dans une arborescence de dossiers, c'est ce qu'on appelle le portage tree . Le nom \"arbre portage\" fait r\u00e9f\u00e9rence \u00e0 l'arborescence organis\u00e9e par cat\u00e9gorie des packages. Cette arborescence est stock\u00e9e par d\u00e9faut dans le dossier /usr/portage/ dont voici un exemple : /usr/portage/www-apache /usr/portage/www-apps /usr/portage/www-client /usr/portage/www-misc Dans chaque cat\u00e9gorie, on retrouve un dossier par package disponible : /usr/portage/www-client/chromium /usr/portage/www-client/firefox /usr/portage/www-client/opera On voit que firefox et opera font partie de la cat\u00e9gorie www-client , leur nom complet de package sous Gentoo est : www-client/firefox www-client/opera Bien s\u00fbr, on pourra aussi les appeler par leur petit nom mais il est important de noter qu'il est possible que deux packages aient le m\u00eame nom s'ils font partie d'une arborescence diff\u00e9rente. Mieux vaut donc toujours les appeler par leur nom complet.","title":"Portage"},{"location":"Tech%20Blog/2012/2012-01-11-portage-basics/#les-commandes","text":"Toutes les commandes suivantes font partie de portage et permettent de le manipuler. La plus connue est sans aucun doute emerge. /usr/bin/ebuild /usr/bin/egencache /usr/bin/emerge /usr/bin/portageq /usr/bin/quickpkg /usr/bin/repoman /usr/sbin/archive-conf /usr/sbin/dispatch-conf /usr/sbin/emaint /usr/sbin/emerge-webrsync /usr/sbin/env-update /usr/sbin/etc-update /usr/sbin/fixpackages /usr/sbin/regenworld /usr/sbin/update-env /usr/sbin/update-etc Dans un prochain post, je parlerai de l'utilisation des principales commandes de portage et de leur configuration.","title":"Les commandes"},{"location":"Tech%20Blog/2012/2012-01-13-portage-internals/","text":"Maintenant que nous savons ce qu'est Portage , comprenons simplement comment il fonctionne. Que se passe-t'il lorsque l'on veut installer un package, et d'ailleurs \u00e7a ressemble \u00e0 quoi un package sous Gentoo ? Les ebuilds \u00b6 Les packages disponibles dans l'arbre portage sont repr\u00e9sent\u00e9s par des fichiers appel\u00e9s ebuilds . Les ebuilds contiennent toutes les informations n\u00e9cessaires \u00e0 la manipulation du package en question par portage (o\u00f9 t\u00e9l\u00e9charger les sources, quelle licence prot\u00e8ge le logiciel, quelle est l'URL du projet, etc...). Pour toute action vis \u00e0 vis d'un package , portage se base sur les informations des ebuilds correspondants. Je dis de s ebuild s car un ebuild contient aussi la version du package qu'il repr\u00e9sente. Il y a donc autant de fichiers ebuild que de versions disponibles d'un package. Prenons l'exemple du package www-client/firefox : $ ls /usr/portage/www-client/firefox firefox-3.6.20.ebuild firefox-3.6.22.ebuild firefox-8.0.ebuild firefox-9.0.ebuild Les versions 3.6.20, 3.6.22, 8.0 et 9.0 sont donc disponibles sur portage. Si nous voulions des informations suppl\u00e9mentaires ou installer une de ces versions de firefox, portage n'aurait qu'\u00e0 ex\u00e9cuter les instructions contenues dans le fichier ebuild correspondant, et voil\u00e0 ! Quand Mozilla sortira firefox 10, un d\u00e9veloppeur ou contributeur Gentoo devra cr\u00e9er l'ebuild pour cette version afin qu'il soit disponible dans portage, il est donc crucial de tenir sa liste d'ebuilds \u00e0 jour sur son syst\u00e8me. Synchroniser portage \u00b6 Mettre \u00e0 jour portage, c'est donc mettre \u00e0 jour la liste des ebuilds disponibles sur son syst\u00e8me ! # emerge --sync Le fameux sync t\u00e9l\u00e9charge les nouveaux ebuilds et supprime les obsol\u00e8tes pour nous, c'est gr\u00e2ce \u00e0 cela que nous disposerons du nouveau firefox quand il sortira, et il en va bien s\u00fbr de m\u00eame pour tous les packages. Les d\u00e9veloppeurs et contributeurs Gentoo tiennent ensemble \u00e0 jour un arbre portage commun qui est t\u00e9l\u00e9charg\u00e9 et r\u00e9pliqu\u00e9 par des serveurs qu'on appelle mirrors (le terme mirroir signifie qu'ils contiennent une copie exacte de l'arbre de d\u00e9veloppement). Tous les utilisateurs r\u00e9pliquent \u00e0 leur tour leur arbre portage local (par d\u00e9faut dans /usr/portage/ ) en se connectant sur un de ces serveurs mirrors lors du sync. A l'heure o\u00f9 j'\u00e9cris ces lignes, le portage tree contient 15459 packages repr\u00e9sentant 29931 ebuilds !","title":"Portage internals"},{"location":"Tech%20Blog/2012/2012-01-13-portage-internals/#les-ebuilds","text":"Les packages disponibles dans l'arbre portage sont repr\u00e9sent\u00e9s par des fichiers appel\u00e9s ebuilds . Les ebuilds contiennent toutes les informations n\u00e9cessaires \u00e0 la manipulation du package en question par portage (o\u00f9 t\u00e9l\u00e9charger les sources, quelle licence prot\u00e8ge le logiciel, quelle est l'URL du projet, etc...). Pour toute action vis \u00e0 vis d'un package , portage se base sur les informations des ebuilds correspondants. Je dis de s ebuild s car un ebuild contient aussi la version du package qu'il repr\u00e9sente. Il y a donc autant de fichiers ebuild que de versions disponibles d'un package. Prenons l'exemple du package www-client/firefox : $ ls /usr/portage/www-client/firefox firefox-3.6.20.ebuild firefox-3.6.22.ebuild firefox-8.0.ebuild firefox-9.0.ebuild Les versions 3.6.20, 3.6.22, 8.0 et 9.0 sont donc disponibles sur portage. Si nous voulions des informations suppl\u00e9mentaires ou installer une de ces versions de firefox, portage n'aurait qu'\u00e0 ex\u00e9cuter les instructions contenues dans le fichier ebuild correspondant, et voil\u00e0 ! Quand Mozilla sortira firefox 10, un d\u00e9veloppeur ou contributeur Gentoo devra cr\u00e9er l'ebuild pour cette version afin qu'il soit disponible dans portage, il est donc crucial de tenir sa liste d'ebuilds \u00e0 jour sur son syst\u00e8me.","title":"Les ebuilds"},{"location":"Tech%20Blog/2012/2012-01-13-portage-internals/#synchroniser-portage","text":"Mettre \u00e0 jour portage, c'est donc mettre \u00e0 jour la liste des ebuilds disponibles sur son syst\u00e8me ! # emerge --sync Le fameux sync t\u00e9l\u00e9charge les nouveaux ebuilds et supprime les obsol\u00e8tes pour nous, c'est gr\u00e2ce \u00e0 cela que nous disposerons du nouveau firefox quand il sortira, et il en va bien s\u00fbr de m\u00eame pour tous les packages. Les d\u00e9veloppeurs et contributeurs Gentoo tiennent ensemble \u00e0 jour un arbre portage commun qui est t\u00e9l\u00e9charg\u00e9 et r\u00e9pliqu\u00e9 par des serveurs qu'on appelle mirrors (le terme mirroir signifie qu'ils contiennent une copie exacte de l'arbre de d\u00e9veloppement). Tous les utilisateurs r\u00e9pliquent \u00e0 leur tour leur arbre portage local (par d\u00e9faut dans /usr/portage/ ) en se connectant sur un de ces serveurs mirrors lors du sync. A l'heure o\u00f9 j'\u00e9cris ces lignes, le portage tree contient 15459 packages repr\u00e9sentant 29931 ebuilds !","title":"Synchroniser portage"},{"location":"Tech%20Blog/2012/2012-02-27-nginx-conditional-uwsgi-error-handling/","text":"To ensure the best possible quality of service we want to make sure that we catch our uWSGI application failures on the nginx side and react accordingly. Our goal is to never serve a HTTP 500 error to visitors. I'll show you how you can adapt nginx error handling behavior based on the URI called by the visitor . nginx + uWSGI base configuration (nginx.conf) \u00b6 Suppose we have the following configuration to handle our uWSGI apps. We have set our gateway timeouts to 10 seconds to make sure no request will take more than that time to be answered, no matter what our application do. upstream uwsgi_app1 { server 127.0.0.1:1000; } location / { uwsgi_pass uwsgi_app1; include uwsgi_params; uwsgi_ignore_client_abort on; uwsgi_connect_timeout 10; uwsgi_read_timeout 10; uwsgi_send_timeout 10; } Static uWSGI error handling \u00b6 Now we don't want nginx to reply to clients with errors such as 500 (our app crashed) and 504 (timeout has been triggered). At first, we'll serve a simple 1x1 GIF pixel instead. location = /px.gif { empty_gif; } upstream uwsgid { server 127.0.0.1:1000; } location / { uwsgi_pass uwsgi_app1; include uwsgi_params; uwsgi_ignore_client_abort on; uwsgi_connect_timeout 10; uwsgi_read_timeout 10; uwsgi_send_timeout 10; uwsgi\\_intercept\\_errors on; error\\_page 500 504 /px.gif; } The uwsgi_intercept_errors directive tells nginx to handle errors from uWSGI. Then we just have to use the usual nginx error handling using the error_page directive which in our case calls for /px.gif, returning our 1x1 GIF pixel using the empty_gif nginx module. Dynamic uWSGI error handling \u00b6 Let's go conditional, suppose we have two types or URLS : http://www.mysite.com/APP1?query=bar http://www.mysite.com/APP1?query=foo& redir=http://www.ultrabug.fr &word=bar For URL #1, we want to serve the 1x1 pixel whereas for URL #2, when we receive the redir parameter, we want to redirect the visitor to exactly that URI. It's standard error_page handling remember ? Let's use the named location feature to process the request. location @uwsgi_errors { rewrite_log on; if ($arg_redir ~* (.+)) { set $redir $1; rewrite ^ $redir? redirect; } rewrite ^ /px.gif? redirect; } location / { uwsgi_pass uwsgi_app1; include uwsgi_params; uwsgi_ignore_client_abort on; uwsgi_connect_timeout 10; uwsgi_read_timeout 10; uwsgi_send_timeout 10; uwsgi\\_intercept\\_errors on; error\\_page 500 504 @uwsgi\\_errors; } Upon HTTP 500/504 error, the @uwsgi_errors location is called by nginx internals. Let's detail its processing : rewrite_log on : turn the rewriting logging on for debugging / monitoring reasons $arg_redir ~* (.+) : $arg_PARAMETER is a neat way to get the value of the given GET parameter ( redir in our case). The condition here means that if the parameter is present, we'll use it and enter the condition. rewrite ^ $redir? redirect : we call the rewrite module using the redirect method to send a HTTP 302 to the client with the value of the previously defined $redir variable which contains the URI of the redir parameter. The important part here is the question mark after the $redir variable which makes sure that the original URI parameters are stripped from the redirection URI. rewrite ^ /px.gif? redirect : if no redir parameter was received, we redirect to the the px.gif as usual. The question mark has the same meaning as above. That's it, we managed to handle our uWSGI errors based on certain conditions. Of course we could go further and use more named locations for different types of HTTP errors and use more nginx variables and conditions but that's up to you now !","title":"nginx : conditional uWSGI error handling"},{"location":"Tech%20Blog/2012/2012-02-27-nginx-conditional-uwsgi-error-handling/#nginx-uwsgi-base-configuration-nginxconf","text":"Suppose we have the following configuration to handle our uWSGI apps. We have set our gateway timeouts to 10 seconds to make sure no request will take more than that time to be answered, no matter what our application do. upstream uwsgi_app1 { server 127.0.0.1:1000; } location / { uwsgi_pass uwsgi_app1; include uwsgi_params; uwsgi_ignore_client_abort on; uwsgi_connect_timeout 10; uwsgi_read_timeout 10; uwsgi_send_timeout 10; }","title":"nginx + uWSGI base configuration (nginx.conf)"},{"location":"Tech%20Blog/2012/2012-02-27-nginx-conditional-uwsgi-error-handling/#static-uwsgi-error-handling","text":"Now we don't want nginx to reply to clients with errors such as 500 (our app crashed) and 504 (timeout has been triggered). At first, we'll serve a simple 1x1 GIF pixel instead. location = /px.gif { empty_gif; } upstream uwsgid { server 127.0.0.1:1000; } location / { uwsgi_pass uwsgi_app1; include uwsgi_params; uwsgi_ignore_client_abort on; uwsgi_connect_timeout 10; uwsgi_read_timeout 10; uwsgi_send_timeout 10; uwsgi\\_intercept\\_errors on; error\\_page 500 504 /px.gif; } The uwsgi_intercept_errors directive tells nginx to handle errors from uWSGI. Then we just have to use the usual nginx error handling using the error_page directive which in our case calls for /px.gif, returning our 1x1 GIF pixel using the empty_gif nginx module.","title":"Static uWSGI error handling"},{"location":"Tech%20Blog/2012/2012-02-27-nginx-conditional-uwsgi-error-handling/#dynamic-uwsgi-error-handling","text":"Let's go conditional, suppose we have two types or URLS : http://www.mysite.com/APP1?query=bar http://www.mysite.com/APP1?query=foo& redir=http://www.ultrabug.fr &word=bar For URL #1, we want to serve the 1x1 pixel whereas for URL #2, when we receive the redir parameter, we want to redirect the visitor to exactly that URI. It's standard error_page handling remember ? Let's use the named location feature to process the request. location @uwsgi_errors { rewrite_log on; if ($arg_redir ~* (.+)) { set $redir $1; rewrite ^ $redir? redirect; } rewrite ^ /px.gif? redirect; } location / { uwsgi_pass uwsgi_app1; include uwsgi_params; uwsgi_ignore_client_abort on; uwsgi_connect_timeout 10; uwsgi_read_timeout 10; uwsgi_send_timeout 10; uwsgi\\_intercept\\_errors on; error\\_page 500 504 @uwsgi\\_errors; } Upon HTTP 500/504 error, the @uwsgi_errors location is called by nginx internals. Let's detail its processing : rewrite_log on : turn the rewriting logging on for debugging / monitoring reasons $arg_redir ~* (.+) : $arg_PARAMETER is a neat way to get the value of the given GET parameter ( redir in our case). The condition here means that if the parameter is present, we'll use it and enter the condition. rewrite ^ $redir? redirect : we call the rewrite module using the redirect method to send a HTTP 302 to the client with the value of the previously defined $redir variable which contains the URI of the redir parameter. The important part here is the question mark after the $redir variable which makes sure that the original URI parameters are stripped from the redirection URI. rewrite ^ /px.gif? redirect : if no redir parameter was received, we redirect to the the px.gif as usual. The question mark has the same meaning as above. That's it, we managed to handle our uWSGI errors based on certain conditions. Of course we could go further and use more named locations for different types of HTTP errors and use more nginx variables and conditions but that's up to you now !","title":"Dynamic uWSGI error handling"},{"location":"Tech%20Blog/2012/2012-03-19-uwsgi-network-spooling-of-messages-between-applications/","text":"One of the great new uWSGI v1.1 features is network spooling of messages between applications. This short article demonstrates how to use it between a front end django app and a back end python app. Download the fully packaged demo I advise you to use a uWSGI emperor and simply drop the provided ini files in its folder. The example is simple enough but here is an explanation of how it works. The sender is a django app which you call via your browser, the front end. The sender app uses the mashal module which permits to pass a type rich message (dictionary) through a string only spooling mechanism (yes, it's very handy). The sender sends a type 17 message (spool request message) over the network providing the message. The receiver app is a standalone spooling application written in standard python, this would be the back end. The receiver just prints out what it received via the network spooling mechanism. As I said, this is just an illustration of what can be done. You could look into uwsgidecorators and mix this with other stuff that suits your needs. Enjoy !","title":"uWSGI : network spooling of messages between applications"},{"location":"Tech%20Blog/2012/2012-05-11-mongodb-v2-0-5-released/","text":"This is a bug fix release of mongoDB, it is now live in portage as well. +*mongodb-2.0.5 (11 May 2012) + + 11 May 2012; Ultrabug ultrabug@gentoo.org -mongodb-2.0.3.ebuild, + -files/mongodb-2.0.3-fix-scons.patch, +mongodb-2.0.5.ebuild: + Version bump, generic mms-agent URL, drop old. + Bug fix highlight : \u00b6 Inconsistent query results on large data and result sets Race during static destruction of CommitJob object See the complete changelog .","title":"mongoDB : v2.0.5 released"},{"location":"Tech%20Blog/2012/2012-05-11-mongodb-v2-0-5-released/#bug-fix-highlight","text":"Inconsistent query results on large data and result sets Race during static destruction of CommitJob object See the complete changelog .","title":"Bug fix highlight :"},{"location":"Tech%20Blog/2012/2012-05-11-pymongo-v2-2-released/","text":"The mongoDB python driver pymongo was bumped to v2.2 and is now in portage. Changelog highlights : \u00b6 Support for Python 3 Support for Gevent Improved connection pooling See the complete changelog .","title":"pymongo : v2.2 released"},{"location":"Tech%20Blog/2012/2012-05-11-pymongo-v2-2-released/#changelog-highlights","text":"Support for Python 3 Support for Gevent Improved connection pooling See the complete changelog .","title":"Changelog highlights :"},{"location":"Tech%20Blog/2012/2012-05-15-uwsgi-new-ebuild-in-portage/","text":"I started to rework the uwsgi ebuild on March 7th because I was not satisfied with the one available in portage. The current version was out of date and the package itself was not really suited for production deployment. Luckily my fellow Gentoo Linux developer Tiziano M\u00fcller (dev-zero) was also in the same kind of process for his own needs so we teamed up to achieve this goal. Our main focuses were : Bring the emperor mode support Ease and clarify the overall configuration Code a more versatile init script and conf.d file Add a better support of the available plugins and python versions Support PHP I'm glad to announce that our reworked ebuild is now available in portage for all users, we hope that it will come handy to everyone who needs it. Thanks again Tiziano, it's always a pleasure to work with you !","title":"uWSGI : new ebuild in portage"},{"location":"Tech%20Blog/2012/2012-05-16-clustering-corosync-v1-4-3-pacemaker-v1-1-7-released/","text":"I've finally taken the time to take care of the corosync and pacemaker ebuilds. The new versions are now available in portage. Corosync 1.4.3 (10/04/2012) \u00b6 This is one of the last supported old stable release of the Corosync Cluster Engine. FYI, I've also bumped the new corosync-2.0.0 version but it needs more testing before I hard-unmask it. Pacemaker 1.1.7 (28/03/12) \u00b6 This is a bug fix release of Pacemaker. See the changelog for details. Special thanks to my fellow Gentoo Linux developer Kacper Kowalik (xarthisius) for his help on these bumps.","title":"Clustering : corosync v1.4.3 & pacemaker v1.1.7 released"},{"location":"Tech%20Blog/2012/2012-05-16-clustering-corosync-v1-4-3-pacemaker-v1-1-7-released/#corosync-143-10042012","text":"This is one of the last supported old stable release of the Corosync Cluster Engine. FYI, I've also bumped the new corosync-2.0.0 version but it needs more testing before I hard-unmask it.","title":"Corosync 1.4.3 (10/04/2012)"},{"location":"Tech%20Blog/2012/2012-05-16-clustering-corosync-v1-4-3-pacemaker-v1-1-7-released/#pacemaker-117-280312","text":"This is a bug fix release of Pacemaker. See the changelog for details. Special thanks to my fellow Gentoo Linux developer Kacper Kowalik (xarthisius) for his help on these bumps.","title":"Pacemaker 1.1.7 (28/03/12)"},{"location":"Tech%20Blog/2012/2012-05-25-state-of-the-event-log-architecture-enhancements/","text":"Interesting stuff is happening on the event log (syslog) community and more precisely on the topic of syslog format extension and structuring syslog data . As of today there's no real standard on how to format and structure data on a syslog message. Every project has its own log message structure and syntax (qmail and postfix don't log a mail delivery failure the same way for example), so we rely on parsers to extract any given data from a log message because the syslog software has no way to do it for us. I for one have coded a postfix log parser and believe me it's not a pleasant thing to do and maintain ! The main idea about structuring syslog messages is to represent them using JSON along with the current free form strings to prevent backward compatibility breakage. To achieve this, we need to normalize and extend this format so that syslog softwares such as rsyslog and syslog-ng can directly understand them. That's where CEE-enhanced messages and Lumberjack kick in. CEE-enhanced messages \u00b6 The CEE project aims at defining a syntax which extends the current log message format while being compatible with all the currently and widely used log frameworks or the well known glibc's syslog() call. To achieve this the main idea is to use what is called a cookie before the JSON representation of the data we want to pass to the syslog software. To make it simple, let's pretend we see this postfix log meaning that a queued mail has been removed from the queue (I removed the date etc to only focus on the message part) : CAA3B607DA: removed The equivalent CEE-enhanced message could (this would be up to postfix) be represented as : @cee: {\"id\":\"CAA3B607DA\", \"removed\":\"true\"} @cee: is what is called the cookie which tells the syslog software that this message is using the CEE-enhanced syntax I guess you already see how handy this would be and how we could then rely on the syslog software to automagically use our favorite storage backend to store this structured data (think mongoDB). More information on the handy and quick video presentation by Rainer Gerhards and his article about it . The Lumberjack project \u00b6 So now how do we format the JSON part ? Could we have other types such as booleans and integers directly interpreted by the syslog software ? Well this needs definitions and standardization proposals, that's what project Lumberjack is for. Have a nice read on Lumberjack origins on Rainer Gerhards's blog .","title":"State of the event log architecture enhancements"},{"location":"Tech%20Blog/2012/2012-05-25-state-of-the-event-log-architecture-enhancements/#cee-enhanced-messages","text":"The CEE project aims at defining a syntax which extends the current log message format while being compatible with all the currently and widely used log frameworks or the well known glibc's syslog() call. To achieve this the main idea is to use what is called a cookie before the JSON representation of the data we want to pass to the syslog software. To make it simple, let's pretend we see this postfix log meaning that a queued mail has been removed from the queue (I removed the date etc to only focus on the message part) : CAA3B607DA: removed The equivalent CEE-enhanced message could (this would be up to postfix) be represented as : @cee: {\"id\":\"CAA3B607DA\", \"removed\":\"true\"} @cee: is what is called the cookie which tells the syslog software that this message is using the CEE-enhanced syntax I guess you already see how handy this would be and how we could then rely on the syslog software to automagically use our favorite storage backend to store this structured data (think mongoDB). More information on the handy and quick video presentation by Rainer Gerhards and his article about it .","title":"CEE-enhanced messages"},{"location":"Tech%20Blog/2012/2012-05-25-state-of-the-event-log-architecture-enhancements/#the-lumberjack-project","text":"So now how do we format the JSON part ? Could we have other types such as booleans and integers directly interpreted by the syslog software ? Well this needs definitions and standardization proposals, that's what project Lumberjack is for. Have a nice read on Lumberjack origins on Rainer Gerhards's blog .","title":"The Lumberjack project"},{"location":"Tech%20Blog/2012/2012-06-01-rsyslog-new-v6-branch-in-portage/","text":"The first ebuild of the v6.2 stable branch of rsyslog is finally available in portage. This branch provides functional and performance enhancements of rsyslog. Quick highlights : \u00b6 Hadoop (HDFS) support has been considerably speeded up by supporting batched insert mode. TCP transmission overhead for TLS has been dramatically improved. TCP supports input worker thread pools. Support of log normalization via liblognorm rule bases. This permits very high performance normalization of semantically equal messages from different devices (and thus in different syntaxes). Interesting upcoming features such as mongoDB support and the enhanced config language are on the way with v6.4 . Stay tuned !","title":"rsyslog : new v6 branch in portage"},{"location":"Tech%20Blog/2012/2012-06-01-rsyslog-new-v6-branch-in-portage/#quick-highlights","text":"Hadoop (HDFS) support has been considerably speeded up by supporting batched insert mode. TCP transmission overhead for TLS has been dramatically improved. TCP supports input worker thread pools. Support of log normalization via liblognorm rule bases. This permits very high performance normalization of semantically equal messages from different devices (and thus in different syntaxes). Interesting upcoming features such as mongoDB support and the enhanced config language are on the way with v6.4 . Stay tuned !","title":"Quick highlights :"},{"location":"Tech%20Blog/2012/2012-06-06-mongodb-v2-0-6-release/","text":"Bug fix release, it is now available in portage. Starting from this package version I introduced a logrotate script which compresses daily the mongodb logs and keeps them for a year. Release highlights : mongos does not send reads to secondaries after replica restart when using keyFiles If only slaveDelay'd nodes are available, use them OplogReader has no socket timeout See the the complete changelog .","title":"mongoDB : v2.0.6 released"},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/","text":"I needed to export a set of data from a mongoDB collection based on their objectIDs' (_id) timestamp using mongoexport . The mongoexport documentation is everything but helpful on the subject so I had to find a workaround to answer this simple question : \"export all documents inserted yesterday on this collection in a CSV format\" . Relevant mongoexport options \u00b6 --host : specify the mongoDB host --username / --pasword : if you're using authentication on your server -d : database to use -c : collection to use --fields : fields you want to export (omit for all) --query : the actual query selecting the result set you want to export --csv : export in a CSV format The date range query workaround So the hard part is to actually ask mongoexport to only return the documents in the desired time frame using an objectID compliant query . I overcame this problem using a simple but efficient python script generating the query for me. !/usr/bin/python \u00b6 using pymongo-2.2 \u00b6 from bson.objectid import ObjectId import datetime now = datetime.datetime.now() yesterday = now - datetime.timedelta(days=1) start_date = datetime.datetime(yesterday.year, yesterday.month, yesterday.day, 0, 0, 0) end_date = datetime.datetime(now.year, now.month, now.day, 0, 0, 0) oid_start = ObjectId.from_datetime(start_date) oid_stop = ObjectId.from_datetime(end_date) print '{ \"_id\" : { \"$gte\" : { \"$oid\": \"%s\" }, \"$lt\" : { \"$oid\": \"%s\" } } }' % ( str(oid_start), str(oid_stop) ) This script just prints out a command line compliant representation of the objectIDs for yesterday and today. So this query will select exactly what I wanted : all yesterday's objectIDs. Example : { \"_id\" : { \"$gte\" : { \"$oid\": \"4fd535000000000000000000\" } , \"$lt\" : { \"$oid\": \"4fd686800000000000000000\" } } } Using mongoexport \u00b6 We then can simply use mongoexport from the shell by issuing (I left the optional parameters out) : $ mongoexport -h localhost -d myDatabase -c theCollection --query \"$(python oid.py)\" --csv Et voil\u00e0 ! I guess there must be a cleaner way to do it out there, but I was unable to find it in my limited search time frame, so comment this post if you have a better solution please !","title":"mongoDB : export based on objectIDs' timestamp"},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/#relevant-mongoexport-options","text":"--host : specify the mongoDB host --username / --pasword : if you're using authentication on your server -d : database to use -c : collection to use --fields : fields you want to export (omit for all) --query : the actual query selecting the result set you want to export --csv : export in a CSV format The date range query workaround So the hard part is to actually ask mongoexport to only return the documents in the desired time frame using an objectID compliant query . I overcame this problem using a simple but efficient python script generating the query for me.","title":"Relevant mongoexport options"},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/#usrbinpython","text":"","title":"!/usr/bin/python"},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/#using-pymongo-22","text":"from bson.objectid import ObjectId import datetime now = datetime.datetime.now() yesterday = now - datetime.timedelta(days=1) start_date = datetime.datetime(yesterday.year, yesterday.month, yesterday.day, 0, 0, 0) end_date = datetime.datetime(now.year, now.month, now.day, 0, 0, 0) oid_start = ObjectId.from_datetime(start_date) oid_stop = ObjectId.from_datetime(end_date) print '{ \"_id\" : { \"$gte\" : { \"$oid\": \"%s\" }, \"$lt\" : { \"$oid\": \"%s\" } } }' % ( str(oid_start), str(oid_stop) ) This script just prints out a command line compliant representation of the objectIDs for yesterday and today. So this query will select exactly what I wanted : all yesterday's objectIDs. Example : { \"_id\" : { \"$gte\" : { \"$oid\": \"4fd535000000000000000000\" } , \"$lt\" : { \"$oid\": \"4fd686800000000000000000\" } } }","title":"using pymongo-2.2"},{"location":"Tech%20Blog/2012/2012-06-12-mongodb-export-based-on-objectids-timestamp/#using-mongoexport","text":"We then can simply use mongoexport from the shell by issuing (I left the optional parameters out) : $ mongoexport -h localhost -d myDatabase -c theCollection --query \"$(python oid.py)\" --csv Et voil\u00e0 ! I guess there must be a cleaner way to do it out there, but I was unable to find it in my limited search time frame, so comment this post if you have a better solution please !","title":"Using mongoexport"},{"location":"Tech%20Blog/2012/2012-06-15-rsyslog-v6-2-2-released/","text":"This is a bug fix release of rsyslog, it is now available in portage . Bug fix highlights : \u00b6 disk queue was not persisted on shutdown --enable-smcustbindcdr configure directive did not work (my fix, yay!) potential hang due to mutex deadlock \u201clast message repeated n times\u201d message was missing hostname See the complete changelog .","title":"rsyslog : v6.2.2 released"},{"location":"Tech%20Blog/2012/2012-06-15-rsyslog-v6-2-2-released/#bug-fix-highlights","text":"disk queue was not persisted on shutdown --enable-smcustbindcdr configure directive did not work (my fix, yay!) potential hang due to mutex deadlock \u201clast message repeated n times\u201d message was missing hostname See the complete changelog .","title":"Bug fix highlights :"},{"location":"Tech%20Blog/2012/2012-06-18-flask-pymongo-new-ebuild-in-portage/","text":"I just baked a new ebuild available now in portage : dev-python/flask-pymongo ! This is an extension for the Flask python microframework which simplifies the use of the mighty PyMongo for all your mongoDB usages. Enjoy !","title":"Flask-PyMongo : new ebuild in portage"},{"location":"Tech%20Blog/2012/2012-07-09-pymongo-v2-2-1-released/","text":"The mongoDB python driver pymongo was bumped to v2.2.1 in portage. Changelog highlights : \u00b6 fixes an incompatibility with mod_wsgi 2.x that could cause connections to leak See the complete changelog .","title":"pymongo : v2.2.1 released"},{"location":"Tech%20Blog/2012/2012-07-09-pymongo-v2-2-1-released/#changelog-highlights","text":"fixes an incompatibility with mod_wsgi 2.x that could cause connections to leak See the complete changelog .","title":"Changelog highlights :"},{"location":"Tech%20Blog/2012/2012-07-09-uwsgi-v1-2-4-released/","text":"This is a maintenance release of uWSGI whicih contains two new features and lot of bug fixes, it is now available in portage. Two of those fixes I was longing for :) Bug fix highlights : \u00b6 fixed python atexit usage in the spooler fixed a threading issue with uwsgi.send() fixed a leak in python uwsgi.workers() fixed spooler with chdir fixed async+threading fixed the spooler-max-tasks respawn allow gevent's greenlets to send jobs to the spooler See the complete changelog .","title":"uWSGI : v1.2.4 released"},{"location":"Tech%20Blog/2012/2012-07-09-uwsgi-v1-2-4-released/#bug-fix-highlights","text":"fixed python atexit usage in the spooler fixed a threading issue with uwsgi.send() fixed a leak in python uwsgi.workers() fixed spooler with chdir fixed async+threading fixed the spooler-max-tasks respawn allow gevent's greenlets to send jobs to the spooler See the complete changelog .","title":"Bug fix highlights :"},{"location":"Tech%20Blog/2012/2012-07-23-clustering-glue-v1-0-10-released/","text":"The newly released cluster glue libraries for Pacemaker / Heartbeat are available in portage. From my perspective, the major enhancement is that the clplumbing (the code responsible for passing along cib messages between nodes) now include compression. This was something I reported upstream a long time ago and I was handling with the large-cluster USE flag on the ebuild (I thus dropped it from IUSE). Highlights : \u00b6 Compression modules included and compression handling improved in clplumbing one memory leak fixed in clplumbing support for asynchronous I/O in sbd","title":"Clustering : glue v1.0.10 released"},{"location":"Tech%20Blog/2012/2012-07-23-clustering-glue-v1-0-10-released/#highlights","text":"Compression modules included and compression handling improved in clplumbing one memory leak fixed in clplumbing support for asynchronous I/O in sbd","title":"Highlights :"},{"location":"Tech%20Blog/2012/2012-08-03-httping/","text":"Quick post about a great tool I came across to test the response time of a web service : httping As its name suggests, it's like ping but for http requests ! Its options are vast enough to fit with any of your dreams so just try it out, it's a must-have. Sample usage : $ httping -g 'http://google.com' PING google.com:80 (http://google.com): connected to 74.125.230.195:80 (321 bytes), seq=0 time=17.70 ms connected to 74.125.230.197:80 (321 bytes), seq=1 time=13.97 ms connected to 74.125.230.199:80 (321 bytes), seq=2 time=20.49 ms PS: yes, you can emerge it.","title":"Httping"},{"location":"Tech%20Blog/2012/2012-08-07-rabbitmq-v2-8-5-released/","text":"As the new maintainer of net-misc/rabbitmq-server , I'm pleased to announce the availability of the bug fix release 2.8.5 of our beloved Rabbit Message Queuing server. A special care have been taken on HA queues, so read the Changelog for more details.","title":"rabbitMQ : v2.8.5 released"},{"location":"Tech%20Blog/2012/2012-08-14-uwsgi-v1-2-5-released/","text":"This is a maintenance release of uWSGI with one new feature supporting pam authentication mechanism. The ebuild has been bumped with a new pam USE flag. Bug fix highlights : \u00b6 allows ugreen with threads added the pam plugin (sponsored by PythonAnywhere.com) added --so-keepalive option to enable TCP keepalives on sockets See the complete changelog .","title":"uWSGI : v1.2.5 released"},{"location":"Tech%20Blog/2012/2012-08-14-uwsgi-v1-2-5-released/#bug-fix-highlights","text":"allows ugreen with threads added the pam plugin (sponsored by PythonAnywhere.com) added --so-keepalive option to enable TCP keepalives on sockets See the complete changelog .","title":"Bug fix highlights :"},{"location":"Tech%20Blog/2012/2012-08-20-mongodb-v2-0-7-released/","text":"This is yet a new bug fix release, nothing too fancy here in my pov. I've bumped it in portage along with v1.8.5 of the previous branch. Release highlights : option to turn off splitVectors on particular mongoses See the the complete changelog .","title":"mongoDB : v2.0.7 released"},{"location":"Tech%20Blog/2012/2012-08-27-les-vosges/","text":"Premier roadtrip en Harley : les Vosges. Nous les avons atteint apr\u00e8s un passage sur Nancy pour profiter des \u00e9v\u00e9nements organis\u00e9s pour rendre hommage au designer et architecte Jean Prouv\u00e9. Au programme : des paysages magnifiques, des routes qui sentent le sapin mais dans le vrai sens du terme, de l'urbex dans un manoir incroyable et du vert, beaucoup de vert. 5 jours, 1350 km parcourus et une t\u00eate de pare-brise \u00e0 l'arriv\u00e9e : on va recommencer, c'est s\u00fbr ! Disclamer : photos de mauvaise qualit\u00e9 prises avec le t\u00e9l\u00e9phone portable.","title":"Les Vosges"},{"location":"Tech%20Blog/2012/2012-08-30-the-mongodb-2-2-era/","text":"Yesterday MongoDB entered the version 2.2 era with all its long time awaited new features . For the strong MongoDB fans across the world, those features were known and expected because some of them were requested for quite long now. They often were tested and debugged with 10gen before being released also, that's the way it goes. In mongoParis 2012, I took the chance to discuss a few matters with a 10gen engineer about some of those new features and he predicted that the release would take place in September. He was not far from the truth and I'm glad that for once a release is launched before what I was expecting (even if mongo jira was even more optimistic). My main points in this discussions and his answers were : Will they take the chance to drop the spidermonkey 1.7 requirement and finally move their code to use v1.8 ? No, they'd rather focus on switching over to the v8 javascript engine, even if it's a long time goal. They're still not satisfied with the current v8 stability regarding locks so it's not suited for production yet. Will the new aggregation framework be capable of handling \"join\" like requests ? No, that's not what it was designed for yet. What about the output of the new aggregation framework, can we write it into a collection or is it RAM only (thus limiting the result set size) ? The result set is RAM only so we have the same limitations as a distinct command : 16MB. We plan on being able to store the output on a collection or something later. I must admit, for my planned use cases the last answer was a head shot on the aggregation framework but I still think it's a remarkable achievement and I can only say 10gen did a good job and thank them for releasing it. mongodb-2.2.0 \u00b6 My thoughts on this new release are clouded with both my intensive administrator point of view (our main DB is over 2,5 billions documents large) and my packager point of view (Gentoo Linux). I'll start with my packager pov, which is bad unfortunately : I am annoyed by their stubbornness with sticking to spidermonkey-1.7. Hell, it's not like the community didn't do the work for them already : Fedora do provide a working spidermonkey-1.8 patch for mongoDB. I may reconsider using it for the mongodb-2.2.0 series if they update their patch again. I am not happy with them not being strict in their compile-from-source procedure. Scons sucks but everything is arguable, what is not is that they don't seem to make some real efforts and testing on proper compilation. I feel like they neglect the people caring for the sources and not some pre-baked binary and it's not good imo because this is also a great field for optimizing your software. They gave me a headache debugging the 2.2.x release to make it compile fine. I won't go into too much details (I'll fill bugs for them) but hell, they say they provide a scons option --use-system-all which they don't even honor by hard sourcing spidermonkey-1.7 libraries from their own sources ! They now use boost-1.49, I won't blame them here. Now come the good news, at last, from my user perspective : No strong upgrade plan needed. Just upgrade the clients (mongos) first then roll on the servers, perfect. The concurrency improvements are just so awesome on paper : locks are now per database, not for the whole mongod process. This is good news if you have multiple databases, which I don't, but they also implemented a new subsystem which avoids locks on most page faulting operations so even I can benefit from this. The aggregation framework will simplify some queries we could only achieve using mapReduce and offer interesting possibilities for calculations such as statistics. TTL collections : those I have been waiting since mongoParis 2011. I have tons of ideas and use cases. mongotop and mongostat now support authentication while mongodump can read from secondaries. We can now log to syslog instead of fixed log files using the --syslog command line argument to mongod. Lower migration thresholds ensure a better distribution of data for small collections on clusters. See the full changelog for all the details. pymongo-2.3 \u00b6 All drivers also had to adapt to be able to offer those new features and benefit from them. Apart from supporting the aggregation framework, I will highlight this change as it affects me potentially : Users of authentication must upgrade to PyMongo 2.3 (or newer) for \u201csafe\u201d write operations to function correctly. Better be aware and safe than sorry mates, remember to update your drivers and read the changelog . Those versions are already available in portage, you can stop reading and go compiling now.","title":"The mongoDB 2.2 era"},{"location":"Tech%20Blog/2012/2012-08-30-the-mongodb-2-2-era/#mongodb-220","text":"My thoughts on this new release are clouded with both my intensive administrator point of view (our main DB is over 2,5 billions documents large) and my packager point of view (Gentoo Linux). I'll start with my packager pov, which is bad unfortunately : I am annoyed by their stubbornness with sticking to spidermonkey-1.7. Hell, it's not like the community didn't do the work for them already : Fedora do provide a working spidermonkey-1.8 patch for mongoDB. I may reconsider using it for the mongodb-2.2.0 series if they update their patch again. I am not happy with them not being strict in their compile-from-source procedure. Scons sucks but everything is arguable, what is not is that they don't seem to make some real efforts and testing on proper compilation. I feel like they neglect the people caring for the sources and not some pre-baked binary and it's not good imo because this is also a great field for optimizing your software. They gave me a headache debugging the 2.2.x release to make it compile fine. I won't go into too much details (I'll fill bugs for them) but hell, they say they provide a scons option --use-system-all which they don't even honor by hard sourcing spidermonkey-1.7 libraries from their own sources ! They now use boost-1.49, I won't blame them here. Now come the good news, at last, from my user perspective : No strong upgrade plan needed. Just upgrade the clients (mongos) first then roll on the servers, perfect. The concurrency improvements are just so awesome on paper : locks are now per database, not for the whole mongod process. This is good news if you have multiple databases, which I don't, but they also implemented a new subsystem which avoids locks on most page faulting operations so even I can benefit from this. The aggregation framework will simplify some queries we could only achieve using mapReduce and offer interesting possibilities for calculations such as statistics. TTL collections : those I have been waiting since mongoParis 2011. I have tons of ideas and use cases. mongotop and mongostat now support authentication while mongodump can read from secondaries. We can now log to syslog instead of fixed log files using the --syslog command line argument to mongod. Lower migration thresholds ensure a better distribution of data for small collections on clusters. See the full changelog for all the details.","title":"mongodb-2.2.0"},{"location":"Tech%20Blog/2012/2012-08-30-the-mongodb-2-2-era/#pymongo-23","text":"All drivers also had to adapt to be able to offer those new features and benefit from them. Apart from supporting the aggregation framework, I will highlight this change as it affects me potentially : Users of authentication must upgrade to PyMongo 2.3 (or newer) for \u201csafe\u201d write operations to function correctly. Better be aware and safe than sorry mates, remember to update your drivers and read the changelog . Those versions are already available in portage, you can stop reading and go compiling now.","title":"pymongo-2.3"},{"location":"Tech%20Blog/2012/2012-09-03-rabbitmq-2-8-6-released/","text":"I was in a bug killing spree today and I'm pleased to announce that I closed the 5 open bugs open for net-misc/rabbitmq-server while bumping this package to its new v2.8.6 version. +*rabbitmq-server-2.8.6 (03 Sep 2012) + + 03 Sep 2012; Ultrabug ultrabug@gentoo.org -rabbitmq-server-2.8.1-r1.ebuild, + rabbitmq-server-2.8.4.ebuild, rabbitmq-server-2.8.5.ebuild, + +rabbitmq-server-2.8.6.ebuild, files/rabbitmq.service, + files/rabbitmq-script-wrapper: + Drop old. Add GPL-2 LICENSE fix #426092. Enhanced systemd service file fix + #419531 and init script fix #416345 thx to Maksim Melnikau. Fix #430510 VCS + fetching in compilation. Fix #430508 parallel building. Version bump. + Bug fix version mates, the changelog is here for more details as always.","title":"rabbitMQ 2.8.6 released"},{"location":"Tech%20Blog/2012/2012-09-11-gentoo-make-conf-et-make-profile-demenagent/","text":"Bien que cette nouvelle soit diffus\u00e9e par le syst\u00e8me de news interne \u00e0 Portage, je sais d'exp\u00e9rience qu'elles ne sont pas tr\u00e8s lues. Sachez simplement que prochainement, les fichiers make.conf et make.profile seront pr\u00e9sents dans les stages d'installation dans /etc/portage et non plus dans /etc comme actuellement. Il est important de noter que ce changement ne concerne que les nouvelles installations et n'a aucun impact sur les syst\u00e8mes actuels, les deux r\u00e9pertoires sont donc support\u00e9s mais ne vous \u00e9tonnez pas si vous ne les trouvez pas \u00e0 leur place habituelle lors de vos prochaines installs. Pour ceux qui utilisent des scripts automatis\u00e9s d'installation ou des d\u00e9mons comme Puppet ou Chef, pensez \u00e0 modifier vos configurations ! La news en question, en anglais : Title make.conf and make.profile move Author Jorge Manuel B. S. Vicetto jmbsvicetto@gentoo.org Posted 2012-09-09 Revision 1 Starting next week, new stages will have make.conf and make.profile moved from /etc to /etc/portage. This is a change in the installation defaults, that will only affect new installs so it doesn't affect current systems. Current users don't need to do anything. But if you want to follow the preferred location, you may want to take the chance to move the files in your system(s) to the new location.","title":"Gentoo : make.conf et make.profile d\u00e9m\u00e9nagent"},{"location":"Tech%20Blog/2012/2012-10-29-uwsgi-v1-2-6-and-v1-3-released/","text":"The v1.2.6 is a maintenance release with some interesting backports while the v1.3 series start with a couple of new interesting features and plugins such as a mongodb logger. On my async python usage side, I'm glad to see this new version coming for its enhanced --lazy-apps option and gevent grace reloading. v1.2.6 highlights : \u00b6 fixed idle mode on busy workers backported subscription round robin weight handling from 1.3 See the complete changelog . v1.3 highlights : \u00b6 New plugin : router_http (compiled-in by default) New feature : --if-env (and --if-opt) can compare values http/https non-blocking writes Busyness cheaper algorithm by \u0141ukasz Mierzwa MongoDB integration for logging","title":"uWSGI : v1.2.6 and v1.3 released"},{"location":"Tech%20Blog/2012/2012-10-29-uwsgi-v1-2-6-and-v1-3-released/#v126-highlights","text":"fixed idle mode on busy workers backported subscription round robin weight handling from 1.3 See the complete changelog .","title":"v1.2.6 highlights :"},{"location":"Tech%20Blog/2012/2012-10-29-uwsgi-v1-2-6-and-v1-3-released/#v13-highlights","text":"New plugin : router_http (compiled-in by default) New feature : --if-env (and --if-opt) can compare values http/https non-blocking writes Busyness cheaper algorithm by \u0141ukasz Mierzwa MongoDB integration for logging","title":"v1.3 highlights :"},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/","text":"We're having a pretty hot debate in my company about the core development language we would like to embrace in order to enhance our work flow and unlock both our innovation and development iteration. I thought of doing a blog post instead of writing an email summarizing my point of view about why we should choose python in case it could help other people to make their own mind or at least understand mine. I don't want to be dragged into the typical \"language x VS language y\" type of post as there are a lot of those already but instead focus on specific use cases. Anyway, to be totally transparent with you, dear reader, I have to tell you that the main \" opponent \" we're considering is Javascript / node.js. Python is easy to learn and write \u00b6 I work for an online marketing/advertising company and we have four different IT/development teams with specific work to do. That means that we all have our own constraints to take into account and we use different technologies / languages to achieve them. Opening a debate on a development language rationalization thus means that people will have to be able to learn the one we will choose, the quicker the better. Well let's be honest, python is not widely taught on IT schools so our guys definitely WILL have to learn it. But python is one of the most simple language to learn as you can quickly test and make progress. This mainly comes from its coding syntax which makes the code cleaner and easy to read, also the language's keywords are simple to remember, no boring brackets or semicolons to drive you crazy. Python is about coding clean and instinctively. A simple and straightforward syntax is very important to me because : it is easier to read it is easier to maintain your code is lighter you spend less time coding so you spend more time thinking Python everywhere \u00b6 When choosing your core language, you must make sure you can rely on it today and tomorrow for a wide range of use cases . I find it very interesting to have a look at a list of its application domains as it does really show that with this language at our core, we would be able to achieve anything. The versatility of python is my main argument in favor of its adoption. This is a mature and proven language with a lot of libraries and frameworks to suit all our present and future needs. It is widely supported on an extensive set of platforms and I can't think of an open-source project not supporting python right from the start. At this point, I will quote the folks at AppNexus from their conference in the recent PyData NYC 2012 : \"Python's versatility allows us to use it both for offline analytical tasks as well as production system development. Doing so allows us to bridge the gap between prototypes and production by relying on the same code libraries and frameworks for both, thereby tightening our innovation loop\". They say python helped them grow very rapidly and efficiently by permitting them to focus on innovation, needless to say that I share their point of view and would love to see this happening in my company. Python use cases in my company \u00b6 Now let's talk about our present and projected use cases. This is not an exhaustive list as I want to keep it simple and demonstrate the versatility I just talked about. Scripting and automation \u00b6 I am a sysadmin and I am lazy. Nothing new here okay, but that's how I met python a bunch of years ago. At that time my company's infrastructure became more complex everyday as was growing very rapidly. New servers were arriving and provided new functionalities and new technologies which lead more and more to heterogeneity in the things we had to monitor, automate and configure. Python is a sysadmin's heaven with all its libraries capable of handling complex tasks easily, even in our cluster environments where you have to deal with parallel and high availability computing. This is a big relief to know that whatever the task you're asked to carry you can safely say : \"python can do it\". The keyword here is efficiency . Complete and complex applications \u00b6 A lot of modern and cross-platform applications are written in python or based on it. I for one wrote an email parsing software a few years ago and it's still kicking in production, its maintenance is easy and it has evolved smoothly with our growth and needs. Another thing I like about this language is that it's fast and can benefit from a semi-compiled \"byte-code\" which speeds up your application. No, python is not C++ and speed is not it's biggest advantage of course but it's really fast enough to compete with others easily. Let's sample some famous software written in python : BitTorrent , original client, along with several derivatives Dropbox , a web-based file hosting service OpenStack , a cloud computing IaaS platform Portage , the heart of Gentoo Linux Ubuntu Software Center, a graphical package manager Web applications \u00b6 Python also have some solid and very powerful librairies able to manage asynchronous, real-time and scalable web applications and services. We already do have some of those robust web apps running in production and python demonstrates everyday all of the strenghts I already talked about here. We use librairies such as gevent along with web frameworks like flask and message queuing with zeromq . Someday I may write a post about our python web stack, it may be interesting to share about it. I have been able to recode a web app written in .NET very quickly while enhancing it in every way possible. It is way faster, reliable, fault tolerant and maintainable that it was before. Thanks to python we have a short development iteration which proves itself everyday as the application grows and is capable to meet and achieve any new challenge we're asked to take care of. I'm convinced that no other language could have been so powerful and versatile than python to do so. We're not the only ones thinking and experiencing this of course, still in the list we can see : Google uses Python for many tasks including the backends of web apps such as Google Groups, Gmail, and Google Maps, as well as for some of its search-engine internals AppNexus uses Python for some of their web apps backend YouTube uses Python \"to produce maintainable features in record times, with a minimum of developers\" Yahoo! Groups uses Python \"to maintain its discussion groups\" That's some big players indeed and it's interesting to see they use python for their web app backends . Conclusion \u00b6 I am not a software developer as I never took strong development courses at school. I am a sysadmin of complex, clustered and heterogeneous environments so this affects my development standards and point of view in a way that my expectations will surely be different from a pure developer. My main concerns can be defined with words like proven, easy, clean, versatile, maintainable, fast (to code and execute), scalable, fault-tolerant and cross-platform . All of my choices have been based on those standards and concerns and I think they apply well in our debate because I chose Python to meet them all and I've never been disappointed or limited by it. I hope this post reflects my thoughts and helped you understand them. I will tell you about the result and the decision my company made on this debate.","title":"My views on Python"},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#python-is-easy-to-learn-and-write","text":"I work for an online marketing/advertising company and we have four different IT/development teams with specific work to do. That means that we all have our own constraints to take into account and we use different technologies / languages to achieve them. Opening a debate on a development language rationalization thus means that people will have to be able to learn the one we will choose, the quicker the better. Well let's be honest, python is not widely taught on IT schools so our guys definitely WILL have to learn it. But python is one of the most simple language to learn as you can quickly test and make progress. This mainly comes from its coding syntax which makes the code cleaner and easy to read, also the language's keywords are simple to remember, no boring brackets or semicolons to drive you crazy. Python is about coding clean and instinctively. A simple and straightforward syntax is very important to me because : it is easier to read it is easier to maintain your code is lighter you spend less time coding so you spend more time thinking","title":"Python is easy to learn and write"},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#python-everywhere","text":"When choosing your core language, you must make sure you can rely on it today and tomorrow for a wide range of use cases . I find it very interesting to have a look at a list of its application domains as it does really show that with this language at our core, we would be able to achieve anything. The versatility of python is my main argument in favor of its adoption. This is a mature and proven language with a lot of libraries and frameworks to suit all our present and future needs. It is widely supported on an extensive set of platforms and I can't think of an open-source project not supporting python right from the start. At this point, I will quote the folks at AppNexus from their conference in the recent PyData NYC 2012 : \"Python's versatility allows us to use it both for offline analytical tasks as well as production system development. Doing so allows us to bridge the gap between prototypes and production by relying on the same code libraries and frameworks for both, thereby tightening our innovation loop\". They say python helped them grow very rapidly and efficiently by permitting them to focus on innovation, needless to say that I share their point of view and would love to see this happening in my company.","title":"Python everywhere"},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#python-use-cases-in-my-company","text":"Now let's talk about our present and projected use cases. This is not an exhaustive list as I want to keep it simple and demonstrate the versatility I just talked about.","title":"Python use cases in my company"},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#scripting-and-automation","text":"I am a sysadmin and I am lazy. Nothing new here okay, but that's how I met python a bunch of years ago. At that time my company's infrastructure became more complex everyday as was growing very rapidly. New servers were arriving and provided new functionalities and new technologies which lead more and more to heterogeneity in the things we had to monitor, automate and configure. Python is a sysadmin's heaven with all its libraries capable of handling complex tasks easily, even in our cluster environments where you have to deal with parallel and high availability computing. This is a big relief to know that whatever the task you're asked to carry you can safely say : \"python can do it\". The keyword here is efficiency .","title":"Scripting and automation"},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#complete-and-complex-applications","text":"A lot of modern and cross-platform applications are written in python or based on it. I for one wrote an email parsing software a few years ago and it's still kicking in production, its maintenance is easy and it has evolved smoothly with our growth and needs. Another thing I like about this language is that it's fast and can benefit from a semi-compiled \"byte-code\" which speeds up your application. No, python is not C++ and speed is not it's biggest advantage of course but it's really fast enough to compete with others easily. Let's sample some famous software written in python : BitTorrent , original client, along with several derivatives Dropbox , a web-based file hosting service OpenStack , a cloud computing IaaS platform Portage , the heart of Gentoo Linux Ubuntu Software Center, a graphical package manager","title":"Complete and complex applications"},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#web-applications","text":"Python also have some solid and very powerful librairies able to manage asynchronous, real-time and scalable web applications and services. We already do have some of those robust web apps running in production and python demonstrates everyday all of the strenghts I already talked about here. We use librairies such as gevent along with web frameworks like flask and message queuing with zeromq . Someday I may write a post about our python web stack, it may be interesting to share about it. I have been able to recode a web app written in .NET very quickly while enhancing it in every way possible. It is way faster, reliable, fault tolerant and maintainable that it was before. Thanks to python we have a short development iteration which proves itself everyday as the application grows and is capable to meet and achieve any new challenge we're asked to take care of. I'm convinced that no other language could have been so powerful and versatile than python to do so. We're not the only ones thinking and experiencing this of course, still in the list we can see : Google uses Python for many tasks including the backends of web apps such as Google Groups, Gmail, and Google Maps, as well as for some of its search-engine internals AppNexus uses Python for some of their web apps backend YouTube uses Python \"to produce maintainable features in record times, with a minimum of developers\" Yahoo! Groups uses Python \"to maintain its discussion groups\" That's some big players indeed and it's interesting to see they use python for their web app backends .","title":"Web applications"},{"location":"Tech%20Blog/2012/2012-11-02-my-views-on-python/#conclusion","text":"I am not a software developer as I never took strong development courses at school. I am a sysadmin of complex, clustered and heterogeneous environments so this affects my development standards and point of view in a way that my expectations will surely be different from a pure developer. My main concerns can be defined with words like proven, easy, clean, versatile, maintainable, fast (to code and execute), scalable, fault-tolerant and cross-platform . All of my choices have been based on those standards and concerns and I think they apply well in our debate because I chose Python to meet them all and I've never been disappointed or limited by it. I hope this post reflects my thoughts and helped you understand them. I will tell you about the result and the decision my company made on this debate.","title":"Conclusion"},{"location":"Tech%20Blog/2012/2012-11-04-mongodb-ebuilds-cleanup-and-v2-2-1-released/","text":"Another bug killing spree has happened :) I'm glad to have closed quite a bunch of bugs related to mongoDB today. No more spidermonkey-1.7 dependency \u00b6 Thanks to the help from Ian Stakenvicius , we managed to drop the spidermonkey-1.7 dependency and use the embedded version shipped in the sources. I extended this fix to all 2.0.x and 2.2.x ebuilds, the only package remaining is the one for v1.8.5 but it will be dropped soon. Newer boost compatibility \u00b6 Boost-1.50 introduced a new filesystem v3 version while breaking compatibility with older v2 filesystems. This broke mongoDB compilation for the guys running unstable version of boost. As I didn't want to force stable users to keyword their boost versions, I kept a version of each 2.x series compatible with v2 <boost-1.50 filesystem. <dev-libs/boost-1.50 users should use the ebuilds revisions 1 (-r1) >=dev-libs/boost-1.50 users should use the ebuilds revisions 2 (-r2) v2.2.1 version bump \u00b6 Last but not least, the new v2.2.1 is also available but only for >=boost-1.50 users. This is a nice bugfix release which you should consider to apply since it's the first of the 2.2 series. See the full changelog .","title":"mongoDB : ebuilds cleanup and v2.2.1 released"},{"location":"Tech%20Blog/2012/2012-11-04-mongodb-ebuilds-cleanup-and-v2-2-1-released/#no-more-spidermonkey-17-dependency","text":"Thanks to the help from Ian Stakenvicius , we managed to drop the spidermonkey-1.7 dependency and use the embedded version shipped in the sources. I extended this fix to all 2.0.x and 2.2.x ebuilds, the only package remaining is the one for v1.8.5 but it will be dropped soon.","title":"No more spidermonkey-1.7 dependency"},{"location":"Tech%20Blog/2012/2012-11-04-mongodb-ebuilds-cleanup-and-v2-2-1-released/#newer-boost-compatibility","text":"Boost-1.50 introduced a new filesystem v3 version while breaking compatibility with older v2 filesystems. This broke mongoDB compilation for the guys running unstable version of boost. As I didn't want to force stable users to keyword their boost versions, I kept a version of each 2.x series compatible with v2 <boost-1.50 filesystem. <dev-libs/boost-1.50 users should use the ebuilds revisions 1 (-r1) >=dev-libs/boost-1.50 users should use the ebuilds revisions 2 (-r2)","title":"Newer boost compatibility"},{"location":"Tech%20Blog/2012/2012-11-04-mongodb-ebuilds-cleanup-and-v2-2-1-released/#v221-version-bump","text":"Last but not least, the new v2.2.1 is also available but only for >=boost-1.50 users. This is a nice bugfix release which you should consider to apply since it's the first of the 2.2 series. See the full changelog .","title":"v2.2.1 version bump"},{"location":"Tech%20Blog/2012/2012-11-05-portugal-tavira/","text":"So long since I didn't post a photo I've taken here. Since I got back from 3 weeks in Portugal, I guess it's a good time to share some of my favorite shots with you. Nikon FM2 with Kodak Ektachrome 100","title":"Portugal, Tavira"},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/","text":"I recently bumped quite a bunch of the clustering suite packages such as : sys-cluster/cluster-glue-1.0.11 sys-cluster/libqb-0.14.3 sys-cluster/corosync-1.4.4 sys-cluster/corosync-2.1.0 sys-cluster/crmsh-1.2.1 (new package) corosync-2.1.0 \u00b6 You should be aware that the corosync-2.x packages are still hard masked because the 2.0.x ones didn't compile properly and we didn't have a suitable pacemaker version for it to work with. There is another thing for us to consider and handle in the ebuilds when we'll be willing to release corosync-2 : it is not backward compatible ! So yes, you will either have to start with a fresh cluster or break your existing ones to migrate to corosync-2. The reason is that upstream decided to drop the plugins support from their software. So a non-plugin cluster cannot work with a plugin-enabled one. pacemaker-1.1.8 \u00b6 As for pacemaker-1.1.8, the bump request took quite some time. The reason is that it was released without backward compatibility support as well so you couldn't join your existing pacemaker-1.1.x cluster even when using corosync-1.x ! I found it unacceptable because this meant I would have to force for corosync-2 usage starting from pacemaker-1.1.8 for no real reason. Pacemaker upstream, namely Andrew Beekhof , is very responsive and kind so he offered a solution which I'm happy to provide to Gentoo users. crmsh-1.2.1 \u00b6 The crm command is not included in the pacemaker sources anymore. It is now an independent project lead by Dejan Muhamedagic to allow more versatility in its development and a better iteration of releases. I packaged and released it along with pacemaker-1.1.8 as well. Todo \u00b6 I'm still slacking on the sys-cluster/resource-agents package unfortunately. I already discussed with upstream about it as it's not a straightforward bump because they merged two different resource-agents developments. I think to have a pretty good idea of what needs to be done and will do my best to fix this gap as soon as I can.","title":"Clustering : corosync v2.1.0 & pacemaker v1.1.8"},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/#corosync-210","text":"You should be aware that the corosync-2.x packages are still hard masked because the 2.0.x ones didn't compile properly and we didn't have a suitable pacemaker version for it to work with. There is another thing for us to consider and handle in the ebuilds when we'll be willing to release corosync-2 : it is not backward compatible ! So yes, you will either have to start with a fresh cluster or break your existing ones to migrate to corosync-2. The reason is that upstream decided to drop the plugins support from their software. So a non-plugin cluster cannot work with a plugin-enabled one.","title":"corosync-2.1.0"},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/#pacemaker-118","text":"As for pacemaker-1.1.8, the bump request took quite some time. The reason is that it was released without backward compatibility support as well so you couldn't join your existing pacemaker-1.1.x cluster even when using corosync-1.x ! I found it unacceptable because this meant I would have to force for corosync-2 usage starting from pacemaker-1.1.8 for no real reason. Pacemaker upstream, namely Andrew Beekhof , is very responsive and kind so he offered a solution which I'm happy to provide to Gentoo users.","title":"pacemaker-1.1.8"},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/#crmsh-121","text":"The crm command is not included in the pacemaker sources anymore. It is now an independent project lead by Dejan Muhamedagic to allow more versatility in its development and a better iteration of releases. I packaged and released it along with pacemaker-1.1.8 as well.","title":"crmsh-1.2.1"},{"location":"Tech%20Blog/2012/2012-11-09-clustering-corosync-v2-1-0-pacemaker-v1-1-8/#todo","text":"I'm still slacking on the sys-cluster/resource-agents package unfortunately. I already discussed with upstream about it as it's not a straightforward bump because they merged two different resource-agents developments. I think to have a pretty good idea of what needs to be done and will do my best to fix this gap as soon as I can.","title":"Todo"},{"location":"Tech%20Blog/2012/2012-11-19-uwsgi-v1-4-1-lts-released/","text":"Ok that's a neat uwsgi release here and stamped for Long Time Support by unbit. I've tested it before bumping it to portage and it's behaving nicely with my gevent apps as there have been some nice improvements on this part as well. Quite a bunch of code have been refactored and optimized for enhanced performances. Have a look at the highlights, they're to be read with care. highlights : \u00b6 Support for the Go language Enhanced gevent reloading and handling (truly non-blocking wsgi.input, truly non-blocking writes...) Improved http/https router and fastrouter (better event-based engineering, reduced syscall usage) Improved systemd support Log filtering and routing Smart attach daemon to start a daemon along with your app A big work is being done about the documentation Yep that's some heavy stuff, there is more so you should definitely read the different changelogs on the mailing list !","title":"uWSGI : v1.4.1 LTS released"},{"location":"Tech%20Blog/2012/2012-11-19-uwsgi-v1-4-1-lts-released/#highlights","text":"Support for the Go language Enhanced gevent reloading and handling (truly non-blocking wsgi.input, truly non-blocking writes...) Improved http/https router and fastrouter (better event-based engineering, reduced syscall usage) Improved systemd support Log filtering and routing Smart attach daemon to start a daemon along with your app A big work is being done about the documentation Yep that's some heavy stuff, there is more so you should definitely read the different changelogs on the mailing list !","title":"highlights :"},{"location":"Tech%20Blog/2012/2012-11-20-rsyslog-new-v7-branch-released/","text":"It's been a long time since I took care of the rsyslog package so bear with me for these quite huge version releases. The main thing to note is that I finally packaged the new v7 branch which is stamped \" stable for production \" by upstream. The v5 branch is not supported anymore unless you have a professional contract with Adiscon so I encourage you to read Rainer's blog post . v7.2.2 \u00b6 Optimizations, code refactoring for way improved performances and a lot of new features are there. The v7 series bring a lot of changes and neat stuff to rsyslog as you can see on this v5 vs v7 link . Here are some hints : Improved configuration language Improved execution engine Full support for structured logging and JSON-based log messages (I discussed this matter on a previous post ) Ability to normalize legacy text log messages to JSON The changelog is very long, so you can browse this to find out more. v5.10.1 and v6.6.0 \u00b6 Those releases contain backports bugfixes (v5) and enhancements (v6) fixed on the v7 branch so if you're not ready to jump straight to the new version, you might consider updating your branch for a last time :)","title":"rsyslog : new v7 branch released"},{"location":"Tech%20Blog/2012/2012-11-20-rsyslog-new-v7-branch-released/#v722","text":"Optimizations, code refactoring for way improved performances and a lot of new features are there. The v7 series bring a lot of changes and neat stuff to rsyslog as you can see on this v5 vs v7 link . Here are some hints : Improved configuration language Improved execution engine Full support for structured logging and JSON-based log messages (I discussed this matter on a previous post ) Ability to normalize legacy text log messages to JSON The changelog is very long, so you can browse this to find out more.","title":"v7.2.2"},{"location":"Tech%20Blog/2012/2012-11-20-rsyslog-new-v7-branch-released/#v5101-and-v660","text":"Those releases contain backports bugfixes (v5) and enhancements (v6) fixed on the v7 branch so if you're not ready to jump straight to the new version, you might consider updating your branch for a last time :)","title":"v5.10.1 and v6.6.0"},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/","text":"mongodb-2.2.2 \u00b6 This is a bugfix release of mongoDB, there is nothing major to note about it so just have a look at the changelog . pymongo-2.4 \u00b6 Ok here's a bigger cake we'll have to focus on as you will have to adapt your code to use it properly. Don't be scared, upgrading will not instantly break your current apps but... Connection / ReplicaSetConnection deprecation \u00b6 Those classes are still available and provide the old safe=False behavior meaning that by default, operations are not acknowledged. They are being replaced by MongoClient and MongoReplicaSetClient classes which on the contrary do acknowledge operations by default. So yes, now your operations will run with a safe=True by default ! Write concern \u00b6 In the mongoDB talks we never hear about safe writes but about write concerns. A new API now handles these operations' behavior such as fsync / journal committing / write acknowledgment which is in line with the internals of mongoDB. I think it's more clear and straightforward to handle this that way so it's a good job done by upstream even if it means we have to adapt our code for it. They come in the number of four options which are applied on a database or collection level : w \\= integer : A value of 0 means we don't care so it's the fire-and-forget behavior we knew as safe=False. A value > 0 is the equivalent of the safe=True but with a more fine tuning on how many servers should confirm the operation. wtimeout = integer : Adds a timeout on the w parameter. j = bool : Wait until the operation has been committed to the journal. fsync = bool : Wait until the database to fsync all files to disk. Highlights \u00b6 Cursor can be copied with functions from the copy module The set_profiling_level() method now supports a slow_ms option See the rest in the full changelog .","title":"mongoDB v2.2.2 and pymongo v2.4 released"},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#mongodb-222","text":"This is a bugfix release of mongoDB, there is nothing major to note about it so just have a look at the changelog .","title":"mongodb-2.2.2"},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#pymongo-24","text":"Ok here's a bigger cake we'll have to focus on as you will have to adapt your code to use it properly. Don't be scared, upgrading will not instantly break your current apps but...","title":"pymongo-2.4"},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#connection-replicasetconnection-deprecation","text":"Those classes are still available and provide the old safe=False behavior meaning that by default, operations are not acknowledged. They are being replaced by MongoClient and MongoReplicaSetClient classes which on the contrary do acknowledge operations by default. So yes, now your operations will run with a safe=True by default !","title":"Connection / ReplicaSetConnection deprecation"},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#write-concern","text":"In the mongoDB talks we never hear about safe writes but about write concerns. A new API now handles these operations' behavior such as fsync / journal committing / write acknowledgment which is in line with the internals of mongoDB. I think it's more clear and straightforward to handle this that way so it's a good job done by upstream even if it means we have to adapt our code for it. They come in the number of four options which are applied on a database or collection level : w \\= integer : A value of 0 means we don't care so it's the fire-and-forget behavior we knew as safe=False. A value > 0 is the equivalent of the safe=True but with a more fine tuning on how many servers should confirm the operation. wtimeout = integer : Adds a timeout on the w parameter. j = bool : Wait until the operation has been committed to the journal. fsync = bool : Wait until the database to fsync all files to disk.","title":"Write concern"},{"location":"Tech%20Blog/2012/2012-11-28-mongodb-v2-2-2-and-pymongo-v2-4-released/#highlights","text":"Cursor can be copied with functions from the copy module The set_profiling_level() method now supports a slow_ms option See the rest in the full changelog .","title":"Highlights"},{"location":"Tech%20Blog/2012/2012-11-28-uwsgi-v1-4-2-released/","text":"Quick post for a bugfix release with some added flavor features for PHP users. highlights \u00b6 improved perl mules support fixed pending datas management in https router fixed thread-offloading of really big static files added --php-app-qs as micro-optimization (in-place of rewrite rules) for php apps added --php-var to inject custom (fixed) vars in the request Full changelog here , as usual.","title":"uWSGI : v1.4.2 released"},{"location":"Tech%20Blog/2012/2012-11-28-uwsgi-v1-4-2-released/#highlights","text":"improved perl mules support fixed pending datas management in https router fixed thread-offloading of really big static files added --php-app-qs as micro-optimization (in-place of rewrite rules) for php apps added --php-var to inject custom (fixed) vars in the request Full changelog here , as usual.","title":"highlights"},{"location":"Tech%20Blog/2012/2012-12-05-python-appnexus/","text":"On my earlier blog post about python I talked about Appnexus' motivation and use cases of this language. The PyData 2012 video of the talk is available now and I strongly encourage you to listen to it as it's not only focused on python but covers also teamwork management and organization. [ylwm_vimeo height='360' width='640']53053331[/ylwm_vimeo]","title":"Python @ Appnexus"},{"location":"Tech%20Blog/2012/2012-12-13-uwsgi-v1-4-3-released/","text":"Bugfix release with a few features. highlights \u00b6 close useless file descriptors when spawning daemons or external commands fixed log format in cheaper busyness algo allow to override the number of cpus detected by the build system using the CPUCOUNT env var See the full changelog .","title":"uWSGI : v1.4.3 released"},{"location":"Tech%20Blog/2012/2012-12-13-uwsgi-v1-4-3-released/#highlights","text":"close useless file descriptors when spawning daemons or external commands fixed log format in cheaper busyness algo allow to override the number of cpus detected by the build system using the CPUCOUNT env var See the full changelog .","title":"highlights"},{"location":"Tech%20Blog/2012/2012-12-16-portugal-algarve/","text":"Rolleiflex","title":"Portugal, Algarve"},{"location":"Tech%20Blog/2013/2013-01-10-uwsgi-v1-4-4-released/","text":"Bug fix release only, I've just bumped it on portage. highlights \u00b6 backported a couple of fixes for the https router fixed wrong typecasting in yaml and fixed subscription system on 32 bit added and additional error report for the gevent plugin if a read() fails improved apache2 mod_proxy_uwsgi See the full changelog as usual.","title":"uWSGI : v1.4.4 released"},{"location":"Tech%20Blog/2013/2013-01-10-uwsgi-v1-4-4-released/#highlights","text":"backported a couple of fixes for the https router fixed wrong typecasting in yaml and fixed subscription system on 32 bit added and additional error report for the gevent plugin if a read() fails improved apache2 mod_proxy_uwsgi See the full changelog as usual.","title":"highlights"},{"location":"Tech%20Blog/2013/2013-01-11-rabbitmq-2-8-7-3-0-1-released/","text":"It took me quite a while to bump this package as v3.0.0 and v3.0.1 were released respectively on Nov. 19 and Dec. 11 but here they are. This bump is dedicated to Jasper @darkroom bar in NZ :) Anyway, that's quite a huge release for the rabbitMQ team which they described themselves very well : This release introduces dynamic, policy-based control of mirroring and federation, improves the user friendliness of clustering, adds support for per-message TTL, introduces plugins for web-STOMP and MQTT, and adds many smaller new features and bug fixes. In addition, performance is improved in several cases. Most notably, mirrored queues are substantially faster. So my highlights will be very modest compared to the real thing and I strongly encourage you to spend 5 minutes to read the changelogs. highlights \u00b6 allow queue mirroring to be defined by broker-wide policy, not queue declaration, and add \"exactly\" mode support per-message TTL enable heartbeats by default remove support for AMQP's \"immediate\" publish mode greatly improve performance of mirrored queues improve performance of SSL when using HiPE compilation improve performance of bulk dead-lettering new plugin: implement Message Queue Telemetry Transport version 3.1 allow mixed patch versions of RabbitMQ in a cluster Read the 3.0.0 changelog and 3.0.1 changelog for more juicy stuff.","title":"rabbitMQ 2.8.7 & 3.0.1 released"},{"location":"Tech%20Blog/2013/2013-01-11-rabbitmq-2-8-7-3-0-1-released/#highlights","text":"allow queue mirroring to be defined by broker-wide policy, not queue declaration, and add \"exactly\" mode support per-message TTL enable heartbeats by default remove support for AMQP's \"immediate\" publish mode greatly improve performance of mirrored queues improve performance of SSL when using HiPE compilation improve performance of bulk dead-lettering new plugin: implement Message Queue Telemetry Transport version 3.1 allow mixed patch versions of RabbitMQ in a cluster Read the 3.0.0 changelog and 3.0.1 changelog for more juicy stuff.","title":"highlights"},{"location":"Tech%20Blog/2013/2013-01-15-coming-soon-on-mongodb/","text":"Some interesting stuff is cooking on the 2.3.x development branch of mongoDB, let's take a look at what we can expect to see in the future 2.4.x releases. Switch to v8 Javascript engine \u00b6 It's finally real since 2.3.1 as the folks at 10gen switched to v8 as the default JS engine powering mongoDB. This is a huge and a long time craved move which will primary improve performance and allow concurrent queries to be executed (aka collection level locking). Full text search \u00b6 This one is around since 2.3.2 and will be available as a new type of index you'll have to create using the textSearchEnabled=true parameter. It's still a new feature under development so don't expect something able to compete with solr of course but still, it's a very nice feature ! You'll find more information about this on A. Jesse Jiryu Davis' blog . Other highlights \u00b6 Aggregation framework performance improvements New circular geospatial index type. Support for line, polygon, and point intersection queries as well as GeoJSON parsing. Better server stats framework Storage engine improvements to reduce fragmentation New operators : $push to sorted and fixed size arrays, $setOnInsert modifier for upserts, $geoNear and $within operators in aggregation framework _secondaryThrottle is now on by default : this adds a write concern support for chunk migration reducing the replication lag caused by chunk moves --objcheck is now on by default : the server validates the requests' objects before inserting the data. This used to have a slight performance impact but should be countered by v8 fairly well","title":"Coming soon on mongoDB"},{"location":"Tech%20Blog/2013/2013-01-15-coming-soon-on-mongodb/#switch-to-v8-javascript-engine","text":"It's finally real since 2.3.1 as the folks at 10gen switched to v8 as the default JS engine powering mongoDB. This is a huge and a long time craved move which will primary improve performance and allow concurrent queries to be executed (aka collection level locking).","title":"Switch to v8 Javascript engine"},{"location":"Tech%20Blog/2013/2013-01-15-coming-soon-on-mongodb/#full-text-search","text":"This one is around since 2.3.2 and will be available as a new type of index you'll have to create using the textSearchEnabled=true parameter. It's still a new feature under development so don't expect something able to compete with solr of course but still, it's a very nice feature ! You'll find more information about this on A. Jesse Jiryu Davis' blog .","title":"Full text search"},{"location":"Tech%20Blog/2013/2013-01-15-coming-soon-on-mongodb/#other-highlights","text":"Aggregation framework performance improvements New circular geospatial index type. Support for line, polygon, and point intersection queries as well as GeoJSON parsing. Better server stats framework Storage engine improvements to reduce fragmentation New operators : $push to sorted and fixed size arrays, $setOnInsert modifier for upserts, $geoNear and $within operators in aggregation framework _secondaryThrottle is now on by default : this adds a write concern support for chunk migration reducing the replication lag caused by chunk moves --objcheck is now on by default : the server validates the requests' objects before inserting the data. This used to have a slight performance impact but should be countered by v8 fairly well","title":"Other highlights"},{"location":"Tech%20Blog/2013/2013-01-18-clustering-corosync-v2-3-0-resource-agents-3-9-4/","text":"Some new stuff related to clustering are now available on portage ! Here are the highlights. crmsh-1.2.4 \u00b6 better pacemaker-1.1.8 compatibility fine tuning history and regression fixes resource-agents-3.9.4 \u00b6 Oh yes I've been slacking on that one but it's finally here ! As a reminder this huge bump (1.0.4 -> 3.9.4) is the result of the upstream merge from the pacemaker and rgmanager resource agents developments. This is reflected by a new rgmanager USE flag for those who want to install those resources. zabbixserver : new resource agent IPaddr2 : partial rewrite and support for IPv6 iscsi : support for auto recovery and performance improvements tools: replace the findif binary by findif.sh See the full changelog . corosync-1.4.5 and corosync-2.3.0 \u00b6 The next releases of the flatiron and needle branches of corosync are rich of bug fixes, man updated and performance improvements. I wasn't able to find the proper changelog pages but you can have a look here where a bunch of the fixes are listed.","title":"Clustering : corosync v2.3.0 & resource-agents-3.9.4"},{"location":"Tech%20Blog/2013/2013-01-18-clustering-corosync-v2-3-0-resource-agents-3-9-4/#crmsh-124","text":"better pacemaker-1.1.8 compatibility fine tuning history and regression fixes","title":"crmsh-1.2.4"},{"location":"Tech%20Blog/2013/2013-01-18-clustering-corosync-v2-3-0-resource-agents-3-9-4/#resource-agents-394","text":"Oh yes I've been slacking on that one but it's finally here ! As a reminder this huge bump (1.0.4 -> 3.9.4) is the result of the upstream merge from the pacemaker and rgmanager resource agents developments. This is reflected by a new rgmanager USE flag for those who want to install those resources. zabbixserver : new resource agent IPaddr2 : partial rewrite and support for IPv6 iscsi : support for auto recovery and performance improvements tools: replace the findif binary by findif.sh See the full changelog .","title":"resource-agents-3.9.4"},{"location":"Tech%20Blog/2013/2013-01-18-clustering-corosync-v2-3-0-resource-agents-3-9-4/#corosync-145-and-corosync-230","text":"The next releases of the flatiron and needle branches of corosync are rich of bug fixes, man updated and performance improvements. I wasn't able to find the proper changelog pages but you can have a look here where a bunch of the fixes are listed.","title":"corosync-1.4.5 and corosync-2.3.0"},{"location":"Tech%20Blog/2013/2013-01-20-portugal-villa-extramuros/","text":"Rolleiflex","title":"Portugal, Villa Extramuros"},{"location":"Tech%20Blog/2013/2013-01-25-python-new-zeromq-and-mongodb-drivers/","text":"pyzmq-2.2.0.1 \u00b6 This one is very interesting to me because the code from the mighty gevent-zeromq library which brought gevent support to pyzmq has been merged into it ! I find it very humble and positive for the Open Source community to see such merges and want to express my gratitude to Travis Cline and the zeroMQ team for that. Migrating is as easy as : # gevent-zeromq previous way from gevent_zeromq import zmq pyzmq new way \u00b6 from pyzmq.green import zmq I strongly encourage you to read the changelog . pymongo-2.4.2 \u00b6 Bugfix release, the main point is that PyMongo will no longer select a replica set member for read operations that is not in primary or secondary state. Here is the changelog .","title":"Python : new zeroMQ and mongoDB drivers"},{"location":"Tech%20Blog/2013/2013-01-25-python-new-zeromq-and-mongodb-drivers/#pyzmq-2201","text":"This one is very interesting to me because the code from the mighty gevent-zeromq library which brought gevent support to pyzmq has been merged into it ! I find it very humble and positive for the Open Source community to see such merges and want to express my gratitude to Travis Cline and the zeroMQ team for that. Migrating is as easy as : # gevent-zeromq previous way from gevent_zeromq import zmq","title":"pyzmq-2.2.0.1"},{"location":"Tech%20Blog/2013/2013-01-25-python-new-zeromq-and-mongodb-drivers/#pyzmq-new-way","text":"from pyzmq.green import zmq I strongly encourage you to read the changelog .","title":"pyzmq new way"},{"location":"Tech%20Blog/2013/2013-01-25-python-new-zeromq-and-mongodb-drivers/#pymongo-242","text":"Bugfix release, the main point is that PyMongo will no longer select a replica set member for read operations that is not in primary or secondary state. Here is the changelog .","title":"pymongo-2.4.2"},{"location":"Tech%20Blog/2013/2013-01-27-turkey-istanbul/","text":"I used to go there quite a lot when I was young so I had a strange feeling to go back there after all those years. I feared not to be able to find the old Istanbul streets I knew but thankfully they do still exist. Rolleiflex, dec. 2012","title":"Turkey, Istanbul"},{"location":"Tech%20Blog/2013/2013-02-02-mongodb-v2-2-3-uwsgi-v1-4-5/","text":"Today's bumps are interesting, kudos to both upstreams for their work. mongodb-2.2.3 \u00b6 The main benefit of this version is the performance improvements on replicaSets. You can see the changelog here . uwsgi-1.4.5 \u00b6 This one is quite a bump. I was particulary interested to know that the memory leak discovered on the gevent loop got fixed. Have a look at all the nice backports too, they're worth it. added --python-version and --perl-version fixed a gevent memory leak backported --extract option added HTTP_Authorization support in mod_proxy_uwsgi backported --php-fallback backported perl uwsgi::postfork and uwsgi::atexit fixed a memory leak with --http-socket during uploads fixed unix signals usage in mules and spoolers reset cores/requests on worker startup fixed a refcnt bug in python uwsgi.workers() fixed async mode when multiple fds are in place fixed a cache collision bug backported --emperor-procname backported --touch-reload refactoring from 1.5 backported master+emperor fix from 1.5 new rbtree implementation (based on nginx) backported from 1.5 backported new logvars from 1.5","title":"mongoDB v2.2.3 & uwsgi v1.4.5"},{"location":"Tech%20Blog/2013/2013-02-02-mongodb-v2-2-3-uwsgi-v1-4-5/#mongodb-223","text":"The main benefit of this version is the performance improvements on replicaSets. You can see the changelog here .","title":"mongodb-2.2.3"},{"location":"Tech%20Blog/2013/2013-02-02-mongodb-v2-2-3-uwsgi-v1-4-5/#uwsgi-145","text":"This one is quite a bump. I was particulary interested to know that the memory leak discovered on the gevent loop got fixed. Have a look at all the nice backports too, they're worth it. added --python-version and --perl-version fixed a gevent memory leak backported --extract option added HTTP_Authorization support in mod_proxy_uwsgi backported --php-fallback backported perl uwsgi::postfork and uwsgi::atexit fixed a memory leak with --http-socket during uploads fixed unix signals usage in mules and spoolers reset cores/requests on worker startup fixed a refcnt bug in python uwsgi.workers() fixed async mode when multiple fds are in place fixed a cache collision bug backported --emperor-procname backported --touch-reload refactoring from 1.5 backported master+emperor fix from 1.5 new rbtree implementation (based on nginx) backported from 1.5 backported new logvars from 1.5","title":"uwsgi-1.4.5"},{"location":"Tech%20Blog/2013/2013-02-12-rabbitmq-v3-0-2-released/","text":"Quick post for a bugfix release. highlights \u00b6 fix broken error reporting for rabbitmqctl fix race causing queues to crash when stopping mirroring prevent rabbitmqctl status from killing web-STOMP connections fix hang of rabbitmqctl status when JSON-RPC plugin enabled Read the full changelog .","title":"rabbitMQ : v3.0.2 released"},{"location":"Tech%20Blog/2013/2013-02-12-rabbitmq-v3-0-2-released/#highlights","text":"fix broken error reporting for rabbitmqctl fix race causing queues to crash when stopping mirroring prevent rabbitmqctl status from killing web-STOMP connections fix hang of rabbitmqctl status when JSON-RPC plugin enabled Read the full changelog .","title":"highlights"},{"location":"Tech%20Blog/2013/2013-02-13-moosefs-v1-6-26-released/","text":"This was one of my first package creation in portage and although it doesn't have a quick iteration I still find this software very interesting. I was glad to see upstream release a new stable version but I have to admit I slacked quite a lot on actually seeing this bump (2012-08-16) :) Anyway, it's now live for some time in portage and I hope some folks were happy to update their platform with it. changelog \u00b6 (all) fixed signal handling in multithreaded modules (master) added goal and trashtime limits to mfsexport.cfg (metalogger) added simple check for downloaded metadata file (inspired by Davies Liu) (master) better handle disk full (inspired by Davies Liu) (master+metalogger) added keeping previous copies of metadata (inspired by Davies Liu) (all) reload all settings on \"reload\" (SIGHUP) (cs) disk scanning in background (cs) fixed long termination issue (found by Davies Liu)","title":"MooseFS : v1.6.26 released"},{"location":"Tech%20Blog/2013/2013-02-13-moosefs-v1-6-26-released/#changelog","text":"(all) fixed signal handling in multithreaded modules (master) added goal and trashtime limits to mfsexport.cfg (metalogger) added simple check for downloaded metadata file (inspired by Davies Liu) (master) better handle disk full (inspired by Davies Liu) (master+metalogger) added keeping previous copies of metadata (inspired by Davies Liu) (all) reload all settings on \"reload\" (SIGHUP) (cs) disk scanning in background (cs) fixed long termination issue (found by Davies Liu)","title":"changelog"},{"location":"Tech%20Blog/2013/2013-02-15-clustering-resource-agents-v3-9-5-crmsh-v1-2-5/","text":"Quick post about two bumps I made yesterday. The important one is sys-cluster/resource-agents-3.9.5 because the previous release contained a regression on the IPaddr2 resource . IPaddr2 didn't send unsolicited ARPs on start, depending on the ARP cache timeout time of the hosts on your topology, this could cause some serious delay when a failover takes place ! Also note the nice additions on crmsh which will make our lives easier. resource-agents-3.9.5 \u00b6 fix IPaddr2 ARP regression pgsql: support starting as Hot Standby support for RA tracing crmsh-1.2.5 \u00b6 cibconfig: modgroup command cibconfig: directed graph support history: diff command (between PE inputs) history: show command (show configuration of PE inputs) history: graph command","title":"Clustering : resource-agents v3.9.5 & crmsh v1.2.5"},{"location":"Tech%20Blog/2013/2013-02-15-clustering-resource-agents-v3-9-5-crmsh-v1-2-5/#resource-agents-395","text":"fix IPaddr2 ARP regression pgsql: support starting as Hot Standby support for RA tracing","title":"resource-agents-3.9.5"},{"location":"Tech%20Blog/2013/2013-02-15-clustering-resource-agents-v3-9-5-crmsh-v1-2-5/#crmsh-125","text":"cibconfig: modgroup command cibconfig: directed graph support history: diff command (between PE inputs) history: show command (show configuration of PE inputs) history: graph command","title":"crmsh-1.2.5"},{"location":"Tech%20Blog/2013/2013-02-16-postfix-2-10-0/","text":"Earlier this week, Wietse Venema announced the latest stable release of postfix , the famous Mail Transfer Agent. As I'm a long time user of this MTA, I thought I'd give it an echo on my blog with the usual highlights for you lazy readers. highlights \u00b6 Separation of relay policy (with smtpd_relay_restrictions) from spam policy (with smtpd_{client, helo, sender, recipient}_restrictions), which makes accidental open relay configuration less likely. The default is backwards compatible. HAproxy load-balancer support for postscreen(8) and smtpd(8). The nginx proxy was already supported by Postfix 2.9 smtpd(8), using XCLIENT commands. Support for the TLSv1 and TLSv2 protocols, as well as support to turn them off if needed for inter-operability. Laptop-friendly configuration. By default, Postfix now uses UNIX-domain sockets instead of FIFOs, and thus avoids MTIME file system updates on an idle mail system. Revised postconf (1) command. The \"-x\" option expands $name in a parameter value (both main.cf and master.cf); the \"-o name=value\" option overrides a main.cf parameter setting; and postconf(1) now warns about a $name that has no name=value setting.","title":"Postfix 2.10.0"},{"location":"Tech%20Blog/2013/2013-02-16-postfix-2-10-0/#highlights","text":"Separation of relay policy (with smtpd_relay_restrictions) from spam policy (with smtpd_{client, helo, sender, recipient}_restrictions), which makes accidental open relay configuration less likely. The default is backwards compatible. HAproxy load-balancer support for postscreen(8) and smtpd(8). The nginx proxy was already supported by Postfix 2.9 smtpd(8), using XCLIENT commands. Support for the TLSv1 and TLSv2 protocols, as well as support to turn them off if needed for inter-operability. Laptop-friendly configuration. By default, Postfix now uses UNIX-domain sockets instead of FIFOs, and thus avoids MTIME file system updates on an idle mail system. Revised postconf (1) command. The \"-x\" option expands $name in a parameter value (both main.cf and master.cf); the \"-o name=value\" option overrides a main.cf parameter setting; and postconf(1) now warns about a $name that has no name=value setting.","title":"highlights"},{"location":"Tech%20Blog/2013/2013-02-18-mongodb-2-4-0-rc/","text":"Last month I talked about what was coming on the next major stable release of mongoDB and I'm glad to point you to the recent 2.4.0 release notes which were announced today. Undoubtedly, the \"new\" cool stuff is about the hashed index feature which will help those who don't have a really even shard key to distribute their data on. Promising future as always !","title":"mongoDB 2.4.0 RC"},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/","text":"This is the first public release of one of my open-source projects, don't hesitate to share some feedback and/or thoughts with me. Background \u00b6 As a sysadmin, I have a lot of consoles open on multiples desktops and my 30\" screen was still not enough to cover my needs. To make things short, I needed to spare every pixel I could and KDE was really frustrating me as it was wasting a lot of space and ran quite a bunch of useless stuff in the background (akonadi/nepomuk anyone ?). Then came my cyclic rage about it and I finally found my precious : i3wm . I just love it as it is what I ever needed : a lightweight yet very functional and handy WM. No more resizing my consoles to fit next to each other and I can still use floating windows for the needed applications. No more huge and pixel-hungry task bar, just a simple and very efficient one. customization \u00b6 The problem when you start using something new and awesome is that you get a lot of ideas on what you could do with it and how you'd love to customize it. I mean, when using KDE or Gnome, your ideas are quickly shaped by the fact that you'd have to learn some exotic framework or language to implement them. Did you ever ask yourself how to add your own stuff in your task bar on KDE or Gnome ? What if the customization options you want are not available in your WM menus ? Well, my answer was \"never mind\" tbh and I slowly even lost the idea of implementing anything on my task bar. i3bar & i3status \u00b6 After switching to i3wm, my first customization was to name my workspaces and setup my own colors to adjust the look & feel of my desktop. Then I started to tune the program responsible for displaying useful information on my bar : i3status . As you may know, you have some limited modules which can take care of displaying some useful information on your bar such as the free disk space on a disk partition or your wired/wireless network status. But then I asked myself the same questions as I used to on my KDE days : what if I want more ? my own stuff on my task bar ? Introducing py3status \u00b6 Thanks to the i3bar open and simple protocol and the robust (even if somewhat limited) i3status program, I could finally hack into my bar . Naturally, I had to do it myself and there was a few examples available on the net but nothing really handy and extensible enough. That's how I had the idea of developping py3status ! philosophy & goals \u00b6 no extra configuration file needed rely on i3status and its existing configuration as much as possible be extensible , it must be easy for users to add their own stuff/output by writing a simple python class which will be loaded and executed dynamically add some built-in enhancement/transformation of basic i3status modules output available now on github \u00b6 I'm glad to announce that I pushed it today on github ! You can start using py3status now and give your feedback. I hope this project will help users get more of their i3wm environment and encourage their hacking power !","title":"Meet py3status"},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#background","text":"As a sysadmin, I have a lot of consoles open on multiples desktops and my 30\" screen was still not enough to cover my needs. To make things short, I needed to spare every pixel I could and KDE was really frustrating me as it was wasting a lot of space and ran quite a bunch of useless stuff in the background (akonadi/nepomuk anyone ?). Then came my cyclic rage about it and I finally found my precious : i3wm . I just love it as it is what I ever needed : a lightweight yet very functional and handy WM. No more resizing my consoles to fit next to each other and I can still use floating windows for the needed applications. No more huge and pixel-hungry task bar, just a simple and very efficient one.","title":"Background"},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#customization","text":"The problem when you start using something new and awesome is that you get a lot of ideas on what you could do with it and how you'd love to customize it. I mean, when using KDE or Gnome, your ideas are quickly shaped by the fact that you'd have to learn some exotic framework or language to implement them. Did you ever ask yourself how to add your own stuff in your task bar on KDE or Gnome ? What if the customization options you want are not available in your WM menus ? Well, my answer was \"never mind\" tbh and I slowly even lost the idea of implementing anything on my task bar.","title":"customization"},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#i3bar-i3status","text":"After switching to i3wm, my first customization was to name my workspaces and setup my own colors to adjust the look & feel of my desktop. Then I started to tune the program responsible for displaying useful information on my bar : i3status . As you may know, you have some limited modules which can take care of displaying some useful information on your bar such as the free disk space on a disk partition or your wired/wireless network status. But then I asked myself the same questions as I used to on my KDE days : what if I want more ? my own stuff on my task bar ?","title":"i3bar &amp; i3status"},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#introducing-py3status","text":"Thanks to the i3bar open and simple protocol and the robust (even if somewhat limited) i3status program, I could finally hack into my bar . Naturally, I had to do it myself and there was a few examples available on the net but nothing really handy and extensible enough. That's how I had the idea of developping py3status !","title":"Introducing py3status"},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#philosophy-goals","text":"no extra configuration file needed rely on i3status and its existing configuration as much as possible be extensible , it must be easy for users to add their own stuff/output by writing a simple python class which will be loaded and executed dynamically add some built-in enhancement/transformation of basic i3status modules output","title":"philosophy &amp; goals"},{"location":"Tech%20Blog/2013/2013-02-21-meet-py3status/#available-now-on-github","text":"I'm glad to announce that I pushed it today on github ! You can start using py3status now and give your feedback. I hope this project will help users get more of their i3wm environment and encourage their hacking power !","title":"available now on github"},{"location":"Tech%20Blog/2013/2013-02-23-packaging-py3status/","text":"So I exchanged some mails with Michael Stapelberg of i3wm who rightly pointed out that my initial installation method of py3status was un-pythonic. I was not satisfied of using a bash setup either and I couldn't imagine a better opportunity to learn how to write a proper setup.py for my project. Thanks to my Gentoo Linux packager experience, I knew what I had to do, so a few searchs and tests later I'm glad to announce that py3status installation is standard ! I of course also packaged py3status for Gentoo Linux users : meet x11-misc/py3status on my overlay . py3status being a real command and not a simple python module, I had to find the way to have setuptools taking care of this for me. I was happy to find out that this is pretty easy and that it works on both Linux & Windows, it's awesome ! I will explain all this in one of my next blog post as I'm sure it can be of interest.","title":"Packaging py3status"},{"location":"Tech%20Blog/2013/2013-02-26-uwsgi-mono-asp-net-support/","text":"Quick post to point out that the upcoming 1.9 release of uWSGI will support running ASP.NET applications using Mono . Not to mention all the benefits of the uWSGI application deployment and monitoring mechanisms, this is a very interesting point as you can have a unique platform running your Python and ASP.NET applications ! I'll certainly keep an eye on it.","title":"uWSGI : Mono ASP.NET support"},{"location":"Tech%20Blog/2013/2013-02-26-uwsgi-v1-4-6-released/","text":"This is quite a big release for its backports and improvements. But there are two more interesting things to note here : uWSGI supports Heroku and Dreamhost deployments ! tutorials \u00b6 uWSGI + Heroku + Python tutorial uWSGI + Dreamhost tutorial highlights \u00b6 fix SERVER_PORT value in corerouters when using shared sockets backported --thunder-lock option to reduce thundering herd problem (use with caution) fixed pthread robust mutexes in newer glibc backported improvements for the alarm_xmpp plugin fixed suspend when harakiri is in place reset sigmask on startup fixed master+emperor configurations backported more logvars (check here: https://uwsgi-docs.readthedocs.org/en/latest/LogFormat.html) fixed muleloop in uwsgidecorators fixed a refcnt bug in the psgi plugin spotted by Nick Gregory (issue #158) backported new python build system (Heroku friendly) backported --perl-arg and --perl-args options (to add items in @ARGV) backported perl async fixes allows --attach-daemon without workers","title":"uWSGI : v1.4.6 released"},{"location":"Tech%20Blog/2013/2013-02-26-uwsgi-v1-4-6-released/#tutorials","text":"uWSGI + Heroku + Python tutorial uWSGI + Dreamhost tutorial","title":"tutorials"},{"location":"Tech%20Blog/2013/2013-02-26-uwsgi-v1-4-6-released/#highlights","text":"fix SERVER_PORT value in corerouters when using shared sockets backported --thunder-lock option to reduce thundering herd problem (use with caution) fixed pthread robust mutexes in newer glibc backported improvements for the alarm_xmpp plugin fixed suspend when harakiri is in place reset sigmask on startup fixed master+emperor configurations backported more logvars (check here: https://uwsgi-docs.readthedocs.org/en/latest/LogFormat.html) fixed muleloop in uwsgidecorators fixed a refcnt bug in the psgi plugin spotted by Nick Gregory (issue #158) backported new python build system (Heroku friendly) backported --perl-arg and --perl-args options (to add items in @ARGV) backported perl async fixes allows --attach-daemon without workers","title":"highlights"},{"location":"Tech%20Blog/2013/2013-03-04-uwsgi-v1-4-8-released/","text":"Yet another bump for uWSGI as upstream is working hard on the 1.9 branch which will lead to the 2.0 LTS version. I guess it's time I take a few moments to give you some hints about what's coming for the v2.0 of uWSGI, be aware that this is some heavy stuff. future v2.0 highlights \u00b6 new fully non-blocking API which applies to all plugins, this will benefit the perl/PSGI plugin as well faster uwsgi/HTTP/FastCGI/SCGI native sockets thanks to better parsers splitted error logging from request logging for enhanced debugging more offloading improvements such as a new function to write files on disk and non-blocking workers for static files service better static files handling thanks to the new caching system totally rewritten web cache system allows you to have multiple caches per instance and tune them finely replaced the old clustering system with a new Legion subsystem providing resources management (yeah you wouldn't need stuff like pacemaker to handle your uWSGI cluster) advanced exception subsystem SPDY v3 support SNI support support for HTTP router keepalive, auto-chunking, auto-gzip and transparent websockets a SSL router will be available websockets API sponsored by 20Tab S.r.l. (a company working on HTML5 browsers game, thanks guys) programmable internal router and of course, the Mono/ASP.NET plugin I talked about in my previous post See the full and detailed list here v1.4.8 highlights \u00b6 added support for ruby 2.0 removed the mono/asp.net plugin (a new, working one, is in 1.9) backported the improved carbon plugin fixed a corner-case bug with the caching subsystem (Laurent Luce) fixed ipcsem on Linux backported --not-log-alarm (negative version of --log-alarm) backported add_timer and add_rb_timer api functions for the perl/psgi plugin backported --for-glob, this is like --for but with glob expansion (Guido Berhoerster) avoid gateways crash on master shutdown backported https re-handshake management improved gevent timeout management uWSGI can now be installed as a ruby gem backported --http-socket-modifier1/2","title":"uWSGI : v1.4.8 released & v2.0 sneak peek"},{"location":"Tech%20Blog/2013/2013-03-04-uwsgi-v1-4-8-released/#future-v20-highlights","text":"new fully non-blocking API which applies to all plugins, this will benefit the perl/PSGI plugin as well faster uwsgi/HTTP/FastCGI/SCGI native sockets thanks to better parsers splitted error logging from request logging for enhanced debugging more offloading improvements such as a new function to write files on disk and non-blocking workers for static files service better static files handling thanks to the new caching system totally rewritten web cache system allows you to have multiple caches per instance and tune them finely replaced the old clustering system with a new Legion subsystem providing resources management (yeah you wouldn't need stuff like pacemaker to handle your uWSGI cluster) advanced exception subsystem SPDY v3 support SNI support support for HTTP router keepalive, auto-chunking, auto-gzip and transparent websockets a SSL router will be available websockets API sponsored by 20Tab S.r.l. (a company working on HTML5 browsers game, thanks guys) programmable internal router and of course, the Mono/ASP.NET plugin I talked about in my previous post See the full and detailed list here","title":"future v2.0 highlights"},{"location":"Tech%20Blog/2013/2013-03-04-uwsgi-v1-4-8-released/#v148-highlights","text":"added support for ruby 2.0 removed the mono/asp.net plugin (a new, working one, is in 1.9) backported the improved carbon plugin fixed a corner-case bug with the caching subsystem (Laurent Luce) fixed ipcsem on Linux backported --not-log-alarm (negative version of --log-alarm) backported add_timer and add_rb_timer api functions for the perl/psgi plugin backported --for-glob, this is like --for but with glob expansion (Guido Berhoerster) avoid gateways crash on master shutdown backported https re-handshake management improved gevent timeout management uWSGI can now be installed as a ruby gem backported --http-socket-modifier1/2","title":"v1.4.8 highlights"},{"location":"Tech%20Blog/2013/2013-03-05-switching-from-xchat-to-hexchat/","text":"As polynomial-c announced last year , xchat upstream is dead for some time now. Fortunately for us the fork hexchat is taking over and is compatible with your current xchat configuration so it's really easy to migrate. migrating to hexchat \u00b6 $ mv .xchat2/ .config/hexchat/ $ mv .config/hexchat/xchat.conf .config/hexchat/hexchat.conf As mentioned in the comments by Louis Tim Larsen, if you have some servers configured with more than one channel make sure to run this command as well : sed -i 's/,#/\\nJ=#/g' ~/.config/hexchat/servlist.conf Then run hexchat as you would xchat. theme your IRC \u00b6 If like me you're a fan of Solarized , you can get hexchat to use these colors thanks to some contributed themes . Here's the quick hack to have it done, for the Solarized Light theme. $ cd .config/hexchat $ wget http://dl.hexchat.org/themes/Solarized%20Light.hct $ unzip -o Solarized\\ Light.hct Restart hexchat and enjoy your up to date and colorized IRC client.","title":"Switching from Xchat to HexChat"},{"location":"Tech%20Blog/2013/2013-03-05-switching-from-xchat-to-hexchat/#migrating-to-hexchat","text":"$ mv .xchat2/ .config/hexchat/ $ mv .config/hexchat/xchat.conf .config/hexchat/hexchat.conf As mentioned in the comments by Louis Tim Larsen, if you have some servers configured with more than one channel make sure to run this command as well : sed -i 's/,#/\\nJ=#/g' ~/.config/hexchat/servlist.conf Then run hexchat as you would xchat.","title":"migrating to hexchat"},{"location":"Tech%20Blog/2013/2013-03-05-switching-from-xchat-to-hexchat/#theme-your-irc","text":"If like me you're a fan of Solarized , you can get hexchat to use these colors thanks to some contributed themes . Here's the quick hack to have it done, for the Solarized Light theme. $ cd .config/hexchat $ wget http://dl.hexchat.org/themes/Solarized%20Light.hct $ unzip -o Solarized\\ Light.hct Restart hexchat and enjoy your up to date and colorized IRC client.","title":"theme your IRC"},{"location":"Tech%20Blog/2013/2013-03-11-uwsgi-v1-4-9-released/","text":"Yet another version bump for this very active package. highlights \u00b6 avoid crashing carbon on master shutdown call ERR_clear_error after each https session close fixed broodlord mode removed broken JVM and JWSGI plugins (stable versions are in 1.9) backported cache_update for lua and fixed its lock handling Full changelog is here.","title":"uWSGI : v1.4.9 released"},{"location":"Tech%20Blog/2013/2013-03-11-uwsgi-v1-4-9-released/#highlights","text":"avoid crashing carbon on master shutdown call ERR_clear_error after each https session close fixed broodlord mode removed broken JVM and JWSGI plugins (stable versions are in 1.9) backported cache_update for lua and fixed its lock handling Full changelog is here.","title":"highlights"},{"location":"Tech%20Blog/2013/2013-03-13-pacemaker-vulnerability-and-v1-1-9-release/","text":"A security vulnerability ( CVE-2013-0281 ) was found on pacemaker which permitted attackers to prevent your cluster from serving more CIB requests. Although this issue was quickly fixed by upstream, they didn't add a new tag to pacemaker so I did ask Andrew Beekhof for one so I could take care of bug #457572 . Gentoo users, here comes pacemaker-1.1.9 ! important \u00b6 While packaging and testing pacemaker-1.1.9, I ran into some weird permission issues which I debugged with @ beekhof and @ asalkeld (thx again guys). Turns out that when enabling ACL support on pacemaker, you now need to add root to the haclient group ! The reason is that pacemaker now uses shared memory IPC sockets from libqb to communicate with corosync (on /dev/shm/). v1.1.9 changelog \u00b6 corosync: Allow cman and corosync 2.0 nodes to use a name other than uname() corosync: Use queues to avoid blocking when sending CPG messages Drop per-user core directories ipc: Compress messages that exceed the configured IPC message limit ipc: Use queues to prevent slow clients from blocking the server ipc: Use shared memory by default lrmd: Support nagios remote monitoring lrmd: Pacemaker Remote Daemon for extending pacemaker functionality outside corosync cluster. pengine: Check for master/slave resources that are not OCF agents pengine: Support a 'requires' resource meta-attribute for controlling whether it needs quorum, fencing or nothing pengine: Support for resource container pengine: Support resources that require unfencing before start Since the main focus of the bump was to fix a security issue, I didn't add the new nagios feature to the ebuild. If you're interested in it, just say so and I'll do my best to add it asap.","title":"Pacemaker vulnerability and v1.1.9 release"},{"location":"Tech%20Blog/2013/2013-03-13-pacemaker-vulnerability-and-v1-1-9-release/#important","text":"While packaging and testing pacemaker-1.1.9, I ran into some weird permission issues which I debugged with @ beekhof and @ asalkeld (thx again guys). Turns out that when enabling ACL support on pacemaker, you now need to add root to the haclient group ! The reason is that pacemaker now uses shared memory IPC sockets from libqb to communicate with corosync (on /dev/shm/).","title":"important"},{"location":"Tech%20Blog/2013/2013-03-13-pacemaker-vulnerability-and-v1-1-9-release/#v119-changelog","text":"corosync: Allow cman and corosync 2.0 nodes to use a name other than uname() corosync: Use queues to avoid blocking when sending CPG messages Drop per-user core directories ipc: Compress messages that exceed the configured IPC message limit ipc: Use queues to prevent slow clients from blocking the server ipc: Use shared memory by default lrmd: Support nagios remote monitoring lrmd: Pacemaker Remote Daemon for extending pacemaker functionality outside corosync cluster. pengine: Check for master/slave resources that are not OCF agents pengine: Support a 'requires' resource meta-attribute for controlling whether it needs quorum, fencing or nothing pengine: Support for resource container pengine: Support resources that require unfencing before start Since the main focus of the bump was to fix a security issue, I didn't add the new nagios feature to the ebuild. If you're interested in it, just say so and I'll do my best to add it asap.","title":"v1.1.9 changelog"},{"location":"Tech%20Blog/2013/2013-03-15-follow-up-on-pacemaker-v1-1-9-and-updated-pacemaker-gui/","text":"In my previous post I talked about a permission problem introduced in pacemaker-1.1.9 which requires root to be a member of the haclient group. I've been helping @beekhof to investigate on this and I'm glad he found and fixed both the problem and a memory leak ! We're still investigating on another issue but we should be seeing a new version bump pretty soon, thank you Andrew ! pacemaker-gui v2.1.2 \u00b6 One of my colleagues recently complained that pacemaker-gui-2.1.1 was not compatible with newer pacemaker releases (>=1.1.8) so he had to install pacemaker-1.1.7 if he wanted to benefit from the GUI. I contacted @gao-yan from SUSE who's the main upstream for this package and asked him for a tag bump. Here comes pacemaker-gui-2.1.2 which is compatible with all newer pacemaker releases ! Thanks again mate.","title":"Follow-up on pacemaker v1.1.9 and updated pacemaker-gui"},{"location":"Tech%20Blog/2013/2013-03-15-follow-up-on-pacemaker-v1-1-9-and-updated-pacemaker-gui/#pacemaker-gui-v212","text":"One of my colleagues recently complained that pacemaker-gui-2.1.1 was not compatible with newer pacemaker releases (>=1.1.8) so he had to install pacemaker-1.1.7 if he wanted to benefit from the GUI. I contacted @gao-yan from SUSE who's the main upstream for this package and asked him for a tag bump. Here comes pacemaker-gui-2.1.2 which is compatible with all newer pacemaker releases ! Thanks again mate.","title":"pacemaker-gui v2.1.2"},{"location":"Tech%20Blog/2013/2013-03-16-rabbitmq-v3-0-4-released/","text":"Within a week, rabbitMQ got bumped twice. I'm happy to quickly post about those recent bumps so here is a highlight of 3.0.3 and 3.0.4 changelogs. highlights \u00b6 fix connection failure to start reading again in rare circumstances when coming out of flow control ensure invocation of \"rabbitmqctl stop_app\" during server startup on a fresh node does not leave a corrupted Mnesia schema ensure messages expire immediately when reaching the head of a queue after basic.get ensure parameters and policies for a vhost are removed with that vhost do not log spurious errors for connections that close very early ensure \"rabbitmqctl forget_cluster_node\" removes durable queue records for unmirrored queues on the forgotten node clean up connection and channel records from nodes that have crashed do not show 404 errors when rabbitmq_federation_management is installed and rabbitmq_federation is not ensure the reader process hibernates when idle prevent x-received-from header from leaking upstream credentials","title":"rabbitMQ : v3.0.4 released"},{"location":"Tech%20Blog/2013/2013-03-16-rabbitmq-v3-0-4-released/#highlights","text":"fix connection failure to start reading again in rare circumstances when coming out of flow control ensure invocation of \"rabbitmqctl stop_app\" during server startup on a fresh node does not leave a corrupted Mnesia schema ensure messages expire immediately when reaching the head of a queue after basic.get ensure parameters and policies for a vhost are removed with that vhost do not log spurious errors for connections that close very early ensure \"rabbitmqctl forget_cluster_node\" removes durable queue records for unmirrored queues on the forgotten node clean up connection and channel records from nodes that have crashed do not show 404 errors when rabbitmq_federation_management is installed and rabbitmq_federation is not ensure the reader process hibernates when idle prevent x-received-from header from leaking upstream credentials","title":"highlights"},{"location":"Tech%20Blog/2013/2013-03-19-py3status-v0-5/","text":"Since the first release of py3status , quite a bunch of bugfixes and features came such as python3 support and SIGUSR1 signal handling to force an update of the bar. changelog \u00b6 bugfix : fix delta variable declaration examples : add GLPI open tickets counter module example python3 compatibility inspired by waaaaargh (Johannes Firlefanz) improvement : iterate over user classes in a sorted manner to allow a predictive ordering of outputs bugfix : dont fail if i3status output comes slower than py3status message polling interval feature : signal SIGUSR1 forces i3status and i3bar refresh, feature request by Michael Schaefer","title":"py3status v0.5"},{"location":"Tech%20Blog/2013/2013-03-19-py3status-v0-5/#changelog","text":"bugfix : fix delta variable declaration examples : add GLPI open tickets counter module example python3 compatibility inspired by waaaaargh (Johannes Firlefanz) improvement : iterate over user classes in a sorted manner to allow a predictive ordering of outputs bugfix : dont fail if i3status output comes slower than py3status message polling interval feature : signal SIGUSR1 forces i3status and i3bar refresh, feature request by Michael Schaefer","title":"changelog"},{"location":"Tech%20Blog/2013/2013-03-20-mongodb-v2-4-0-released/","text":"A few months ago , I pointed out what was coming with this release and did an update of this cooking 2.4.0 later. Yesterday, 10gen announced the release of the new stable branch of mongoDB v2.4.0 . Instead of talking about it again, I'll focus on what this release brings to Gentoo users as I'm glad to announce that it's already available in portage. SSL support \u00b6 First of all, I think it was a good time to close bug #421289 and finally enable the SSL support via the ssl USE flag . I'll support it as much as upstream does, so don't expect some big magic about it. Shared client library \u00b6 Since this has always been a mess, I also added the sharedclient USE flag so that users who really need the client shared library can toggle its installation easily. This also permits me to isolate possible problems from the main ebuild. Upgrading to 2.4 \u00b6 This is seamless unless you're running a sharded cluster ! In this case, take great care of what you do and note that the upgrade is only possible if your cluster is running v2.2 ! Please read with care the upgrade plan .","title":"mongoDB : v2.4.0 released"},{"location":"Tech%20Blog/2013/2013-03-20-mongodb-v2-4-0-released/#ssl-support","text":"First of all, I think it was a good time to close bug #421289 and finally enable the SSL support via the ssl USE flag . I'll support it as much as upstream does, so don't expect some big magic about it.","title":"SSL support"},{"location":"Tech%20Blog/2013/2013-03-20-mongodb-v2-4-0-released/#shared-client-library","text":"Since this has always been a mess, I also added the sharedclient USE flag so that users who really need the client shared library can toggle its installation easily. This also permits me to isolate possible problems from the main ebuild.","title":"Shared client library"},{"location":"Tech%20Blog/2013/2013-03-20-mongodb-v2-4-0-released/#upgrading-to-24","text":"This is seamless unless you're running a sharded cluster ! In this case, take great care of what you do and note that the upgrade is only possible if your cluster is running v2.2 ! Please read with care the upgrade plan .","title":"Upgrading to 2.4"},{"location":"Tech%20Blog/2013/2013-03-24-python-writing-a-proper-setup-for-your-project/","text":"For an open-source project to be adopted, it must be easy to install and test it. Before publicly releasing py3status I thus had to figure out how to get my python project installed properly for every user who would be interested in it. The common and most efficient way of writing a setup process in python is by using the setuptools package and writing your own setup.py file. Doing so is not hard at all as there are quite a bunch of examples you can start from but my challenge was that py3status is not a library you install and then import on your python code, instead it must be seen and used as an executable available for all users (something like /usr/bin/py3status). I'll cover the steps I used to achieve this kind of installation. setup.py basics \u00b6 You can see the setup.py file just like any other python program you write where you import the functions you need from setuptools. import os from setuptools import find_packages, setup Basically, a setup.py file is just a call to setup with a fair amount of parameters depending on the size and complexity of your project. Let's see a basic usage with no real magic. setup( name='py3status', version='0.5', url='https://github.com/ultrabug/py3status/wiki', download_url='https://github.com/ultrabug/py3status', license='BSD', author='Ultrabug', author_email='ultrabug@sikritdomain.com', description='py3status is an extensible i3status wrapper written in python', long_description='this is a very long description which im writing for example', platforms='any', ) As you can see those parameters are just fields describing your project but there are of course more parameters you can use to become more specific about it such as which other packages it depends from or special operations to be made upon installation. classifiers \u00b6 As with a literal description, you must categorize your project so that it will be correctly understood by automatic classifiers for example. The classifiers parameter is a list of those categories which you can find a list here . classifiers=\\[ 'License :: OSI Approved :: BSD License', 'Operating System :: POSIX :: Linux', 'Programming Language :: Python', 'Programming Language :: Python :: 2.5', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.2', 'Topic :: Software Development :: Libraries :: Python Modules', 'Topic :: Desktop Environment :: Window Managers :: i3wm', \\], getting an executable from your python program \u00b6 As I explained earlier, py3status must be used as an executable available in the users' PATH just like any other binary or commands on the system. I was thrilled to discover that achieving this is a piece of cake using setuptools, you just have to use the entry_points parameter and it will be taken care of for you . entry_points={ 'console_scripts': [ 'py3status = py3status:main', ] }, So here I'm asking setuptools to create a script which will execute py3status' main function. It will generate a python program that just does that, call it py3status, place it in /usr/bin and make it executable. Et voil\u00e0 ! An important thing to note is that it also works in Windows and that's how you'll get a .exe from your python code ! Learn more on this subject by reading the excellent documentation on how to get started with setuptools .","title":"Python : writing a proper setup for your project"},{"location":"Tech%20Blog/2013/2013-03-24-python-writing-a-proper-setup-for-your-project/#setuppy-basics","text":"You can see the setup.py file just like any other python program you write where you import the functions you need from setuptools. import os from setuptools import find_packages, setup Basically, a setup.py file is just a call to setup with a fair amount of parameters depending on the size and complexity of your project. Let's see a basic usage with no real magic. setup( name='py3status', version='0.5', url='https://github.com/ultrabug/py3status/wiki', download_url='https://github.com/ultrabug/py3status', license='BSD', author='Ultrabug', author_email='ultrabug@sikritdomain.com', description='py3status is an extensible i3status wrapper written in python', long_description='this is a very long description which im writing for example', platforms='any', ) As you can see those parameters are just fields describing your project but there are of course more parameters you can use to become more specific about it such as which other packages it depends from or special operations to be made upon installation.","title":"setup.py basics"},{"location":"Tech%20Blog/2013/2013-03-24-python-writing-a-proper-setup-for-your-project/#classifiers","text":"As with a literal description, you must categorize your project so that it will be correctly understood by automatic classifiers for example. The classifiers parameter is a list of those categories which you can find a list here . classifiers=\\[ 'License :: OSI Approved :: BSD License', 'Operating System :: POSIX :: Linux', 'Programming Language :: Python', 'Programming Language :: Python :: 2.5', 'Programming Language :: Python :: 2.6', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.2', 'Topic :: Software Development :: Libraries :: Python Modules', 'Topic :: Desktop Environment :: Window Managers :: i3wm', \\],","title":"classifiers"},{"location":"Tech%20Blog/2013/2013-03-24-python-writing-a-proper-setup-for-your-project/#getting-an-executable-from-your-python-program","text":"As I explained earlier, py3status must be used as an executable available in the users' PATH just like any other binary or commands on the system. I was thrilled to discover that achieving this is a piece of cake using setuptools, you just have to use the entry_points parameter and it will be taken care of for you . entry_points={ 'console_scripts': [ 'py3status = py3status:main', ] }, So here I'm asking setuptools to create a script which will execute py3status' main function. It will generate a python program that just does that, call it py3status, place it in /usr/bin and make it executable. Et voil\u00e0 ! An important thing to note is that it also works in Windows and that's how you'll get a .exe from your python code ! Learn more on this subject by reading the excellent documentation on how to get started with setuptools .","title":"getting an executable from your python program"},{"location":"Tech%20Blog/2013/2013-03-25-mongodb-v2-4-1-and-pymongo-2-5-released/","text":"10gen released a critical update for mongoDB 2.4.0 which affected queries on secondaries, you should upgrade asap. The python mongo driver followed the 2.4.x releases and got bumped to 2.5 this week-end. I am pleased to announce that I took the chance to add the kerberos authentication support to both ebuilds while bumping them. pymongo-2.5 \u00b6 GSSAPI (Kerberos) authentication SSL certificate validation with hostname matching Delegated and role based authentication","title":"mongoDB v2.4.1 and pymongo 2.5 released"},{"location":"Tech%20Blog/2013/2013-03-25-mongodb-v2-4-1-and-pymongo-2-5-released/#pymongo-25","text":"GSSAPI (Kerberos) authentication SSL certificate validation with hostname matching Delegated and role based authentication","title":"pymongo-2.5"},{"location":"Tech%20Blog/2013/2013-03-30-tokyo-mont-fuji/","text":"Photos prises du haut de la mairie de Bunkyo-ku, il y avait beaucoup de Japonais car le coucher de soleil laissait voir le mont Fuji entre les tours. Moment magique ! Rolleiflex","title":"Tokyo, mont Fuji"},{"location":"Tech%20Blog/2013/2013-04-06-py3status-v0-7/","text":"Some cool bugfixes happened since v0.5 and py3status broke the 20 github stars, I hope people are enjoying it. changelog \u00b6 clear the user class cache when receiving SIGUSR1 specify default folder for user defined classes fix time transformation thx to @Lujeni add Pingdom checks latency example module fix issue #2 reported by @Detegr which caused the clock to drift on some use cases","title":"py3status v0.7"},{"location":"Tech%20Blog/2013/2013-04-06-py3status-v0-7/#changelog","text":"clear the user class cache when receiving SIGUSR1 specify default folder for user defined classes fix time transformation thx to @Lujeni add Pingdom checks latency example module fix issue #2 reported by @Detegr which caused the clock to drift on some use cases","title":"changelog"},{"location":"Tech%20Blog/2013/2013-04-14-py3status-v0-8/","text":"I went on a coding frenzy to implement most of the stuff I was not happy with py3status so far. Here comes py3status code name : San Francisco (more photos to come). PEP8 \u00b6 I always had the habit of using tabulators to indent my code. @ Lujeni pointed out that this is not a PEP8 recommended method and that we should start respecting more of it in the near future. Well, he's right and I guess it was time to move on so I switched to using spaces and corrected a lot of other coding style stuff which got my code a score going from around -1/10 to around 9.5/10 on pylint ! Threaded modules' execution \u00b6 This was the major thing I was not happy with : when a user-written module was executed for injection, the time it took to get its response would cause py3status to stop updating the bar. This means that if you had a database call to make to get some stuff you need displayed on the bar and it took 10 seconds, py3status was sleeping for those 10 seconds to update the bar ! This behavior could cause some delays in the clock ticking for example. I decided to offload all of the modules' detection and execution to a thread to solve this problem. To be frank, this also helped to rationalize the code better as well. No more delays and a cleaner handling is what you get, stuff will start appending themselves whatever the time they take to execute ! Python3 \u00b6 It was about time the examples available on py3status would also work using python3.","title":"py3status v0.8"},{"location":"Tech%20Blog/2013/2013-04-14-py3status-v0-8/#pep8","text":"I always had the habit of using tabulators to indent my code. @ Lujeni pointed out that this is not a PEP8 recommended method and that we should start respecting more of it in the near future. Well, he's right and I guess it was time to move on so I switched to using spaces and corrected a lot of other coding style stuff which got my code a score going from around -1/10 to around 9.5/10 on pylint !","title":"PEP8"},{"location":"Tech%20Blog/2013/2013-04-14-py3status-v0-8/#threaded-modules-execution","text":"This was the major thing I was not happy with : when a user-written module was executed for injection, the time it took to get its response would cause py3status to stop updating the bar. This means that if you had a database call to make to get some stuff you need displayed on the bar and it took 10 seconds, py3status was sleeping for those 10 seconds to update the bar ! This behavior could cause some delays in the clock ticking for example. I decided to offload all of the modules' detection and execution to a thread to solve this problem. To be frank, this also helped to rationalize the code better as well. No more delays and a cleaner handling is what you get, stuff will start appending themselves whatever the time they take to execute !","title":"Threaded modules' execution"},{"location":"Tech%20Blog/2013/2013-04-14-py3status-v0-8/#python3","text":"It was about time the examples available on py3status would also work using python3.","title":"Python3"},{"location":"Tech%20Blog/2013/2013-04-16-san-francisco-chinatown/","text":"","title":"San Francisco : chinatown"},{"location":"Tech%20Blog/2013/2013-04-18-mongodb-v2-4-2-released/","text":"After the security issue related bumps of the previous releases which happened last weeks it was about time 10gen released a 2.4.x fixing the following issues: Fix for upgrading sharded clusters TTL assertion on replica set secondaries Several V8 memory leak and performance fixes High volume connection crash I guess everything listed above would have affected our cluster at work so I'm glad we've been patient on following-up this release :) See the changelog for details.","title":"mongoDB v2.4.2 released"},{"location":"Tech%20Blog/2013/2013-04-18-py3status-v0-9/","text":"First of all py3status is on pypi ! You can now install it with the simple and usual : $ pip install py3status This new version features my first pull request from @ Fandekasp who kindly wrote a pomodoro module which helps this technique's adepts by having a counter on their bar. I also fixed a few glitches on module injection and some documentation.","title":"py3status v0.9"},{"location":"Tech%20Blog/2013/2013-04-18-san-francisco-streets/","text":"","title":"San Francisco : streets"},{"location":"Tech%20Blog/2013/2013-04-24-hello-gentoo-planet/","text":"Hey Gentoo folks ! I finally followed a friend's advice and stepped into the Gentoo Planet and Universe feeds. I hope my modest contributions will help and be of interest to some of you readers. As you'll see, I don't talk only about Gentoo but also about photography and technology more generally. I also often post about the packages I maintain or I have an interest in to highlight their key features or bug fixes.","title":"Hello Gentoo Planet"},{"location":"Tech%20Blog/2013/2013-04-26-mongodb-and-pacemaker-recent-bumps/","text":"mongoDB 2.4.3 \u00b6 Yet another bugfix release , this new stable branch is surely one of the most quickly iterated I've ever seen. I guess we'll wait a bit longer at work before migrating to 2.4.x. pacemaker 1.1.10_rc1 \u00b6 This is the release of pacemaker we've been waiting for, fixing among other things, the ACL problem which was introduced in 1.1.9 . Andrew and others are working hard to get a proper 1.1.10 out soon, thanks guys. Meanwhile, we (gentoo cluster herd) have been contacted by @Psi-Jack who has offered his help to follow and keep some of our precious clustering packages up to date, I wish our work together will benefit everyone ! All of this is live on portage, enjoy.","title":"mongoDB and Pacemaker recent bumps"},{"location":"Tech%20Blog/2013/2013-04-26-mongodb-and-pacemaker-recent-bumps/#mongodb-243","text":"Yet another bugfix release , this new stable branch is surely one of the most quickly iterated I've ever seen. I guess we'll wait a bit longer at work before migrating to 2.4.x.","title":"mongoDB 2.4.3"},{"location":"Tech%20Blog/2013/2013-04-26-mongodb-and-pacemaker-recent-bumps/#pacemaker-1110_rc1","text":"This is the release of pacemaker we've been waiting for, fixing among other things, the ACL problem which was introduced in 1.1.9 . Andrew and others are working hard to get a proper 1.1.10 out soon, thanks guys. Meanwhile, we (gentoo cluster herd) have been contacted by @Psi-Jack who has offered his help to follow and keep some of our precious clustering packages up to date, I wish our work together will benefit everyone ! All of this is live on portage, enjoy.","title":"pacemaker 1.1.10_rc1"},{"location":"Tech%20Blog/2013/2013-05-16-fujifilm-gf670w/","text":"It's been so long since I switched to film-only photography that I decided a few months ago to sell all my digital equipment. I already own a Nikon FM2 camera which I love but I've to admit that I was and still am totally amazed by the pictures taken by my girlfriend's Rolleiflex 3.5F. The medium format is the kind of rendering I was craving to get and that sooner or later I'd step into the medium format world. Well, I didn't have to wait as when we were in Tokyo to celebrate new year 2013 I fell in love with what was the perfect match between my love for wide angles and medium format film photography : the Fujifilm GF670W ! For my soon to come birthday, I got myself my new toy in advance so I could use it in my upcoming roadtrip around France (I'll talk about it soon, it was awesome). Oddly, the only places in the world where you can get this camera is in the UK and in Japan so I bought it from the very nice guys at Dale photographic . Here is the beast (literally) : Yes, this is a big camera and it comes with a very nice leather case and a lens hood. This is a telemetric camera with a comfortable visor, it accepts 120 and 220 films and is capable of shooting in standard 6x6 and 6x7 ! In the medium format world, the 55mm lens is actually a wide angle one as it is comparable to a 28mm in the usual 24x36 world. Its performances are not crazy on paper with a 4.5 aperture and a shutter speed going from 4s to 1/500s (as fast as a 1956 Rolleiflex) but the quality is just stunning as it's sharp and offers a somewhat inexistant chromatic abberation. Want proof ? These are some of my first roll's shoots uploaded at full resolution :","title":"Fujifilm GF670W"},{"location":"Tech%20Blog/2013/2013-05-17-squid-proxy-blocking-download-of-some-file-extensions/","text":"It is a common request in squid to have it block downloading certain files based on their extension in the url path. A quick look at google's results on the subject apparently gives us the solution to get this done easily by squid. The common solution is to create an ACL file listing regular expressions of the extensions you want to block and then apply this to your http_access rules. blockExtensions.acl \u00b6 \\.exe$ squid.conf \u00b6 acl blockExtensions urlpath_regex -i \"/etc/squid/blockExtensions.acl\" [...] http_access allow localnet !blockExtensions Unfortunately this is not enough to prevent users from downloading .exe files. The mistake here is that we assume that the URL will strictly finish by the extension we want to block, consider the two examples below : http://download.com/badass.exe // will be DENIED as expected http://download.com/badass.exe? // WON'T be denied as it does not match the regex ! Squid uses the extended regex processor which is the same as egrep. So we need to change our blockExtensions.acl file to handle the possible ?whatever string which may be trailing our url_path . Here's the solution to handle all the cases : blockExtensions.acl \u00b6 \\.exe(\\?.*)?$ \\.msi(\\?.*)?$ \\.msu(\\?.*)?$ \\.torrent(\\?.*)?$ You will still be hated for limiting people's need to download and install shit on their Windows but you implemented it the right way and no script kiddie can brag about bypassing you ;)","title":"Squid proxy : blocking download of some file extensions"},{"location":"Tech%20Blog/2013/2013-05-17-squid-proxy-blocking-download-of-some-file-extensions/#blockextensionsacl","text":"\\.exe$","title":"blockExtensions.acl"},{"location":"Tech%20Blog/2013/2013-05-17-squid-proxy-blocking-download-of-some-file-extensions/#squidconf","text":"acl blockExtensions urlpath_regex -i \"/etc/squid/blockExtensions.acl\" [...] http_access allow localnet !blockExtensions Unfortunately this is not enough to prevent users from downloading .exe files. The mistake here is that we assume that the URL will strictly finish by the extension we want to block, consider the two examples below : http://download.com/badass.exe // will be DENIED as expected http://download.com/badass.exe? // WON'T be denied as it does not match the regex ! Squid uses the extended regex processor which is the same as egrep. So we need to change our blockExtensions.acl file to handle the possible ?whatever string which may be trailing our url_path . Here's the solution to handle all the cases :","title":"squid.conf"},{"location":"Tech%20Blog/2013/2013-05-17-squid-proxy-blocking-download-of-some-file-extensions/#blockextensionsacl_1","text":"\\.exe(\\?.*)?$ \\.msi(\\?.*)?$ \\.msu(\\?.*)?$ \\.torrent(\\?.*)?$ You will still be hated for limiting people's need to download and install shit on their Windows but you implemented it the right way and no script kiddie can brag about bypassing you ;)","title":"blockExtensions.acl"},{"location":"Tech%20Blog/2013/2013-05-21-rabbitmq-v3-1-0-released/","text":"EDIT: okay, they just released v3.1.1 so here it goes on portage as well ! highlights \u00b6 relax validation of x-match binding to headers exchange for compatibility with brokers < 3.1.0 fix bug in ack handling for transactional channels that could cause queues to crash fix race condition in cluster autoheal that could lead to nodes failing to re-join the cluster 3.1.1 changelog is here . I've bumped the rabbitMQ message queuing server on portage. This new version comes with quite a nice bunch of bugfixes and features. highlights \u00b6 eager synchronisation of slaves by policy (manual & automatic) cluster \"autoheal\" mode to automatically choose nodes to restart when a partition has occurred cluster \"pause minority\" mode to prefer partition tolerance over availability improved statistics (including charts) in the management plugin quite a bunch of performance improvements some nice memory leaks fixes Read the full changelog .","title":"rabbitMQ : v3.1.1 released"},{"location":"Tech%20Blog/2013/2013-05-21-rabbitmq-v3-1-0-released/#highlights","text":"relax validation of x-match binding to headers exchange for compatibility with brokers < 3.1.0 fix bug in ack handling for transactional channels that could cause queues to crash fix race condition in cluster autoheal that could lead to nodes failing to re-join the cluster 3.1.1 changelog is here . I've bumped the rabbitMQ message queuing server on portage. This new version comes with quite a nice bunch of bugfixes and features.","title":"highlights"},{"location":"Tech%20Blog/2013/2013-05-21-rabbitmq-v3-1-0-released/#highlights_1","text":"eager synchronisation of slaves by policy (manual & automatic) cluster \"autoheal\" mode to automatically choose nodes to restart when a partition has occurred cluster \"pause minority\" mode to prefer partition tolerance over availability improved statistics (including charts) in the management plugin quite a bunch of performance improvements some nice memory leaks fixes Read the full changelog .","title":"highlights"},{"location":"Tech%20Blog/2013/2013-05-25-san-francisco-street-art/","text":"","title":"San Francisco : street art"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/","text":"Load balancing traffic between servers can sometimes lead to headaches depending on your topology and budget. Here I'll discuss how to create a self load balanced cluster of web servers distributing HTTP requests between themselves and serving them at the same time. Yes, this means that you don't need dedicated load balancers ! I will not go into the details on how to configure your kernel for ipvsadm etc since it's already covered enough on the web but instead focus on the challenges and subtleties of achieving a load balancing based only on the realservers themselves. I expect you reader have a minimal knowledge of the terms and usage of ipvsadm and keepalived . The setup \u00b6 Let's start with a scheme and some principles explaining our topology. 3 web servers / realservers (you can do the same using 2) Local subnet : 192.168.0.0/24 LVS forwarding method : DR (direct routing) LVS scheduler : WRR (you can choose your own) VIP : 192.168.0.254 Main interface for VIP : bond0 Let's take a look at what happens as this will explain a lot of why we should configure the servers in a quite special way. black arrow / serving the master server (the one who has the VIP ) receives a HTTP port connection request the load balancing scheduler decides he's the one who'll serve this request the local web server handles the request and replies to the client blue arrow / direct routing / serving the master server receives a HTTP port connection request the load balancing scheduler decides the blue server should handle this request the HTTP packet is given to the blue server as-this (no modification is made on the packet) the blue server receives a packet whose destination IP is the VIP but he doesn't hold the VIP (tricky part) the blue server's web server handles the request and replies to the client IP configuration \u00b6 Almost all the tricky part lies in what needs to be done in order to solve the point #4 of the blue server example. Since we're using direct routing , we need to configure all our servers so they accept packets directed to the VIP even if they don't have it configured on their receiving interface. The solution is to have the VIP configured on the loopback interface (lo) with a host scope on the keepalived BACKUP servers while it is configured on the main interface (bond0) on the keepalived MASTER server. This is what is usually done when you use pacemaker and ldirectord with IPAddr2 but keepalived does not handle this kind of configuration natively. We'll use the notify_master and notify_backup directives of keepalived.conf to handle this : notify_master /etc/keepalived/to_master.sh notify_backup /etc/keepalived/to_backup.sh We'll discuss a few problems to fix before detailing those scripts. The ARP problem \u00b6 Now some of you wise readers will wonder about the ARP cache corruptions which will happen when multiple hosts claim to own the same IP address on the same subnet. Let's fix this problem now then as the kernel does have a way of handling this properly. Basically we'll ask the kernel not to advert the server's MAC address for the VIP on certain conditions using the arp_ignore and arp_announce sysctl. Add those lines on the sysctl.conf of your servers : net.ipv4.conf.all.arp_ignore = 3 net.ipv4.conf.all.arp_announce = 2 Read more about those parameters for the detailed explanation of those values. The IPVS synchronization problem \u00b6 This is another problem arising from the fact that the load balancers are also acting as realservers. When keepalived starts, it spawns a synchronization process on the master and backup nodes so you load balancers' IPVS tables stay in sync. This is needed for a fully transparent fail over as it keeps track of the sessions' persistence so the clients don't get rebalanced when the master goes down. Well, this is the limitation of our setup : clients' HTTP sessions served by the master node will fail if he goes down. But note that the same will happen to the other nodes because we have to get rid of this synchronization to get our setup working. The reason is simple : IPVS table sync conflicts with the actual acceptance of the packet by our loopback set up VIP. Both mechanisms can't coexist together, so you'd better use this setup for stateless (API?) HTTP servers or if you're okay with this eventuality. Final configuration \u00b6 to_master.sh !/bin/bash \u00b6 ip addr del 192.168.0.254/32 dev lo ipvsadm --restore < /tmp/keepalived.ipvs drop the VIP from the loopback interface (it will be setup by keepalived on the master interface) restore the IPVS configuration to_backup.sh !/bin/bash \u00b6 ip addr add 192.168.0.254/32 scope host dev lo ipvsadm --save > /tmp/keepalived.ipvs ipvsadm --clear add the VIP to the loopback interface, scope host keep a copy of the IPVS configuration, if we get to be master, we'll need it back drop the IPVS local config so it doesn't conflict with our own web serving Conclusion \u00b6 Even if it offers some serious benefits, remember the main limitation of this setup : if the master fails, all sessions of your web servers will be lost. So use it mostly for stateless stuff or if you're okay with this. My setup and explanations may have some glitches, feel free to correct me if I'm wrong somewhere.","title":"Using keepalived for a self-balancing cluster"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#the-setup","text":"Let's start with a scheme and some principles explaining our topology. 3 web servers / realservers (you can do the same using 2) Local subnet : 192.168.0.0/24 LVS forwarding method : DR (direct routing) LVS scheduler : WRR (you can choose your own) VIP : 192.168.0.254 Main interface for VIP : bond0 Let's take a look at what happens as this will explain a lot of why we should configure the servers in a quite special way. black arrow / serving the master server (the one who has the VIP ) receives a HTTP port connection request the load balancing scheduler decides he's the one who'll serve this request the local web server handles the request and replies to the client blue arrow / direct routing / serving the master server receives a HTTP port connection request the load balancing scheduler decides the blue server should handle this request the HTTP packet is given to the blue server as-this (no modification is made on the packet) the blue server receives a packet whose destination IP is the VIP but he doesn't hold the VIP (tricky part) the blue server's web server handles the request and replies to the client","title":"The setup"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#ip-configuration","text":"Almost all the tricky part lies in what needs to be done in order to solve the point #4 of the blue server example. Since we're using direct routing , we need to configure all our servers so they accept packets directed to the VIP even if they don't have it configured on their receiving interface. The solution is to have the VIP configured on the loopback interface (lo) with a host scope on the keepalived BACKUP servers while it is configured on the main interface (bond0) on the keepalived MASTER server. This is what is usually done when you use pacemaker and ldirectord with IPAddr2 but keepalived does not handle this kind of configuration natively. We'll use the notify_master and notify_backup directives of keepalived.conf to handle this : notify_master /etc/keepalived/to_master.sh notify_backup /etc/keepalived/to_backup.sh We'll discuss a few problems to fix before detailing those scripts.","title":"IP configuration"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#the-arp-problem","text":"Now some of you wise readers will wonder about the ARP cache corruptions which will happen when multiple hosts claim to own the same IP address on the same subnet. Let's fix this problem now then as the kernel does have a way of handling this properly. Basically we'll ask the kernel not to advert the server's MAC address for the VIP on certain conditions using the arp_ignore and arp_announce sysctl. Add those lines on the sysctl.conf of your servers : net.ipv4.conf.all.arp_ignore = 3 net.ipv4.conf.all.arp_announce = 2 Read more about those parameters for the detailed explanation of those values.","title":"The ARP problem"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#the-ipvs-synchronization-problem","text":"This is another problem arising from the fact that the load balancers are also acting as realservers. When keepalived starts, it spawns a synchronization process on the master and backup nodes so you load balancers' IPVS tables stay in sync. This is needed for a fully transparent fail over as it keeps track of the sessions' persistence so the clients don't get rebalanced when the master goes down. Well, this is the limitation of our setup : clients' HTTP sessions served by the master node will fail if he goes down. But note that the same will happen to the other nodes because we have to get rid of this synchronization to get our setup working. The reason is simple : IPVS table sync conflicts with the actual acceptance of the packet by our loopback set up VIP. Both mechanisms can't coexist together, so you'd better use this setup for stateless (API?) HTTP servers or if you're okay with this eventuality.","title":"The IPVS synchronization problem"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#final-configuration","text":"to_master.sh","title":"Final configuration"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#binbash","text":"ip addr del 192.168.0.254/32 dev lo ipvsadm --restore < /tmp/keepalived.ipvs drop the VIP from the loopback interface (it will be setup by keepalived on the master interface) restore the IPVS configuration to_backup.sh","title":"!/bin/bash"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#binbash_1","text":"ip addr add 192.168.0.254/32 scope host dev lo ipvsadm --save > /tmp/keepalived.ipvs ipvsadm --clear add the VIP to the loopback interface, scope host keep a copy of the IPVS configuration, if we get to be master, we'll need it back drop the IPVS local config so it doesn't conflict with our own web serving","title":"!/bin/bash"},{"location":"Tech%20Blog/2013/2013-05-31-using-keepalived-for-a-self-balancing-cluster/#conclusion","text":"Even if it offers some serious benefits, remember the main limitation of this setup : if the master fails, all sessions of your web servers will be lost. So use it mostly for stateless stuff or if you're okay with this. My setup and explanations may have some glitches, feel free to correct me if I'm wrong somewhere.","title":"Conclusion"},{"location":"Tech%20Blog/2013/2013-06-02-roadtrip-3600/","text":"Deuxi\u00e8me roadtrip en Harley : la c\u00f4te d'Azur et Biarritz. Un vrai tour de France en un peu moins de deux semaines. 3600 kilom\u00e8tres de libert\u00e9 : Paris - Luberon - Gorges du Verdon - Grimaud (30 ans du HOG) - Marseille - Biarritz + les premi\u00e8res photos de vacances avec le GF670W, magique","title":"Roadtrip 3600"},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/","text":"mongodb-2.4.4 \u00b6 Just bumped it to portage and fixed an open bug along. This is yet another bugfix release which backports the switch to the Cyrus SASL2 library for sasl authentication (kerberos). Dependencies were adjusted so you no longer need libgsasl on your systems (remember to depclean). highlights \u00b6 config upgrade fails if collection missing \"key\" field migrate to Cyrus SASL2 library for sasl authentication rollback files missing after rollback pymongo-2.5.2 \u00b6 This one is important to note and I strongly encourage you to upgrade asap as it fixes an important security bug (CVE-2013-2132). I've almost dropped all other versions from tree anyway... highlights 2.5.x \u00b6 support GSSAPI (kerberos) authentication support for SSL certificate validation with hostname matching support for delegated and role based authentication mongodb-2.5.x dev \u00b6 What's cooking for the next 2.6 releases ? Let's take a quick look as of today. background indexing on secondaries (hell yes!) new implementation of external sort add support for building from source with particular C++11 compilers (will fix a gentoo bug reported quite a long time ago) mongod automatically continues in progress index builds following restart","title":"mongoDB : latest releases"},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#mongodb-244","text":"Just bumped it to portage and fixed an open bug along. This is yet another bugfix release which backports the switch to the Cyrus SASL2 library for sasl authentication (kerberos). Dependencies were adjusted so you no longer need libgsasl on your systems (remember to depclean).","title":"mongodb-2.4.4"},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#highlights","text":"config upgrade fails if collection missing \"key\" field migrate to Cyrus SASL2 library for sasl authentication rollback files missing after rollback","title":"highlights"},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#pymongo-252","text":"This one is important to note and I strongly encourage you to upgrade asap as it fixes an important security bug (CVE-2013-2132). I've almost dropped all other versions from tree anyway...","title":"pymongo-2.5.2"},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#highlights-25x","text":"support GSSAPI (kerberos) authentication support for SSL certificate validation with hostname matching support for delegated and role based authentication","title":"highlights 2.5.x"},{"location":"Tech%20Blog/2013/2013-06-05-mongodb-latest-releases/#mongodb-25x-dev","text":"What's cooking for the next 2.6 releases ? Let's take a quick look as of today. background indexing on secondaries (hell yes!) new implementation of external sort add support for building from source with particular C++11 compilers (will fix a gentoo bug reported quite a long time ago) mongod automatically continues in progress index builds following restart","title":"mongodb-2.5.x dev"},{"location":"Tech%20Blog/2013/2013-06-13-vertigo/","text":"GF670W","title":"Vertigo"},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/","text":"A few months ago I had the chance to get to know about one of the upcoming HP innovation in the server architecture : the Moonshot project. Now that it is public, I thought I'd take some time to talk about it as I'm convinced this is something big enough to change the way we see datacenter infrastructures and servers in general. I'll do my best to keep it short and understandable so if you want deeper technical insights, fell free to ask or search around. The nowdays servers \u00b6 As a reminder, that's what a standard server looks like today : We call them pizza boxes for their flat and tasty aspect. Then in datacenters we put them in enclosures we call racks which look like this : Now the basic stuff to understand and keep in mind is that those racks can typically hold 42 standard servers like the one above. Datacenters are just big hangars where you store and cool hundreds/thousands of those racks. The Moonshot project \u00b6 The more processing power you need, the more racks you need, the more datacenters you need. Think of facebook or google and their enormous amount of servers/racks/datacenters around the world. Every new device (PC/tablet/smartphone) activated is a new client to an always-growing infrastructure of those powerhouses. Basically, there's a limit in the number of full datacenters you can build and operate eventually (not to mention powering them up) but there's worse : the new devices/clients growth is higher than our datacenter building/powering capabilites. The Moonshot project is one of HP's response to this challenge : permit businesses to accommodate and serve this rapidly growing demand of devices/clients without the datacenter model collapsing. Their method ? Invent a new server architecture from scratch. The cartridges are back ! \u00b6 No your Master System is still out of date... But HP's approach to getting more servers in less space while consuming way less power resides in turning the pizza-box above into a cartridge which looks like this : No black magic involved : you can now store 45 servers in 4,3 units of space . Based on their calculation, if you want the same computing power as you would have with standard pizza-boxes you'd need only one full rack of those new servers versus 4 to 6 racks filled with standard ones (depending on their config). Overall gain factors are huge : space divided by 4 to 6 energy divided by 6 to 2 cabling divided by 26 to 18 not to mention the time saved by technicians to put everything up That's what the beast looks like : Of course, they have integrated redundant switches and all the flavors of modern enclosures. The right cartridge at the right place \u00b6 Over the year, HP will launch a series of cartridges with their own specifications (RAM/HDD/CPU) which should be used to meet specific needs and designs. They can also accommodate your needs by designing your own server-cartridges if you're in a hurry of course. As a DevOps, I love this idea of the hardware being designed and used upon your software's architecture because that's closer to real efficiency and will lead both developers and IT architects to distributed and massively scaling designs. I'll conclude with the last thing you need to understand about this technology : it does not fit everyone's need. Moonshot will not take over the world and replace every server around, instead it should be used as a hardware matching a real software design.","title":"HP Moonshot"},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/#the-nowdays-servers","text":"As a reminder, that's what a standard server looks like today : We call them pizza boxes for their flat and tasty aspect. Then in datacenters we put them in enclosures we call racks which look like this : Now the basic stuff to understand and keep in mind is that those racks can typically hold 42 standard servers like the one above. Datacenters are just big hangars where you store and cool hundreds/thousands of those racks.","title":"The nowdays servers"},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/#the-moonshot-project","text":"The more processing power you need, the more racks you need, the more datacenters you need. Think of facebook or google and their enormous amount of servers/racks/datacenters around the world. Every new device (PC/tablet/smartphone) activated is a new client to an always-growing infrastructure of those powerhouses. Basically, there's a limit in the number of full datacenters you can build and operate eventually (not to mention powering them up) but there's worse : the new devices/clients growth is higher than our datacenter building/powering capabilites. The Moonshot project is one of HP's response to this challenge : permit businesses to accommodate and serve this rapidly growing demand of devices/clients without the datacenter model collapsing. Their method ? Invent a new server architecture from scratch.","title":"The Moonshot project"},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/#the-cartridges-are-back","text":"No your Master System is still out of date... But HP's approach to getting more servers in less space while consuming way less power resides in turning the pizza-box above into a cartridge which looks like this : No black magic involved : you can now store 45 servers in 4,3 units of space . Based on their calculation, if you want the same computing power as you would have with standard pizza-boxes you'd need only one full rack of those new servers versus 4 to 6 racks filled with standard ones (depending on their config). Overall gain factors are huge : space divided by 4 to 6 energy divided by 6 to 2 cabling divided by 26 to 18 not to mention the time saved by technicians to put everything up That's what the beast looks like : Of course, they have integrated redundant switches and all the flavors of modern enclosures.","title":"The cartridges are back !"},{"location":"Tech%20Blog/2013/2013-06-15-hp-moonshot/#the-right-cartridge-at-the-right-place","text":"Over the year, HP will launch a series of cartridges with their own specifications (RAM/HDD/CPU) which should be used to meet specific needs and designs. They can also accommodate your needs by designing your own server-cartridges if you're in a hurry of course. As a DevOps, I love this idea of the hardware being designed and used upon your software's architecture because that's closer to real efficiency and will lead both developers and IT architects to distributed and massively scaling designs. I'll conclude with the last thing you need to understand about this technology : it does not fit everyone's need. Moonshot will not take over the world and replace every server around, instead it should be used as a hardware matching a real software design.","title":"The right cartridge at the right place"},{"location":"Tech%20Blog/2013/2013-06-20-py3status-v0-12/","text":"I'm glad to announce a new release of py3status ! I would like to thank @drahier for reporting an issue he found after suspending his computer. I took the opportunity to add a feature which will be helpful at work since we now have a local package installing some modules we share between colleagues (thx to @lujeni). changelog \u00b6 bugfix : don't hang horribly when resuming from a suspend (was caused by an IOError exception which could occur when reading/writing to a suspending system). feature : allow multiple -i include_path options to be passed and handle all the modules thus found. feature : do not try to execute private and special methods on user-written Py3status' classes.","title":"py3status v0.12"},{"location":"Tech%20Blog/2013/2013-06-20-py3status-v0-12/#changelog","text":"bugfix : don't hang horribly when resuming from a suspend (was caused by an IOError exception which could occur when reading/writing to a suspending system). feature : allow multiple -i include_path options to be passed and handle all the modules thus found. feature : do not try to execute private and special methods on user-written Py3status' classes.","title":"changelog"},{"location":"Tech%20Blog/2013/2013-06-24-py3status-v0-13/","text":"Yep, a quick bump of py3status to fix a bug reported by @lathan using python3. The private and special methods detection didn't work on python3 because the class methods are reported differently from python2. A special thanks to @bloodred and @drahier too for debugging, testing and proposing some solutions to this problem. First time I see multiple members of what I could humbly call the py3status community working together, it's very nice of you guys !","title":"py3status v0.13"},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/","text":"Quick catch-up on recent bumps I've made for rabbitMQ and mongoDB before posting about EuroPython2013 ! mongoDB v2.4.5 \u00b6 The 2.4 branch is definitely the most bugged one I've seen so far. We waited until 2.4.4 at work before migrating but this was not enough and we got hit for a few weeks by a bug which finally got fixed in 2.4.5. Also this release fixes two security bugs so you're strongly advised to upgrade. highlights \u00b6 CVE-2013-4650 - Improperly grant user system privileges on databases other than \u201clocal\u201d. CVE-2013-4650 - Remotely triggered segmentation fault in Javascript engine. config server performance improvements improve initial sync resilience to network failure (that's the one) flask-pymongo v0.3.0 \u00b6 @dcrosta finally had the time to take care of my pull request for flask-pymongo . We now rely on pymongo's MongoClient parameters' validation instead of implementing them again on flask-pymongo and added connect and socket timeout options. rabbitMQ v3.1.3 \u00b6 A roll-up bugfix release mainly. highlights \u00b6 fix startup failure when using SSL with Erlang/OTP R16B01 fix queue crash requeuing in-memory messages (since 2.7.0) fix leak affecting HA/mirrored queues (since 3.0.0) fix bug that lead to incorrect reporting of accumulated stats (since 3.1.2)","title":"mongoDB v2.4.5 & rabbitMQ v3.1.3"},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#mongodb-v245","text":"The 2.4 branch is definitely the most bugged one I've seen so far. We waited until 2.4.4 at work before migrating but this was not enough and we got hit for a few weeks by a bug which finally got fixed in 2.4.5. Also this release fixes two security bugs so you're strongly advised to upgrade.","title":"mongoDB v2.4.5"},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#highlights","text":"CVE-2013-4650 - Improperly grant user system privileges on databases other than \u201clocal\u201d. CVE-2013-4650 - Remotely triggered segmentation fault in Javascript engine. config server performance improvements improve initial sync resilience to network failure (that's the one)","title":"highlights"},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#flask-pymongo-v030","text":"@dcrosta finally had the time to take care of my pull request for flask-pymongo . We now rely on pymongo's MongoClient parameters' validation instead of implementing them again on flask-pymongo and added connect and socket timeout options.","title":"flask-pymongo v0.3.0"},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#rabbitmq-v313","text":"A roll-up bugfix release mainly.","title":"rabbitMQ v3.1.3"},{"location":"Tech%20Blog/2013/2013-07-10-mongodb-v2-4-5-rabbitmq-v3-1-3/#highlights_1","text":"fix startup failure when using SSL with Erlang/OTP R16B01 fix queue crash requeuing in-memory messages (since 2.7.0) fix leak affecting HA/mirrored queues (since 3.0.0) fix bug that lead to incorrect reporting of accumulated stats (since 3.1.2)","title":"highlights"},{"location":"Tech%20Blog/2013/2013-07-11-paris-siege-du-pcf/","text":"GF670W","title":"Paris : si\u00e8ge du PCF"},{"location":"Tech%20Blog/2013/2013-08-02-rsyslog-v7-4-3-released/","text":"It's been more than 3 months since the last version bump of rsyslog, I'm sorry about that (special kudos to @Opportunist for his patience). Since June 6th, we have a new stable 7.4 branch of rsyslog containing all the bugfixes and improvements made in the 7.3 dev branch. So here comes the catch-up to current v7.4.3. Please note that as upstream do not support older branches, I will soon remove them from portage and close related bugs. highlights \u00b6 tons of bugfixes I won't bother to list imjournal: add ratelimiting capability max number of templates for plugin use has been increased to five added support for encrypting log files omhiredis: added support for redis pipeline support","title":"rsyslog : v7.4.3 released"},{"location":"Tech%20Blog/2013/2013-08-02-rsyslog-v7-4-3-released/#highlights","text":"tons of bugfixes I won't bother to list imjournal: add ratelimiting capability max number of templates for plugin use has been increased to five added support for encrypting log files omhiredis: added support for redis pipeline support","title":"highlights"},{"location":"Tech%20Blog/2013/2013-08-13-rabbitmq-v3-1-4-released/","text":"Quick post for a quick bugfix release of the rabbitMQ server. Please note that I dropped the remaining 3.0.4 version in tree while doing this bump. highlights \u00b6 security fix : ensure DLX declaration checks for publish permission (since 2.8.0) security fix : update to a later version of Mochiweb that fixes a directory traversal vulnerability allowing arbitrary file access on Windows (since 2.1.0) fix resource leak with mirrored queues when whole clusters stop (since 3.0.0) fix queue crash in mirrored queue handling of messages during promotion (since 2.6.0) fix mirrored queue sync failure in the presence of un-acked messages not at the head of the queue (since 3.1.0) allow hipe compilation on Erlang R16B01 make `rabbitmqctl join_cluster' idempotent (since 3.0.0) improve `rabbitmqctl cluster_status' handling of partition info when cluster nodes are in the process of stopping (since 3.1.0)","title":"rabbitMQ : v3.1.4 released"},{"location":"Tech%20Blog/2013/2013-08-13-rabbitmq-v3-1-4-released/#highlights","text":"security fix : ensure DLX declaration checks for publish permission (since 2.8.0) security fix : update to a later version of Mochiweb that fixes a directory traversal vulnerability allowing arbitrary file access on Windows (since 2.1.0) fix resource leak with mirrored queues when whole clusters stop (since 3.0.0) fix queue crash in mirrored queue handling of messages during promotion (since 2.6.0) fix mirrored queue sync failure in the presence of un-acked messages not at the head of the queue (since 3.1.0) allow hipe compilation on Erlang R16B01 make `rabbitmqctl join_cluster' idempotent (since 3.0.0) improve `rabbitmqctl cluster_status' handling of partition info when cluster nodes are in the process of stopping (since 3.1.0)","title":"highlights"},{"location":"Tech%20Blog/2013/2013-08-16-rabbitmq-v3-1-5-released/","text":"Looks like rabbitMQ upstream likes to bump their stuff right after I catch-up with my bumps :) You are strongly advised to upgrade to this version since it fixes a quite important bug introduced by 3.1.4. highlights \u00b6 fix crash in the delegate mechanism leading to various crashes, and intra-cluster incompatibility between RabbitMQ 3.1.4 and other members of the 3.1.x series (since 3.1.4) prevent (harmless) errors being logged when pausing in pause_minority mode (since 3.1.0)","title":"rabbitMQ : v3.1.5 released"},{"location":"Tech%20Blog/2013/2013-08-16-rabbitmq-v3-1-5-released/#highlights","text":"fix crash in the delegate mechanism leading to various crashes, and intra-cluster incompatibility between RabbitMQ 3.1.4 and other members of the 3.1.x series (since 3.1.4) prevent (harmless) errors being logged when pausing in pause_minority mode (since 3.1.0)","title":"highlights"},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/","text":"mongodb-2.4.6 \u00b6 The folks at 10gen discovered a severe bug in the mongoDB chunk migration process on sharded environments. Basically, depending of the size of your documents, there was a chance that some get lost during data migration ! Relax tho, this case affected only the chunks containing documents which size are in the range of 16,776,185 and 16,777,216 bytes (inclusive) so this means that if you don't have quite big documents in your cluster, you have not been affected by this bug. Still, as a maintainer and production user of mongoDB, this is not the kind of news I like to hear especially when thinking of you Gentoo users. On top of this, I had the bad surprise to experience again the stale replication bug that was supposed to be fixed on 2.4.5 on my production cluster. So I decided this was time for a major cleanup of the mongoDB ebuilds in portage to make sure we're not shipping known broken versions of mongoDB to you guys. I thus : dropped all obsolete <mongodb-2.2 ebuilds (I warned about this quite some time ago now) dropped the known bugged versions of mongodb (<2.2.6 and <2.4.6) cleaned up all obsolete files in $FILESDIR added, on the v2.4.6 ebuild, the embedded-v8 USE flag as a user convenience so you can now have packages requiring v8-3.19 and mongodb installed on your machine. I added an explicit warning about this as this is not the way to go on normal usage as this is against Gentoo policy. *mongodb-2.4.6 (21 Aug 2013) *mongodb-2.2.6 (21 Aug 2013) 21 Aug 2013; Ultrabug ultrabug@gentoo.org -mongodb-2.0.7-r1.ebuild, -mongodb-2.0.7-r2.ebuild, -mongodb-2.0.8-r1.ebuild, -mongodb-2.0.8-r2.ebuild, -mongodb-2.2.0-r1.ebuild, -mongodb-2.2.0-r2.ebuild, -mongodb-2.2.4.ebuild, +mongodb-2.2.6.ebuild, -mongodb-2.4.5.ebuild, -mongodb-2.4.6_rc1.ebuild, +mongodb-2.4.6.ebuild, -files/mongodb-1.8.5-fix-smokepy.patch, -files/mongodb-1.8-fix-scons.patch, -files/mongodb-2.2-fix-scons.patch, -files/mongodb-2.2-fix-sconscript.patch, -files/mongodb-2.4.4-fix-sharedclient.patch, -files/mongodb.initd, -files/mongodb-linux3.patch, -files/mongos.initd, metadata.xml: version bump, add embedded-v8 USE, drop critically bugged versions, drop obsolete versions, filesdir cleanup I understand some people may still need some of those ebuilds so if that's the case, just shout and I'll gladly add them to my overlay so you can still use them easily. other highlights \u00b6 Improved replication robustness in presence of high network latency Resolved replica set initial sync issue on certain virtualized platforms Resolved sharding migration issue that produced excessive small chunks Resolved C++ client shutdown issues pymongo-2.6 \u00b6 This one is quite interesting as it brings both new and improved features as well as some bug fixes. Also note that they fixed some gevent compatibility stuff. highlights / explanations \u00b6 The max_pool_size option actually means what it says now. Pymongo will open at most this number of sockets to your servers. Do remember that if you share a connection between threads, then your ( max_pool_size +1) thread will wait for a socket to be freed before being able to process your command. waitQueueMultiple and waitQueueTimeoutMS options will help you define how much and how long you want a process to wait for a socket to be available before raising an exception. Pymongo automatically splits batch inserts into 48MB chunks so you don't have to worry about pushing a huge list of documents for insertion. Support for aggregation cursors (for use with dev version 2.5.1, not used on production now) Support for exhaust cursors. When you queried a large amount of data, the client had to ask the server for each batch of results. An exhaust cursor will instead stream batches to the client as quick as possible. This make pulling large sets of data faster and more reliable than before ! You can see the full extend of this bump on the pymongo Jira .","title":"mongoDB v2.4.6 & pymongo v2.6"},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/#mongodb-246","text":"The folks at 10gen discovered a severe bug in the mongoDB chunk migration process on sharded environments. Basically, depending of the size of your documents, there was a chance that some get lost during data migration ! Relax tho, this case affected only the chunks containing documents which size are in the range of 16,776,185 and 16,777,216 bytes (inclusive) so this means that if you don't have quite big documents in your cluster, you have not been affected by this bug. Still, as a maintainer and production user of mongoDB, this is not the kind of news I like to hear especially when thinking of you Gentoo users. On top of this, I had the bad surprise to experience again the stale replication bug that was supposed to be fixed on 2.4.5 on my production cluster. So I decided this was time for a major cleanup of the mongoDB ebuilds in portage to make sure we're not shipping known broken versions of mongoDB to you guys. I thus : dropped all obsolete <mongodb-2.2 ebuilds (I warned about this quite some time ago now) dropped the known bugged versions of mongodb (<2.2.6 and <2.4.6) cleaned up all obsolete files in $FILESDIR added, on the v2.4.6 ebuild, the embedded-v8 USE flag as a user convenience so you can now have packages requiring v8-3.19 and mongodb installed on your machine. I added an explicit warning about this as this is not the way to go on normal usage as this is against Gentoo policy. *mongodb-2.4.6 (21 Aug 2013) *mongodb-2.2.6 (21 Aug 2013) 21 Aug 2013; Ultrabug ultrabug@gentoo.org -mongodb-2.0.7-r1.ebuild, -mongodb-2.0.7-r2.ebuild, -mongodb-2.0.8-r1.ebuild, -mongodb-2.0.8-r2.ebuild, -mongodb-2.2.0-r1.ebuild, -mongodb-2.2.0-r2.ebuild, -mongodb-2.2.4.ebuild, +mongodb-2.2.6.ebuild, -mongodb-2.4.5.ebuild, -mongodb-2.4.6_rc1.ebuild, +mongodb-2.4.6.ebuild, -files/mongodb-1.8.5-fix-smokepy.patch, -files/mongodb-1.8-fix-scons.patch, -files/mongodb-2.2-fix-scons.patch, -files/mongodb-2.2-fix-sconscript.patch, -files/mongodb-2.4.4-fix-sharedclient.patch, -files/mongodb.initd, -files/mongodb-linux3.patch, -files/mongos.initd, metadata.xml: version bump, add embedded-v8 USE, drop critically bugged versions, drop obsolete versions, filesdir cleanup I understand some people may still need some of those ebuilds so if that's the case, just shout and I'll gladly add them to my overlay so you can still use them easily.","title":"mongodb-2.4.6"},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/#other-highlights","text":"Improved replication robustness in presence of high network latency Resolved replica set initial sync issue on certain virtualized platforms Resolved sharding migration issue that produced excessive small chunks Resolved C++ client shutdown issues","title":"other highlights"},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/#pymongo-26","text":"This one is quite interesting as it brings both new and improved features as well as some bug fixes. Also note that they fixed some gevent compatibility stuff.","title":"pymongo-2.6"},{"location":"Tech%20Blog/2013/2013-08-21-mongodb-v2-4-6-pymongo-v2-6/#highlights-explanations","text":"The max_pool_size option actually means what it says now. Pymongo will open at most this number of sockets to your servers. Do remember that if you share a connection between threads, then your ( max_pool_size +1) thread will wait for a socket to be freed before being able to process your command. waitQueueMultiple and waitQueueTimeoutMS options will help you define how much and how long you want a process to wait for a socket to be available before raising an exception. Pymongo automatically splits batch inserts into 48MB chunks so you don't have to worry about pushing a huge list of documents for insertion. Support for aggregation cursors (for use with dev version 2.5.1, not used on production now) Support for exhaust cursors. When you queried a large amount of data, the client had to ask the server for each batch of results. An exhaust cursor will instead stream batches to the client as quick as possible. This make pulling large sets of data faster and more reliable than before ! You can see the full extend of this bump on the pymongo Jira .","title":"highlights / explanations"},{"location":"Tech%20Blog/2013/2013-08-30-py3status-v1-0/","text":"I'm glad to announce the release of py3status v1.0 ! This version features a lot of stuff that I wanted to add to py3status for a long time and which I had in a small todo floating around. After the recent i3wm 4.6 release, I found it the perfect time to implement them while benefiting from the new click event support from i3bar . The idea of allowing py3status users to have their modules respond to clicks got my head fuzzing and I started slowly to implement my whole todo while adding some great new features thanks to the enhanced i3bar protocol . I ended up rewriting (again) completely (and slowly) py3status :) But it's for its own good, and yours hopefully. I strongly encourage you to have a look at the empty_class.py and pomodoro.py examples as they showcase the new on_click system. You can already benefit from this bump without modifying your modules thanks to the default middle click event which forces a refresh of the module's method you click on (handy isn't it?). changelog \u00b6 support for i3bar click_events , they're dispatched to user-written py3status classes based on their name/instance add support for on_click methods in user-written modules to handle i3bar click_events (see the pomodoro example) default is to clear the method's cache (force a refresh) if you middle click (button 2) on a method's output and the module does not support click_events rewrite pomodoro example to showcase the on_click usage use i3-nagbar to display warnings/errors to the user and also log them to syslog new user-written module output ordering mechanism is more intuitive as it uses strictly numeric then alphabetical sorting use select/poll() to implement a non-blocking I/O reading mechanism on threads new Events thread is responsible for reading i3bar JSONs and dispatching them to the correct user module (click_events) each user-written module is started and executed in its own thread remove the pointless -d option add a --debug option to be verbose in syslog (useful for debugging your modules) add a real CHANGELOG add a proper LICENSE file make sure all examples are PEP8 compatible update the empty_class example to explain on_click and kill usage Note for Gentoo users : starting with this release, py3status is now available in portage so you don't need my overlay anymore.","title":"py3status v1.0"},{"location":"Tech%20Blog/2013/2013-08-30-py3status-v1-0/#changelog","text":"support for i3bar click_events , they're dispatched to user-written py3status classes based on their name/instance add support for on_click methods in user-written modules to handle i3bar click_events (see the pomodoro example) default is to clear the method's cache (force a refresh) if you middle click (button 2) on a method's output and the module does not support click_events rewrite pomodoro example to showcase the on_click usage use i3-nagbar to display warnings/errors to the user and also log them to syslog new user-written module output ordering mechanism is more intuitive as it uses strictly numeric then alphabetical sorting use select/poll() to implement a non-blocking I/O reading mechanism on threads new Events thread is responsible for reading i3bar JSONs and dispatching them to the correct user module (click_events) each user-written module is started and executed in its own thread remove the pointless -d option add a --debug option to be verbose in syslog (useful for debugging your modules) add a real CHANGELOG add a proper LICENSE file make sure all examples are PEP8 compatible update the empty_class example to explain on_click and kill usage Note for Gentoo users : starting with this release, py3status is now available in portage so you don't need my overlay anymore.","title":"changelog"},{"location":"Tech%20Blog/2013/2013-09-03-pacemaker-v1-1-10-corosync-v2-3-1/","text":"More than 5 months since the last bump of pacemaker. I'm glad that @beekhof did release the final pacemaker-1.1.10 and that the officially stable corosync got bumped to 2.3.1. The changelogs are quite heavy so I won't go into details about them but they both have quite a nice bunch of bugfixes and compatibility features. That's why I'm hoping we should soon be able to fix bug #429416 and drop corosync hard mask. Hopefully some users such as @pvsa will give us some valuable feedback which will allow us to do it smoothly. changelog \u00b6 pacemaker-1.1.10 corosync-2.3.1","title":"pacemaker v1.1.10 & corosync v2.3.1"},{"location":"Tech%20Blog/2013/2013-09-03-pacemaker-v1-1-10-corosync-v2-3-1/#changelog","text":"pacemaker-1.1.10 corosync-2.3.1","title":"changelog"},{"location":"Tech%20Blog/2013/2013-09-10-rsyslog-v7-4-4-released/","text":"Contribution . This must be what I love the most and one of the more rewarding thing in the Open Source community. I always found it more natural with Gentoo as you're really close to the source code so we (devs and users) can see and propose fixes natively to upstream. I known some devs of cool upstreams (mongoDB) do use Gentoo on their developing box because of this close-to-source philosophy. I don't know if Rainer does, but I'm glad our contribution (thx to @hwoarang) has been merged to the newer rsyslog. I'm glad to say that we now need no patch at all to build rsyslog on Gentoo ! Long time warning had been issued, I took the opportunity of this bump to clean and drop older and unsupported versions of rsyslog . The only stable branch remaining is v7 from now on. I also made a step towards systemd integration thanks to @slyfox. highlights \u00b6 make rsyslog use the new json-c pkgconfig file if available. Thanks to the Gentoo team for the patches. bugfix: imfile parameter \u201cpersistStateInterval\u201d was unusable due to a case typo in imfile; work-around was to use legacy config bugfix: slightly malformed SMTP handling in ommail bugfix: segfault in omprog if no template was provided (now dflt is used) bugfix: segfault in ompipe if no template was provided (now dflt is used) bugfix: segfault in omsnmp if no template was provided (now dflt is used) bugfix: some omsnmp optional config params were flagged as mandatory","title":"rsyslog : v7.4.4 released"},{"location":"Tech%20Blog/2013/2013-09-10-rsyslog-v7-4-4-released/#highlights","text":"make rsyslog use the new json-c pkgconfig file if available. Thanks to the Gentoo team for the patches. bugfix: imfile parameter \u201cpersistStateInterval\u201d was unusable due to a case typo in imfile; work-around was to use legacy config bugfix: slightly malformed SMTP handling in ommail bugfix: segfault in omprog if no template was provided (now dflt is used) bugfix: segfault in ompipe if no template was provided (now dflt is used) bugfix: segfault in omsnmp if no template was provided (now dflt is used) bugfix: some omsnmp optional config params were flagged as mandatory","title":"highlights"},{"location":"Tech%20Blog/2013/2013-10-15-wonderlands-roadtrip/","text":"A part of me is still there, in the wild. Our USA West Coast and National Parks roadtrip was amazing, surely the most stunning we ever had so far with such a strong feeling of freedom. I'll give more details about it along with the next photo posts.","title":"Wonderlands roadtrip"},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/","text":"Now that I'm back I've bumped some of the sys-cluster packages. Users of keepalived will be interested in this since it was more than a year that upstream released a version. keepalived-1.2.8 \u00b6 This is a big and long awaited one. It features major enhancements, features and bug fixes. The changelog is pretty huge but here are some quick points which I particulary liked (biased view warning) : Revisited the whole code to use posix declaration style Boon Ang fixed comparison of primary IP addresses. If a router in the master state receives an advertisement with priority equal to the local priority, it must also compare the primary IP addresses (RFC 3768, section 6.4.3). The code to handle this was comparing two IP addresses with different byte-ordering, resulting in multiple routers in the master state . This patches resolves the problem by coverting the local primary IP address to network byte order for the comparison. Henrique Mecking fixed memory leak in libipvs Willy Tarreau and Ryan O'Hara add the ability to use VRRP over unicast . Unicast IP addresses may be specified for each VRRP instance with the 'unicast_peer' configuration keyword. When a VRRP instance has one or more unicast IP address defined, VRRP advertisements will be sent to each of those addresses. Unicast IP addresses may be either IPv4 or IPv6. If you are planing to use this option, ensure every ip addresses present in unicast_peer configuration block do not belong to the same router/box. Otherwise it will generate duplicate packet at reception point. crmsh-1.2.6 \u00b6 Many bug fixes with better performances for this release . This is quite impressive, good work upstream ! corosync-2.3.2 \u00b6 This one is about supporting live config reloading and fix high CPU usage when idle. See the release notes . Soon to come \u00b6 The resource-agents v3.9.6 and cluster-glue v1.0.12 should be released by their upstream pretty soon, stay tuned.","title":"Latest cluster releases"},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/#keepalived-128","text":"This is a big and long awaited one. It features major enhancements, features and bug fixes. The changelog is pretty huge but here are some quick points which I particulary liked (biased view warning) : Revisited the whole code to use posix declaration style Boon Ang fixed comparison of primary IP addresses. If a router in the master state receives an advertisement with priority equal to the local priority, it must also compare the primary IP addresses (RFC 3768, section 6.4.3). The code to handle this was comparing two IP addresses with different byte-ordering, resulting in multiple routers in the master state . This patches resolves the problem by coverting the local primary IP address to network byte order for the comparison. Henrique Mecking fixed memory leak in libipvs Willy Tarreau and Ryan O'Hara add the ability to use VRRP over unicast . Unicast IP addresses may be specified for each VRRP instance with the 'unicast_peer' configuration keyword. When a VRRP instance has one or more unicast IP address defined, VRRP advertisements will be sent to each of those addresses. Unicast IP addresses may be either IPv4 or IPv6. If you are planing to use this option, ensure every ip addresses present in unicast_peer configuration block do not belong to the same router/box. Otherwise it will generate duplicate packet at reception point.","title":"keepalived-1.2.8"},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/#crmsh-126","text":"Many bug fixes with better performances for this release . This is quite impressive, good work upstream !","title":"crmsh-1.2.6"},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/#corosync-232","text":"This one is about supporting live config reloading and fix high CPU usage when idle. See the release notes .","title":"corosync-2.3.2"},{"location":"Tech%20Blog/2013/2013-10-17-latest-cluster-releases/#soon-to-come","text":"The resource-agents v3.9.6 and cluster-glue v1.0.12 should be released by their upstream pretty soon, stay tuned.","title":"Soon to come"},{"location":"Tech%20Blog/2013/2013-10-20-usa-roadtrip-facts-start/","text":"The plan \u00b6 We started our one month roadtrip in Los Angeles where we stayed 3 days to adjust to the -9h time zone shifting. We had planned and rented a SUV for the first 15 days to tour the national parks (just before the shutdown, lucky us) then we rented a motorbike for the last 15 days of our trip to ride the West coast. Needless to say it was an amazing trip, the photos I'll post will show it by themselves. Still, it's always fun and quite stunning to look back at the odometers of the two vehicles we used to see how much miles/km we drove : In the SUV , touring the national parks : 4269 miles / 6870 km With the Harley-Davidson Street Glide , riding the West coast : 2175 miles / 3500 km That's a nice total of 6444 miles / 10 370 km ! It's more than the trip from Paris to LA ;) The start \u00b6 We headed to the Death Valley but took a detour to \"visit\" an abandonned waterpark in the middle of the desert !","title":"USA roadtrip facts & start"},{"location":"Tech%20Blog/2013/2013-10-20-usa-roadtrip-facts-start/#the-plan","text":"We started our one month roadtrip in Los Angeles where we stayed 3 days to adjust to the -9h time zone shifting. We had planned and rented a SUV for the first 15 days to tour the national parks (just before the shutdown, lucky us) then we rented a motorbike for the last 15 days of our trip to ride the West coast. Needless to say it was an amazing trip, the photos I'll post will show it by themselves. Still, it's always fun and quite stunning to look back at the odometers of the two vehicles we used to see how much miles/km we drove : In the SUV , touring the national parks : 4269 miles / 6870 km With the Harley-Davidson Street Glide , riding the West coast : 2175 miles / 3500 km That's a nice total of 6444 miles / 10 370 km ! It's more than the trip from Paris to LA ;)","title":"The plan"},{"location":"Tech%20Blog/2013/2013-10-20-usa-roadtrip-facts-start/#the-start","text":"We headed to the Death Valley but took a detour to \"visit\" an abandonned waterpark in the middle of the desert !","title":"The start"},{"location":"Tech%20Blog/2013/2013-10-22-mongodb-v2-4-7-pymongo-v2-6-3/","text":"First of all, I'd like to point out a quite big change in the Gentoo mongodb package. The Chromium team responsible for the v8 package decided to stop its maintenance as it was too much trouble to be used efficiently as a shared library (mainly due to upstream's breakage behavior). Even tho I don't like bundled libraries on sources, I understand my fellow developers point of view. I've thus been asked and did switch the mongodb ebuild to use the bundled v8 library . This means that mongodb has no more v8 packaging dependency now. The mongodb v2.2.x users are advised that since upstream does not bundle the v8 lib in their source, I dropped the v8 USE flag and support altogether on this version (it's not officially supported anyway) ! This being said, I'll drop the old ebuilds from tree on the next releases iterations. mongodb-2.4.7 \u00b6 Yet another bugfix release on this unfamous 2.4.x series : Fixed over-aggressive caching of V8 Isolates Removed extraneous initial count during mapReduce Cache results of dbhash command Fixed memory leak in aggregation pymongo-2.6.3 \u00b6 BSON parser hardening and fixes in the connection pool mechanism. More info here .","title":"mongoDB v2.4.7 & pymongo v2.6.3"},{"location":"Tech%20Blog/2013/2013-10-22-mongodb-v2-4-7-pymongo-v2-6-3/#mongodb-247","text":"Yet another bugfix release on this unfamous 2.4.x series : Fixed over-aggressive caching of V8 Isolates Removed extraneous initial count during mapReduce Cache results of dbhash command Fixed memory leak in aggregation","title":"mongodb-2.4.7"},{"location":"Tech%20Blog/2013/2013-10-22-mongodb-v2-4-7-pymongo-v2-6-3/#pymongo-263","text":"BSON parser hardening and fixes in the connection pool mechanism. More info here .","title":"pymongo-2.6.3"},{"location":"Tech%20Blog/2013/2013-10-25-drop-down-terminal-in-i3/","text":"One of the reasons I switched from KDE to i3wm is that I love and need terminals. In my field of work you happen to spawn dozens of them and you always end up running out of space / workspaces. Yakuake has been a real ally to me for years as I intensively use a drop-down terminal for sporadic usages. It is hard to match Yakuake's efficiency and ability to split terminals but I couldn't stand all those KDE dependencies anymore; I had to find a great drop-down terminal solution in i3. So I started looking at other drop-down terminals such as Terra and Guake but they didn't fit my low dependencies and features list requirements. Drop-down Terminator \u00b6 My current solution is to take advantage of the floating mode of i3 , use it with my beloved terminator et voil\u00e0 ! Nothing more to install, no extra dependency, using the same shortcuts of my main $TERMINAL and all of its features :) The idea is simple, we'll create a special profile in terminator and have it spawned in floating mode upon i3 start. This profile must cover the following drop-down behaviors : respond to a configurable show/hide key binding present a drop-down terminal at the center of the screen the interface should be dead simple and efficient and support splitting Just edit your terminator configuration in ~/.config/terminator/conf and add : [keybindings] hide_window = F1 Then add the dropdown profile under the [profiles] section : [profiles] [[dropdown]] exit_action = close scrollback_lines = 10000 background_image = None scroll_on_output = False show_titlebar = False That's my minimal config, you can add your own stuff to it as well. Now we only need to configure i3 to spawn this profile at login and have it in floating mode. Modify your i3 config file, usually ~/.i3/config : exec terminator -c dropdown -p dropdown -T \"Le Terminator\" -H --geometry=1550x800 for_window [class=\"Terminator\" instance=\"dropdown\"] floating enable That's as simple as this. EDIT: as per Joe's comment, you can also configure i3 to place your floating Terminator window wherever you want (in his case, top off the screen). This still goes into your i3 config from above : for_window [class=\u201dTerminator\u201d instance=\u201ddropdown\u201d] floating enable move absolute position 0 0 There's still one limitation which I didn't come across yet : Unlike Yakuake, our drop-down terminator has a fixed geometry which you must set in the i3 config above and does not support percentage values. So if you have multiple screens of different resolutions it won't adapt on them based on the screen you want to show your drop-down terminator. So long, Yakuake !","title":"Drop-down terminal in i3"},{"location":"Tech%20Blog/2013/2013-10-25-drop-down-terminal-in-i3/#drop-down-terminator","text":"My current solution is to take advantage of the floating mode of i3 , use it with my beloved terminator et voil\u00e0 ! Nothing more to install, no extra dependency, using the same shortcuts of my main $TERMINAL and all of its features :) The idea is simple, we'll create a special profile in terminator and have it spawned in floating mode upon i3 start. This profile must cover the following drop-down behaviors : respond to a configurable show/hide key binding present a drop-down terminal at the center of the screen the interface should be dead simple and efficient and support splitting Just edit your terminator configuration in ~/.config/terminator/conf and add : [keybindings] hide_window = F1 Then add the dropdown profile under the [profiles] section : [profiles] [[dropdown]] exit_action = close scrollback_lines = 10000 background_image = None scroll_on_output = False show_titlebar = False That's my minimal config, you can add your own stuff to it as well. Now we only need to configure i3 to spawn this profile at login and have it in floating mode. Modify your i3 config file, usually ~/.i3/config : exec terminator -c dropdown -p dropdown -T \"Le Terminator\" -H --geometry=1550x800 for_window [class=\"Terminator\" instance=\"dropdown\"] floating enable That's as simple as this. EDIT: as per Joe's comment, you can also configure i3 to place your floating Terminator window wherever you want (in his case, top off the screen). This still goes into your i3 config from above : for_window [class=\u201dTerminator\u201d instance=\u201ddropdown\u201d] floating enable move absolute position 0 0 There's still one limitation which I didn't come across yet : Unlike Yakuake, our drop-down terminator has a fixed geometry which you must set in the i3 config above and does not support percentage values. So if you have multiple screens of different resolutions it won't adapt on them based on the screen you want to show your drop-down terminator. So long, Yakuake !","title":"Drop-down Terminator"},{"location":"Tech%20Blog/2013/2013-10-27-death-valley/","text":"The Death Valley was our first shock of wilderness and immensity as we were welcomed by strong and sandy winds. We slept in our car at an almost empty campground inside the park. Spending the evening and waking up alone in the desert is something very special, especially there.","title":"Death Valley"},{"location":"Tech%20Blog/2013/2013-10-29-bryce-canyon/","text":"After the Death Valley, we headed North to reach Bryce Canyon. On the way up, we did take a quick detour to cruise Las Vegas by car but didn't stop there. We had a pleasant night in a very old fashioned motel near Bryce Canyon where the tenant asked me if I really was French because \"the French are usually blond and don't have such dark hair as you mister\". I'm willing to bet a great deal on the fact that the vast majority of us are brown/dark haired... Mister blond @Lujeni is NOT representative of us French people ! :)","title":"Bryce Canyon"},{"location":"Tech%20Blog/2013/2013-11-03-north-rim-grand-canyon-the-navajo-indians/","text":"We then headed South towards the North rim of Grand Canyon, unfortunately the nights were already cold out there so the rangers did advise against sleeping in the car. We thus visited Point Imperial and stayed for the sunset. This was our first encounter with the Grand Canyon and it was amazing. It was dark when we got out of there, we had to drive very carefully to avoid the numerous deers along the road. We had planned to sleep in Page but we hit a detour due to road work and had to take a huge detour which took us to Tuba City. I could not stand driving one more mile but the only three hotels of the whole area were full. One person from the front desk of a hotel advised us to go to the nearby Greyhills Inn which remains one of the most exotic experience we had in Indian territory. We slept in a room in the Tuba City High School as they do rent rooms to outsiders ! We were welcomed nicely in this old fashioned place operated by the local Navajo people. We visited Page and its (too) touristic area the next day along with the famous (and so crowded) Antelope Canyon and Horseshoe Bend .","title":"North rim Grand Canyon & the Navajo Indians"},{"location":"Tech%20Blog/2013/2013-11-07-monument-valley/","text":"Forget about Marlboro, this Navajo nation area is way more than that. To be honest, you can see the money made from the tax you have to pay to enter the actual park is invested back in the community but I guess there's more and some drawbacks to it. Meet the first 10 minutes of rain we experienced during a whole month ! Actually, it was a localized storm and it happened at the perfect moment where we had the most beautiful view. We felt this moment was quite unique and we were excited and glad to experience it. I guess you won't argue with me after looking at those pictures.","title":"Monument Valley"},{"location":"Tech%20Blog/2013/2013-11-09-mongodb-v2-4-8-rabbitmq-v3-2-1-rsyslog-v-7-4-6/","text":"mongodb-2.4.8 \u00b6 You should consider this important update if you have a cluster running v2.4.7 . It contains a fix for the config servers which can have them possibly disagree on chunks hashes and thus prevent mongos to start or balancing to happen. See this bug for more info. rabbitMQ-3.2.1 \u00b6 The famous message queuing server got a nice bunch of bug fixes on a lot of its modules along with some interesting additions such as : support for federated queues report client authentication errors during connection establishment explicitly using connection.close inform clients when memory or disk alarms are set or cleared allow policies to target queues or exchanges or both offer greater control over threshold at which messages are paged to disk allow missing exchanges & queues to be deleted and unbound without generating an AMQP error implement consumer priorities Full changelog here and here . rsyslog-7.4.6 \u00b6 This is a bug fix release, nothing too big about it as reported by Thomas D (thanks again). Please note that rsyslog-7.4.4 is being stabilized , mainly for security purposes .","title":"mongoDB v2.4.8, rabbitMQ v3.2.1, rsyslog v7.4.6"},{"location":"Tech%20Blog/2013/2013-11-09-mongodb-v2-4-8-rabbitmq-v3-2-1-rsyslog-v-7-4-6/#mongodb-248","text":"You should consider this important update if you have a cluster running v2.4.7 . It contains a fix for the config servers which can have them possibly disagree on chunks hashes and thus prevent mongos to start or balancing to happen. See this bug for more info.","title":"mongodb-2.4.8"},{"location":"Tech%20Blog/2013/2013-11-09-mongodb-v2-4-8-rabbitmq-v3-2-1-rsyslog-v-7-4-6/#rabbitmq-321","text":"The famous message queuing server got a nice bunch of bug fixes on a lot of its modules along with some interesting additions such as : support for federated queues report client authentication errors during connection establishment explicitly using connection.close inform clients when memory or disk alarms are set or cleared allow policies to target queues or exchanges or both offer greater control over threshold at which messages are paged to disk allow missing exchanges & queues to be deleted and unbound without generating an AMQP error implement consumer priorities Full changelog here and here .","title":"rabbitMQ-3.2.1"},{"location":"Tech%20Blog/2013/2013-11-09-mongodb-v2-4-8-rabbitmq-v3-2-1-rsyslog-v-7-4-6/#rsyslog-746","text":"This is a bug fix release, nothing too big about it as reported by Thomas D (thanks again). Please note that rsyslog-7.4.4 is being stabilized , mainly for security purposes .","title":"rsyslog-7.4.6"},{"location":"Tech%20Blog/2013/2013-11-15-keepalived-v1-2-9/","text":"Another release, 3 months after the mighty 1.2.8. It seems like upstream has awaken ! highlights \u00b6 Jonas Johansson fixed VRRP sync group by sending prio 0 when entering FAULT state. This fix will send prio 0 (VRRP_PRIO_STOP) when the VRRP router transists from MASTER to FAULT state. This will make a sync group leave the MASTER state more quickly by notifying the backup router(s) instead of having them to wait for time out. Jonas Johansson fixed VRRP to honor preempt_delay setting on startup. Jonas Johansson extended VRRP code for faster sync group transition. Some nice bug fixes to unicast mode. Full changelog here !","title":"keepalived v1.2.9"},{"location":"Tech%20Blog/2013/2013-11-15-keepalived-v1-2-9/#highlights","text":"Jonas Johansson fixed VRRP sync group by sending prio 0 when entering FAULT state. This fix will send prio 0 (VRRP_PRIO_STOP) when the VRRP router transists from MASTER to FAULT state. This will make a sync group leave the MASTER state more quickly by notifying the backup router(s) instead of having them to wait for time out. Jonas Johansson fixed VRRP to honor preempt_delay setting on startup. Jonas Johansson extended VRRP code for faster sync group transition. Some nice bug fixes to unicast mode. Full changelog here !","title":"highlights"},{"location":"Tech%20Blog/2013/2013-11-27-rip-stabber/","text":"Today we did shutdown the oldest Gentoo Linux server of our oldest production datacenter. It was running since April, 5th of year 2006 so that's a total of 2793 days of production level service as a stateful firewall. Its name was stabber , in reference of a vessel in the Eve Online MMORPG which I played a lot at the time. Our company has been running on Gentoo Linux since 2004 for its Linux platforms and I often hear and experience the astonishment of the other persons I speak to about this : \" Gentoo Linux in production, really ?\" or \"Wow you guys are a bunch of crazy hardcore Gurus\" ... As if Gentoo Linux did not meet the production level requirements or the security level you expect from another major (usually not free) distribution and as if you had to master some major skills to have it done... 7 years later, stabber is in my opinion a proof that all those assumptions are wrong. I was a junior sysadmin at the time I made this server, we didn't want to pay for having a proper firewall so we decided to make our own (that's what Gentoo is to me : simple things done right, no added sugar) The rolling updates of Gentoo did not brake our system and it evolved along our infrastructure The GLSA kept our server immune to security breaches over the years (thx to the Gentoo security team) This server/firewall passed the security tests of both Paypal and Ebay , this looks production level enough to me We did shutdown this server because it was a single point of failure on an old part of our architecture. Its role has been taken over by two fault tolerant servers/firewalls running... Gentoo Linux of course ! First emerge.log entry Wed Apr 5 12:53:22 2006 >>> sys-kernel/hardened-sources-2.6.14-r5 Latest uname -a Linux stabber 2.6.16-hardened-r11 #1 SMP PREEMPT Wed Aug 30 15:51:49 CEST 2006 i686 Intel(R) Xeon(TM) CPU 3.20GHz GenuineIntel GNU/Linux Latest commands stabber ~ # echo \"je taime\" >> last.letter stabber ~ # shutdown now -h Dear fellow Gentoo Linux developers, your work makes all this possible, thank you !","title":"RIP stabber"},{"location":"Tech%20Blog/2013/2013-12-05-arcosanti/","text":"On the route down from Flagstaff to Phoenix, we stopped and stayed in Arcosanti . This experimental city is a unique place as it was imagined by an Italian architect who began its construction in the 70s with volunteers from all around the world. Today its construction is still ongoing and people come there to contribute for a certain period of time before going back home (we met a French lady from Britany there, this was weird). The cool thing about this community is that you can book a room there to stay and live among them for some time. We had dinner and breakfast with them, everything was very cool and very tasty ! We didn't really know what to expect when arriving there and were amazed by the place and its incredible athmosphere. We had the feeling of being in another world, far from everything we're used to. This is the kind of place where you can feel far from everything but close from the earth, a unique experience really. I even had the chance to take a dive in the swimming pool overlooking the surrounding desert. I couldn't help but feel strange about how the architecture of this place made me feel like I was walking in the video game Riven !","title":"Arcosanti"},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/","text":"I'm glad to announce the v1.1 release of py3status , roughly 6 months after v1.0 which features some nice and often contributed bug fixes and some exciting new features ! fixes / enhancements \u00b6 allow float on refresh interval option fix respect user's locale for time transformation fix i3status time adjustment when format does not contain the necessary items to get an exact datetime fix delay on py3status start waiting for i3status, this caused a useless first refresh delay of py3status of i3status interval seconds fix first click event opening line detection redirect stdout and stderr to null to suppress modules outputs, this prevents i3bar from frezzing when a user module prints something to stdout or stderr, more info features \u00b6 Thanks to py3status, you can now take action on clicks made on your i3status modules ! the new i3bar_click_events.py module allows you to implement some actions when clicking on your i3bar modules more info in the wiki and in the source code of the module new modules \u00b6 new generic click event handler using the special module file named i3bar_click_events.py which will be forwarded any orphan click event for action new example module displaying Yahoo Weather forcast new example whoami displaying the currently logged in user, inspired by user request on i3 FAQ contributors \u00b6 Thanks a lot for their issues ranging from #15 to #20 with feedback, proposals and pull requests ! @alethiophile @Edholm @ifschleife @lathan @patrickshan @ShadowPrince also... \u00b6 Feel free to join the #py3status IRC channel on FreeNode to get help or share your ideas !","title":"py3status v1.1"},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#fixes-enhancements","text":"allow float on refresh interval option fix respect user's locale for time transformation fix i3status time adjustment when format does not contain the necessary items to get an exact datetime fix delay on py3status start waiting for i3status, this caused a useless first refresh delay of py3status of i3status interval seconds fix first click event opening line detection redirect stdout and stderr to null to suppress modules outputs, this prevents i3bar from frezzing when a user module prints something to stdout or stderr, more info","title":"fixes / enhancements"},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#features","text":"Thanks to py3status, you can now take action on clicks made on your i3status modules ! the new i3bar_click_events.py module allows you to implement some actions when clicking on your i3bar modules more info in the wiki and in the source code of the module","title":"features"},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#new-modules","text":"new generic click event handler using the special module file named i3bar_click_events.py which will be forwarded any orphan click event for action new example module displaying Yahoo Weather forcast new example whoami displaying the currently logged in user, inspired by user request on i3 FAQ","title":"new modules"},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#contributors","text":"Thanks a lot for their issues ranging from #15 to #20 with feedback, proposals and pull requests ! @alethiophile @Edholm @ifschleife @lathan @patrickshan @ShadowPrince","title":"contributors"},{"location":"Tech%20Blog/2013/2013-12-15-py3status-v1-1/#also","text":"Feel free to join the #py3status IRC channel on FreeNode to get help or share your ideas !","title":"also..."},{"location":"Tech%20Blog/2013/2013-12-24-rsyslog-v7-4-7/","text":"I've been slacking on this package for some months but slowly got the chance to catch-up with the multiple bugs open. So today I'm glad to say to make this blog post not only for a version bump release but also for some nice added features and ebuild improvements fixing 6 bugs in a row. ebuild \u00b6 added support for the mongoDB output template module thanks to Vadim Kuznetsov added support for sub-slot operators for json-c and libgcrypt dependencies using EAPI5 thanks to Thomas D. libgcrypt is now a required dependency of rsyslog as I didn't want to add another USE flag for such a widely spread dependency I've also bumped net-libs/czmq with approval of @jlec which fixes dependencies when trying to build rsyslog with zeromq support, thanks to Allen Parker for his report and valuable debugging rsyslog v7.4.7 \u00b6 This is a bugfix release, see the full changelog here .","title":"rsyslog v7.4.7"},{"location":"Tech%20Blog/2013/2013-12-24-rsyslog-v7-4-7/#ebuild","text":"added support for the mongoDB output template module thanks to Vadim Kuznetsov added support for sub-slot operators for json-c and libgcrypt dependencies using EAPI5 thanks to Thomas D. libgcrypt is now a required dependency of rsyslog as I didn't want to add another USE flag for such a widely spread dependency I've also bumped net-libs/czmq with approval of @jlec which fixes dependencies when trying to build rsyslog with zeromq support, thanks to Allen Parker for his report and valuable debugging","title":"ebuild"},{"location":"Tech%20Blog/2013/2013-12-24-rsyslog-v7-4-7/#rsyslog-v747","text":"This is a bugfix release, see the full changelog here .","title":"rsyslog v7.4.7"},{"location":"Tech%20Blog/2014/2014-01-03-aircraft-boneyard/","text":"After Arcosanti, we kept on going south into Arizona where we headed to Phoenix and stayed in Tuscon for a few days. It was hot and sunny of course and we loved this part of Arizona. We went to see the US Army Aircraft Boneyard which is located a bit South-East of Tuscon, in the middle of the desert. Imagine hundreds of planes parked in the sand, waiting to be dismantled !","title":"Aircraft Boneyard"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/","text":"We've been running quite a lot of production clusters using pacemaker/corosync for a while. Some of them are large, handling more than 200 resources across multiple nodes and we've exceeded some limits on pacemaker's CIB size. I thought I'd share how to tune your cluster to handle such a bunch of resources since there are some default limits on the IPC buffer size which can lead to problems when your resources (and thus CIB) grows too much. Hitting the IPC limit \u00b6 When running a large cluster you may hit the following problem : error: crm_ipc_prepare: Could not compress the message into less than the configured ipc limit (51200 bytes).Set PCMK_ipc_buffer to a higher value (2071644 bytes suggested) Evaluating the buffer size \u00b6 Have a look at the size of your current CIB : # cibadmin -Ql > cib.xml ls -l cib.xml \u00b6 bzip2 cib.xml \u00b6 ls -l cib.xml.bz2 \u00b6 The CIB is compressed on the wire using bzip2 so you have to compare the compressed cib.xml.bz2 with the IPC default buffer size of 51200 and you'll find the sufficient PCMK_ipc_buffer value for you (take more just to be safe). Setting the environment variables \u00b6 On Gentoo Linux, you'll have to create the /etc/env.d/90pacemaker file containing : PCMK_ipc_type=shared-mem PCMK_ipc_buffer=2071644 PCMK_ipc_buffer : you may need to increase this depending on your cluster size and needs PCMK_ipc_type : the shared-mem one is the default now, other values are socket|posix|sysv You will also need to set these env. vars in your .bashrc so that the crm CLI doesn't break : export PCMK_ipc_type=shared-mem export PCMK_ipc_buffer=2071644 Future \u00b6 Finally, I wanted to let you know that the upcoming Pacemaker v1.1.11 should come with a feature which will allow the IPC layer to adjust the PCMK_ipc_buffer automagically ! Hopefully you shouldn't need this blog post anymore pretty soon :) EDIT, Jan 16 2014 \u00b6 Following this blog post, I had a very interesting comment from @ beekhof (lead dev of pacemaker) beekhof> Ultrabug: regarding large clusters, the cib in 1.1.12 will be O(2) faster than 1.1.11. Ultrabug> beekhof: that's great news mate ! when is it scheduled to be released ? beekhof> 30th of Feb","title":"Tuning pacemaker for large clusters"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#hitting-the-ipc-limit","text":"When running a large cluster you may hit the following problem : error: crm_ipc_prepare: Could not compress the message into less than the configured ipc limit (51200 bytes).Set PCMK_ipc_buffer to a higher value (2071644 bytes suggested)","title":"Hitting the IPC limit"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#evaluating-the-buffer-size","text":"Have a look at the size of your current CIB : # cibadmin -Ql > cib.xml","title":"Evaluating the buffer size"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#ls-l-cibxml","text":"","title":"ls -l cib.xml"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#bzip2-cibxml","text":"","title":"bzip2 cib.xml"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#ls-l-cibxmlbz2","text":"The CIB is compressed on the wire using bzip2 so you have to compare the compressed cib.xml.bz2 with the IPC default buffer size of 51200 and you'll find the sufficient PCMK_ipc_buffer value for you (take more just to be safe).","title":"ls -l cib.xml.bz2"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#setting-the-environment-variables","text":"On Gentoo Linux, you'll have to create the /etc/env.d/90pacemaker file containing : PCMK_ipc_type=shared-mem PCMK_ipc_buffer=2071644 PCMK_ipc_buffer : you may need to increase this depending on your cluster size and needs PCMK_ipc_type : the shared-mem one is the default now, other values are socket|posix|sysv You will also need to set these env. vars in your .bashrc so that the crm CLI doesn't break : export PCMK_ipc_type=shared-mem export PCMK_ipc_buffer=2071644","title":"Setting the environment variables"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#future","text":"Finally, I wanted to let you know that the upcoming Pacemaker v1.1.11 should come with a feature which will allow the IPC layer to adjust the PCMK_ipc_buffer automagically ! Hopefully you shouldn't need this blog post anymore pretty soon :)","title":"Future"},{"location":"Tech%20Blog/2014/2014-01-10-tuning-pacemaker-for-large-clusters/#edit-jan-16-2014","text":"Following this blog post, I had a very interesting comment from @ beekhof (lead dev of pacemaker) beekhof> Ultrabug: regarding large clusters, the cib in 1.1.12 will be O(2) faster than 1.1.11. Ultrabug> beekhof: that's great news mate ! when is it scheduled to be released ? beekhof> 30th of Feb","title":"EDIT, Jan 16 2014"},{"location":"Tech%20Blog/2014/2014-01-12-py3status-v1-2/","text":"I'm glad to announce a new release of py3status with an exciting main new feature giving the ability to modify any of i3status' module output from any of your modules ! feature \u00b6 you can now change i3status modules' output by altering the i3status_output_json parameter received in any of your module ! changelog \u00b6 new module dpms.py allowing activation and deactivation of DPMS thx to Andr\u00e9 Doser order i3status output updates to prevent it from overwritting any modification made on i3status json list by a user module, this avoids a possible user filter flapping on i3status modules fix delay on first execution of each module which could be equal to py3status interval time before being executed : your modules get executed and displayed immediately no matter py3status' interval the real i3status thread output json list is passed to all modules as the i3status_output_json parameter, this allows any user module to change any of the i3status output by simply altering the given json on the list , inspired thx to @drestebon on issue #23 add validation for the position parameter add cpu usage info to sysdata script, by Patrick Shan contributors \u00b6 Many thanks to all contributors for their work and inspiration. Patrick Shan, @patrickshan @drestebon Andr\u00e9 Doser, @tasse","title":"py3status v1.2"},{"location":"Tech%20Blog/2014/2014-01-12-py3status-v1-2/#feature","text":"you can now change i3status modules' output by altering the i3status_output_json parameter received in any of your module !","title":"feature"},{"location":"Tech%20Blog/2014/2014-01-12-py3status-v1-2/#changelog","text":"new module dpms.py allowing activation and deactivation of DPMS thx to Andr\u00e9 Doser order i3status output updates to prevent it from overwritting any modification made on i3status json list by a user module, this avoids a possible user filter flapping on i3status modules fix delay on first execution of each module which could be equal to py3status interval time before being executed : your modules get executed and displayed immediately no matter py3status' interval the real i3status thread output json list is passed to all modules as the i3status_output_json parameter, this allows any user module to change any of the i3status output by simply altering the given json on the list , inspired thx to @drestebon on issue #23 add validation for the position parameter add cpu usage info to sysdata script, by Patrick Shan","title":"changelog"},{"location":"Tech%20Blog/2014/2014-01-12-py3status-v1-2/#contributors","text":"Many thanks to all contributors for their work and inspiration. Patrick Shan, @patrickshan @drestebon Andr\u00e9 Doser, @tasse","title":"contributors"},{"location":"Tech%20Blog/2014/2014-01-17-uwsgi-v2-0/","text":"Yesterday was a big day for the famous application container uWSGI . We released the brand new version 2.0 LTS along with quite a huge bump of the ebuild, closing 6 bugs at once. I thought I'd give some input about the ebuild changes and some quick notes about uWSGI. Many thanks again to @dev-zero ! New plugins selection : UWSGI_PLUGINS \u00b6 We introduced a new USE_EXPAND named UWSGI_PLUGINS so that you can now select which plugins to build individually. This is a great step as it makes the compilation more clear and lets you fine tune your uWSGI installation. Along this work, we had to describe each plugin which was also quite a challenge. To my knownledge, this has not been done anywhere else so here it is . Please ping me if you have something to add or if we failed to describe a plugin correctly. Migration note : You will need to change your package.use configuration to switch to using UWSGI_PLUGINS. As an example, where you had the USE flag spooler enabled you'll now need to use uwsgi_plugins_spooler . uWSGI v2.0 highlights \u00b6 These are my biased favorites, go check for more, it's huge ! A brand new and uber fast caching framework The High-Availability oriented Legion subsystem The long awaited WebSocket support Integrated Transformations mechanisms The new Metrics subsystem A Chunked input API SNI support , virtual hosting for SSL nodes","title":"uWSGI v2.0"},{"location":"Tech%20Blog/2014/2014-01-17-uwsgi-v2-0/#new-plugins-selection-uwsgi_plugins","text":"We introduced a new USE_EXPAND named UWSGI_PLUGINS so that you can now select which plugins to build individually. This is a great step as it makes the compilation more clear and lets you fine tune your uWSGI installation. Along this work, we had to describe each plugin which was also quite a challenge. To my knownledge, this has not been done anywhere else so here it is . Please ping me if you have something to add or if we failed to describe a plugin correctly. Migration note : You will need to change your package.use configuration to switch to using UWSGI_PLUGINS. As an example, where you had the USE flag spooler enabled you'll now need to use uwsgi_plugins_spooler .","title":"New plugins selection : UWSGI_PLUGINS"},{"location":"Tech%20Blog/2014/2014-01-17-uwsgi-v2-0/#uwsgi-v20-highlights","text":"These are my biased favorites, go check for more, it's huge ! A brand new and uber fast caching framework The High-Availability oriented Legion subsystem The long awaited WebSocket support Integrated Transformations mechanisms The new Metrics subsystem A Chunked input API SNI support , virtual hosting for SSL nodes","title":"uWSGI v2.0 highlights"},{"location":"Tech%20Blog/2014/2014-01-31-keepalived-v1-2-11-glusterfs-v3-4-2/","text":"Quick post for two quick bumps related to clustering. glusterfs-3.4.2 \u00b6 quite a lot of bug fixes and improvements contains a backport for libgfapi support for integrating with NFS Ganesha nfs/mount3: fix crash in subdir resolution keepalived-1.2.11 \u00b6 autoconf: better libnl3 detection Fix memory allocation for MD5 digest Quite some nice memory leak fixes on different components vrrp: dont try to load ip_vs module when not needed Pim van den Berg work on libipvs-2.6 to sync with libipvs from ipvsadm 1.27 vrrp: extend ip parser to support default and default6 vrrp: fix/extend gratuitous ARP handling (multiple people reported issues where MASTER didnt recover properly after outage due to no gratuitous ARP sent) Multiple fixes to genhash vrrp: fix vrrp socket sync while leaving FAULT state (old old bug here) Full changelog here","title":"keepalived v1.2.11 & glusterfs v3.4.2"},{"location":"Tech%20Blog/2014/2014-01-31-keepalived-v1-2-11-glusterfs-v3-4-2/#glusterfs-342","text":"quite a lot of bug fixes and improvements contains a backport for libgfapi support for integrating with NFS Ganesha nfs/mount3: fix crash in subdir resolution","title":"glusterfs-3.4.2"},{"location":"Tech%20Blog/2014/2014-01-31-keepalived-v1-2-11-glusterfs-v3-4-2/#keepalived-1211","text":"autoconf: better libnl3 detection Fix memory allocation for MD5 digest Quite some nice memory leak fixes on different components vrrp: dont try to load ip_vs module when not needed Pim van den Berg work on libipvs-2.6 to sync with libipvs from ipvsadm 1.27 vrrp: extend ip parser to support default and default6 vrrp: fix/extend gratuitous ARP handling (multiple people reported issues where MASTER didnt recover properly after outage due to no gratuitous ARP sent) Multiple fixes to genhash vrrp: fix vrrp socket sync while leaving FAULT state (old old bug here) Full changelog here","title":"keepalived-1.2.11"},{"location":"Tech%20Blog/2014/2014-02-18-mongodb-v2-4-9v2-2-7-rabbitmq-v3-2-3/","text":"Quick post about some recent bumps. mongodb-2.4.9 & mongodb-2.2.7 \u00b6 IMPORTANT : These versions fix a mongos bug which could lead it to report a write as successful when it was not. This affects all versions of MongoDB prior to and including v2.4.8. v2.4.9 changelog v2.2.7 changelog Stay tuned on mongoDB, the next post will probably talk about the release of pymongo v2.7 which supports some neat futures from the upcoming mongoDB v2.6 series. rabbitMQ-3.2.3 \u00b6 I skipped a bump post when releasing the v3.2.2 so you should check out the v3.2.3 changelog as well if you're willing to know more about those bug fix releases.","title":"mongoDB v2.4.9/v2.2.7, rabbitMQ v3.2.3"},{"location":"Tech%20Blog/2014/2014-02-18-mongodb-v2-4-9v2-2-7-rabbitmq-v3-2-3/#mongodb-249-mongodb-227","text":"IMPORTANT : These versions fix a mongos bug which could lead it to report a write as successful when it was not. This affects all versions of MongoDB prior to and including v2.4.8. v2.4.9 changelog v2.2.7 changelog Stay tuned on mongoDB, the next post will probably talk about the release of pymongo v2.7 which supports some neat futures from the upcoming mongoDB v2.6 series.","title":"mongodb-2.4.9 &amp; mongodb-2.2.7"},{"location":"Tech%20Blog/2014/2014-02-18-mongodb-v2-4-9v2-2-7-rabbitmq-v3-2-3/#rabbitmq-323","text":"I skipped a bump post when releasing the v3.2.2 so you should check out the v3.2.3 changelog as well if you're willing to know more about those bug fix releases.","title":"rabbitMQ-3.2.3"},{"location":"Tech%20Blog/2014/2014-02-23-py3status-v1-3/","text":"I'm glad to announce the release of py3status v1.3 which brings to life a feature request from @ tasse and @ ttyE0 . Guys, I hope this one will please you ! what's new ? \u00b6 Along with a localization bug fix thanks to @ zetok from Poland, the main new feature is that py3status now supports a standalone mode which you can use when you only want your own modules displayed in an i3bar ! As usual, this release is already available for my fellow Gentoo Linux users and on pypi ! Changelog is here and quick to get, enjoy !","title":"py3status v1.3"},{"location":"Tech%20Blog/2014/2014-02-23-py3status-v1-3/#whats-new","text":"Along with a localization bug fix thanks to @ zetok from Poland, the main new feature is that py3status now supports a standalone mode which you can use when you only want your own modules displayed in an i3bar ! As usual, this release is already available for my fellow Gentoo Linux users and on pypi ! Changelog is here and quick to get, enjoy !","title":"what's new ?"},{"location":"Tech%20Blog/2014/2014-03-07-couchbase-on-gentoo-linux/","text":"Back in 2010 when I was comparing different NoSQL solutions I came across CouchDB. Even tho I went for mongoDB in the end, it was still a nice and promising technology even more since the merge with the Membase guys in late 2012 which lead to the actual Couchbase . I won't go into the details of Couchbase itself since it's way covered all around the net but I wanted to let you guys know that I've packaged most of the couchbase ecosystem for Gentoo Linux : dev-db/ couchbase-server-community-2.2.0 : the community server edition (bin) dev-libs/ libcouchbase-2.2.0 : the C client library dev-python/ couchbase-1.2.0 : the python client library Those packages are still only available on my overlay (ultrabug on layman) since I'm not sure about the interest of other users in the community and I still need to make sure it's production ready enough. If you're interested in seeing this package in portage, please say so ! I dedicate this packaging to @atorgfr :)","title":"Couchbase on Gentoo Linux"},{"location":"Tech%20Blog/2014/2014-03-17-nightster-bw/","text":"This shot was taken some days ago on a short trip near Paris, it's a nice addition to the very few pictures I have of my bike ! click to see full resolution, grain is very nice on this Ilford 3200 ISO film","title":"Nightster B&W"},{"location":"Tech%20Blog/2014/2014-04-04-convert-special-characters-to-ascii-in-python/","text":"I came across a recurrent problem at work which was to convert special characters such as the French-Latin accentuated letter \"\u00e9\" to ASCII \"e\" (this is called transliteration). I wanted to avoid having to use an external library such as Unidecode (which is great obviously) so I ended up wandering around the unicodedata built-in library. Before I had to get too deep in the matter I found this StackOverflow topic which gives an interesting method to do so and works fine for me. def strip_accents(s): \"\"\" Sanitarize the given unicode string and remove all special/localized characters from it. Category \"Mn\" stands for Nonspacing\\_Mark \"\"\" try: return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' ) except: return s PS : thanks to @Flameeyes for his good remark on wording","title":"Convert special characters to ASCII in python"},{"location":"Tech%20Blog/2014/2014-04-08-mongodb-2-4-10-pymongo-2-7/","text":"I'm pleased to announce those latest mongoDB related bumps. The next version bump will be for the brand new mongoDB 2.6 for which I'll add some improvements to the Gentoo ebuild so stay tuned ;) mongodb-2.4.10 \u00b6 fixes some memory leaks start elections if more than one primary is detected fixes issues about indexes building and replication on secondaries chunk size is decreased to 255 KB (from 256 KB) to avoid overhead with usePowerOf2Sizes option All mongodb-2.4.10 changelog here. pymongo-2.7 \u00b6 of course, the main feature is the mongoDB 2.6 support new bulk write API (I love it) much improved concurrency control for MongoClient support for GridFS queries All pymongo-2.7 changelog here.","title":"mongoDB 2.4.10 & pymongo 2.7"},{"location":"Tech%20Blog/2014/2014-04-08-mongodb-2-4-10-pymongo-2-7/#mongodb-2410","text":"fixes some memory leaks start elections if more than one primary is detected fixes issues about indexes building and replication on secondaries chunk size is decreased to 255 KB (from 256 KB) to avoid overhead with usePowerOf2Sizes option All mongodb-2.4.10 changelog here.","title":"mongodb-2.4.10"},{"location":"Tech%20Blog/2014/2014-04-08-mongodb-2-4-10-pymongo-2-7/#pymongo-27","text":"of course, the main feature is the mongoDB 2.6 support new bulk write API (I love it) much improved concurrency control for MongoClient support for GridFS queries All pymongo-2.7 changelog here.","title":"pymongo-2.7"},{"location":"Tech%20Blog/2014/2014-04-16-py3status-v1-4/","text":"I'm glad to announce the release of py3status-1.4 which I'd like to dedicate to @guiniol who provided valuable debugging (a whole Arch VM) to help me solve the problem he was facing (see changelog). I'm gathering wish lists an have some (I hope) cool ideas for the next v1.5 release, feel free to post your most adventurous dreams ! changelog \u00b6 new ordering mechanism with verbose logging on debug mode. fixes rare cases where the modules methods were not always loaded in the same order and caused inconsistent ordering between reloads. thx to @guiniol for reporting/debugging and @IotaSpencer and @tasse for testing. debug: dont catch print() on debug mode debug: add position requested by modules Add new module ns_checker.py, by @nawadanp move README to markdown, change ordering update the README with the new options from --help contributors \u00b6 Special thanks to this release's contributors ! @nawadanp @guiniol @IotaSpencer @tasse","title":"py3status v1.4"},{"location":"Tech%20Blog/2014/2014-04-16-py3status-v1-4/#changelog","text":"new ordering mechanism with verbose logging on debug mode. fixes rare cases where the modules methods were not always loaded in the same order and caused inconsistent ordering between reloads. thx to @guiniol for reporting/debugging and @IotaSpencer and @tasse for testing. debug: dont catch print() on debug mode debug: add position requested by modules Add new module ns_checker.py, by @nawadanp move README to markdown, change ordering update the README with the new options from --help","title":"changelog"},{"location":"Tech%20Blog/2014/2014-04-16-py3status-v1-4/#contributors","text":"Special thanks to this release's contributors ! @nawadanp @guiniol @IotaSpencer @tasse","title":"contributors"},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/","text":"Two weeks vacations always seem short yet the 900+ mails waiting for sorting on my Gentoo Linux inbox was a reminder that our beloved distribution is well alive ! So I guess it was time for a little bug killing spree :) rabbitMQ v3.3.0 \u00b6 This release improves performance in a variety of conditions, adds monitoring information to identify performance bottlenecks, adds dynamically manageable shovels, and allows Java-based clients to reconnect automatically after network failure. This release also corrects a number of defects in the broker and plugins, as well as introducing a host of smaller features as you can see on the changelog . Be warned that the behavior of the guest user has been altered ! I also fixed a long awaiting bug to bump the rabbitMQ C client to v0.5.0 redis v2.8.9 \u00b6 Johan Bergstr\u00f6m is as always doing a great and helpful job and is actively working on redis , thanks mate ! [NEW] The HyperLogLog data structure. You can read more about it in this blog post [NEW] The Sorted Set data type has now support for lexicographic range queries, check the new commands ZRANGEBYLEX, ZLEXCOUNT and ZREMRANGEBYLEX, which are documented at http://redis.io py3status v1.5 \u00b6 fixes installation via pip added a --version command line argument to get the currently installed version of py3status upcoming bumps \u00b6 You might be interested in what's next on the todo list : With the help of Thomas D. aka @Whissi, we're working on bumping and enhancing rsyslog to v7.6.3 . For this a series of its dependencies have been bumped today as well. mongoDB v2.6.0 is also on track, as usual the guys @mongodb have broken the scons building so it's taking more time than it should to fix this hell (all help appreciated).","title":"After vacation bug hunting"},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/#rabbitmq-v330","text":"This release improves performance in a variety of conditions, adds monitoring information to identify performance bottlenecks, adds dynamically manageable shovels, and allows Java-based clients to reconnect automatically after network failure. This release also corrects a number of defects in the broker and plugins, as well as introducing a host of smaller features as you can see on the changelog . Be warned that the behavior of the guest user has been altered ! I also fixed a long awaiting bug to bump the rabbitMQ C client to v0.5.0","title":"rabbitMQ v3.3.0"},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/#redis-v289","text":"Johan Bergstr\u00f6m is as always doing a great and helpful job and is actively working on redis , thanks mate ! [NEW] The HyperLogLog data structure. You can read more about it in this blog post [NEW] The Sorted Set data type has now support for lexicographic range queries, check the new commands ZRANGEBYLEX, ZLEXCOUNT and ZREMRANGEBYLEX, which are documented at http://redis.io","title":"redis v2.8.9"},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/#py3status-v15","text":"fixes installation via pip added a --version command line argument to get the currently installed version of py3status","title":"py3status v1.5"},{"location":"Tech%20Blog/2014/2014-05-04-after-vacation-bug-hunting/#upcoming-bumps","text":"You might be interested in what's next on the todo list : With the help of Thomas D. aka @Whissi, we're working on bumping and enhancing rsyslog to v7.6.3 . For this a series of its dependencies have been bumped today as well. mongoDB v2.6.0 is also on track, as usual the guys @mongodb have broken the scons building so it's taking more time than it should to fix this hell (all help appreciated).","title":"upcoming bumps"},{"location":"Tech%20Blog/2014/2014-05-06-uwsgi-v2-0-4/","text":"Quick post for an interesting version bump of uWSGI which brings an experimental loopengine for python3.4 asyncio (aka tulip) ! If you want to try it out, I added a python_asyncio USE flag. I've also made some cleanups on the ebuild wrt python versions and dropped older versions of uWSGI. highlights \u00b6 experimental asyncio loop engine (python 3.4 only) httprouter advanced timeout management purge LRU cache (v2) feature allow duplicate headers in http parsers faster on_demand Emperor management fixed segfault for unnamed loggers See the full changelog here.","title":"uWSGI v2.0.4"},{"location":"Tech%20Blog/2014/2014-05-06-uwsgi-v2-0-4/#highlights","text":"experimental asyncio loop engine (python 3.4 only) httprouter advanced timeout management purge LRU cache (v2) feature allow duplicate headers in http parsers faster on_demand Emperor management fixed segfault for unnamed loggers See the full changelog here.","title":"highlights"},{"location":"Tech%20Blog/2014/2014-05-14-mongodb-v2-6-1/","text":"This is a great pleasure to announce the version bump of mongoDB to the brand new v2.6 stable branch ! This bump is not trivial and comes with a lot of changes, please read carefully as you will have to modify your mongodb configuration files ! ebuild changes \u00b6 As a long time request and to be more in line with upstream's recommendations (and systemd support) I moved the configuration of the mongoDB daemons to /etc so make sure to adapt to the new YAML format . the mongodb configuration moved from /etc/conf.d/mongodb to the new YAML formatted /etc/mongodb.conf the mongos configuration moved from /etc/conf.d/mongos to the new YAML formatted /etc/mongos.conf the MMS agent configuration file has moved to /etc/mms-agent.conf The init scripts also have been taken care of : new and modern mongodb, mongos and mms-agent init scripts their /etc/conf.d/ configuration files are only used to modify the init script's behavior highlights \u00b6 The changelog is long and the goal of this post is not to give you an already well covered topic on the release notes but here are my favorite features : MongoDB preserves the order of the document fields following write operations. A new write protocol integrates write operations with write concerns. The protocol also provides improved support for bulk operations . MongoDB can now use index intersection to fulfill queries supported by more than one index. Index Filters to limit which indexes can become the winning plan for a query. Background index build allowed on secondaries . New cleanupOrphaned command to remove orphaned documents from a shard . usePowerOf2Sizes is now the default allocation strategy for all new collections. Removed upward limit of 20 000 connections for the maxIncomingConnections for mongod and mongos. New cursor.maxTimeMS() and corresponding maxTimeMS option for commands to specify a time limit . Make sure you follow the official upgrade plan to upgrade from a previous version, this release is not a simple drop-in replacement. thanks \u00b6 Special thanks go to Johan Bergstr\u00f6m for his continuous efforts and responsiveness as well as Mike Limansky and Jason A. Donenfeld.","title":"mongoDB v2.6.1"},{"location":"Tech%20Blog/2014/2014-05-14-mongodb-v2-6-1/#ebuild-changes","text":"As a long time request and to be more in line with upstream's recommendations (and systemd support) I moved the configuration of the mongoDB daemons to /etc so make sure to adapt to the new YAML format . the mongodb configuration moved from /etc/conf.d/mongodb to the new YAML formatted /etc/mongodb.conf the mongos configuration moved from /etc/conf.d/mongos to the new YAML formatted /etc/mongos.conf the MMS agent configuration file has moved to /etc/mms-agent.conf The init scripts also have been taken care of : new and modern mongodb, mongos and mms-agent init scripts their /etc/conf.d/ configuration files are only used to modify the init script's behavior","title":"ebuild changes"},{"location":"Tech%20Blog/2014/2014-05-14-mongodb-v2-6-1/#highlights","text":"The changelog is long and the goal of this post is not to give you an already well covered topic on the release notes but here are my favorite features : MongoDB preserves the order of the document fields following write operations. A new write protocol integrates write operations with write concerns. The protocol also provides improved support for bulk operations . MongoDB can now use index intersection to fulfill queries supported by more than one index. Index Filters to limit which indexes can become the winning plan for a query. Background index build allowed on secondaries . New cleanupOrphaned command to remove orphaned documents from a shard . usePowerOf2Sizes is now the default allocation strategy for all new collections. Removed upward limit of 20 000 connections for the maxIncomingConnections for mongod and mongos. New cursor.maxTimeMS() and corresponding maxTimeMS option for commands to specify a time limit . Make sure you follow the official upgrade plan to upgrade from a previous version, this release is not a simple drop-in replacement.","title":"highlights"},{"location":"Tech%20Blog/2014/2014-05-14-mongodb-v2-6-1/#thanks","text":"Special thanks go to Johan Bergstr\u00f6m for his continuous efforts and responsiveness as well as Mike Limansky and Jason A. Donenfeld.","title":"thanks"},{"location":"Tech%20Blog/2014/2014-05-20-iran-tehran/","text":"For our latest vacations, we spent two weeks in Iran and I must say that this country is astonishingly beautiful ! So forget what you think to know about this country as it's either biased or doesn't apply to its people who are very friendly and welcoming. We started with two days in Tehran , a gigantic and lovely city where an Iranian friend we met on the internet invited us to a treck in the surroundings montains. The first thing to have to get acquainted with is how to cross the streets. Seriously the traffic is madness and uncontrolled (no traffic lights) so you have to watch out everywhere. Then you discover their nice and cooled subway which is clean and shiny and so cheap you try and count four times as you're sure you've heard the man wrong : 10 000 rials a ticket, yeah that's like 0,2\u20ac... Treck startpoint was Darban and then up to close to 3000m, thanks again Shayan. When we were awed by the beautiful mountains they had at the doorstep or their city, our Iranian friends told us that these mountains are also the cause of the massive pollution of Tehran in sping and summer because it blocks the winds from clearing the air... Sunset from the Jamshidieh park , the tall tower on the background is the Milad Tower.","title":"Iran : Tehran"},{"location":"Tech%20Blog/2014/2014-05-23-rsyslog-v7-6-3/","text":"This version bump was long overdue sorry and it has happened only thanks to the great work of Thomas D. aka @ Whissi , thanks again mate. Please read carefully because this version introduces major ebuild changes , you'll probably have to adapt your current configuration ! ebuild changes \u00b6 New default configuration with up to date syntax, we changed a lot of things so I'll quote the README coming with this bump. \"/var/log/syslog\" log file is now deprecated Beginning with rsyslog-7.6, the \"/var/log/syslog\" log file will no longer being written per default. We are considering this file as deprecated/obsolet for the typical user/system. The content from this log file is still availble through other (dedicated) log files, see - /var/log/cron.log - /var/log/daemon.log - /var/log/mail.log - /var/log/messages If you really need the old \"/var/log/syslog\" log file, all you have to do is uncommenting the corresponding configuration directive in \"/etc/rsyslog.d/50-default.conf\". If you do so, don't forget to re-enable log rotation in \"/etc/logrotate.d/rsyslog\", too. An additional input socket in /var/empty/dev/log (default chroot location) will be created per default brand new and modern init script rsyslog-7.6.3 \u00b6 Coming from the rsyslog release announcement page , this is what happened with the 7.6 branch release : With 7.6 being the successor of the 7.5 development branch, everything that has been added there has now found its way into the stable version. The major additions consist of : - imrelp/omrelp now support TLS & (zip) compression - impstats is now emitting resource usage counters, can directly emit delta values and can now be bound to a ruleset - mmpstrucdata is a new module to parse RFC5424 structured data into JSON message properties - mmutf8fix is a new module to fix invalid UTF-8 sequences - mmsequence is a new module that helps with action load balancing - new defaults for main/ruleset queues to be more enterprise-like Also the new stable version has undergone a lot of bug fixes, performance improvements and optimizations that make rsyslog 7.6 a lot more reliable and performing than before.","title":"rsyslog v7.6.3"},{"location":"Tech%20Blog/2014/2014-05-23-rsyslog-v7-6-3/#ebuild-changes","text":"New default configuration with up to date syntax, we changed a lot of things so I'll quote the README coming with this bump. \"/var/log/syslog\" log file is now deprecated Beginning with rsyslog-7.6, the \"/var/log/syslog\" log file will no longer being written per default. We are considering this file as deprecated/obsolet for the typical user/system. The content from this log file is still availble through other (dedicated) log files, see - /var/log/cron.log - /var/log/daemon.log - /var/log/mail.log - /var/log/messages If you really need the old \"/var/log/syslog\" log file, all you have to do is uncommenting the corresponding configuration directive in \"/etc/rsyslog.d/50-default.conf\". If you do so, don't forget to re-enable log rotation in \"/etc/logrotate.d/rsyslog\", too. An additional input socket in /var/empty/dev/log (default chroot location) will be created per default brand new and modern init script","title":"ebuild changes"},{"location":"Tech%20Blog/2014/2014-05-23-rsyslog-v7-6-3/#rsyslog-763","text":"Coming from the rsyslog release announcement page , this is what happened with the 7.6 branch release : With 7.6 being the successor of the 7.5 development branch, everything that has been added there has now found its way into the stable version. The major additions consist of : - imrelp/omrelp now support TLS & (zip) compression - impstats is now emitting resource usage counters, can directly emit delta values and can now be bound to a ruleset - mmpstrucdata is a new module to parse RFC5424 structured data into JSON message properties - mmutf8fix is a new module to fix invalid UTF-8 sequences - mmsequence is a new module that helps with action load balancing - new defaults for main/ruleset queues to be more enterprise-like Also the new stable version has undergone a lot of bug fixes, performance improvements and optimizations that make rsyslog 7.6 a lot more reliable and performing than before.","title":"rsyslog-7.6.3"},{"location":"Tech%20Blog/2014/2014-05-25-iran-shiraz/","text":"Our Iranian friend booked us a night train from Tehran to our next stop in the South of Iran : Shiraz . The Tehran train station is well organized and as foreigners we had to show our passports to the station's police where we were greeted with the big smile by the policemen. The train itself was good, clean, comfortable and on schedule even if it was quite slow : departure at 20h and we arrived at 11h30 a few kilometers up from Shiraz. Travelling by train is always a great experience. As we woke up early in the morning, this is what was waiting for us behind the curtains : Compared to Tehran, the streets are more typical and fitting what you'd expect from a desert city. That's the exact kind of town you want to get lost in, just walking around and going into its nice and large bazaar. Shiraz is known for its many palaces and beautiful gardens. This city is also the usual starting point to visit the famous Persepolis . While the place is great, I must admit that we loved the abandonned park right next to it which is used as a playground and food camping by the schools and other Iranian tourists.","title":"Iran : Shiraz"},{"location":"Tech%20Blog/2014/2014-06-02-uwsgi-v2-0-5-1/","text":"This release is important to me (and my company) as it officially introduces a few features we developed for our needs and then contributed to uWSGI. Special congratulations to my co-worker @btall for his first contribution and for those nice features to the metrics subsystem with many thanks as usual to @unbit for reviewing and merging them so quickly. new features \u00b6 graceful reload of mule processes (Credits: Paul Egan) : SIGHUP is now sent to mules instead of directly killing them, by default you have 60 seconds to react before a SIGKILL \u2013metrics-no-cores, \u2013stats-no-cores, \u2013stats-no-metrics : don't calculate and process all those core related metrics (gevent anyone ?) reset_after_push for metrics (Credits: Babacar Tall) : this metric attribute ensures that the metric value is reset to 0 or its hardcoded initial_value every time the metric is pushed to some external system (like carbon, or statsd) new metric_set_max and metric_set_min helpers (Credits: Babacar Tall) : can be used to avoid having to call ``metric_get`` when you need a metric to be set at a maximal or minimal value. Another simple use case is to use the ``avg`` collector to calculate an average between some *max* and *min* set metrics. Available in C and python. See the full changelog here, especially some interesting bugfixes.","title":"uWSGI v2.0.5.1"},{"location":"Tech%20Blog/2014/2014-06-02-uwsgi-v2-0-5-1/#new-features","text":"graceful reload of mule processes (Credits: Paul Egan) : SIGHUP is now sent to mules instead of directly killing them, by default you have 60 seconds to react before a SIGKILL \u2013metrics-no-cores, \u2013stats-no-cores, \u2013stats-no-metrics : don't calculate and process all those core related metrics (gevent anyone ?) reset_after_push for metrics (Credits: Babacar Tall) : this metric attribute ensures that the metric value is reset to 0 or its hardcoded initial_value every time the metric is pushed to some external system (like carbon, or statsd) new metric_set_max and metric_set_min helpers (Credits: Babacar Tall) : can be used to avoid having to call ``metric_get`` when you need a metric to be set at a maximal or minimal value. Another simple use case is to use the ``avg`` collector to calculate an average between some *max* and *min* set metrics. Available in C and python. See the full changelog here, especially some interesting bugfixes.","title":"new features"},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/","text":"As a clustering and distributed architecture enthusiast, I'm naturally interested in software providing neat ways to coordinate any kind of state/configuration/you-name-it over a large number of machines. My quest, as many of you I guess, were so far limited to tools like zookeeper ( packaged on my overlay but with almost no echo ) and doozerd (last commit nearly 6 months ago) which both cover some of the goals listed above with more or less flavors and elegance (sorry guys, JAVA is NOT elegant to me). I recently heard about consul , a new attempt to solve some of those problems in an interesting way while providing some rich fuctionnalities so I went on giving it a try and naturally started packaging it so others can too. WTF is consul ? \u00b6 Consul is a few months' old project ( and already available on Gentoo ! ) from the guys making Vagrant . I especially like its datacenter centric architecture, intuitive deployment and its DNS + HTTP API query mecanisms. This sounds promising so far ! This is a descripion taken from the Hashicorp's blog : Consul is a solution for service discovery and configuration. Consul is completely distributed, highly available, and scales to thousands of nodes and services across multiple datacenters. Some concrete problems Consul solves: finding the services applications need (database, queue, mail server, etc.), configuring services with key/value information such as enabling maintenance mode for a web application, and health checking services so that unhealthy services aren\u2019t used. These are just a handful of important problems Consul addresses. Consul solves the problem of service discovery and configuration. Built on top of a foundation of rigorous academic research, Consul keeps your data safe and works with the largest of infrastructures. Consul embraces modern practices and is friendly to existing DevOps tooling. app-admin/consul ? \u00b6 This is a RFC and interest call about the packaging and availability of consul for Gentoo Linux. The latest version and live ebuilds are present in my overlay so if you are interested, please tell me (here, IRC, email, whatever) and I'll consider adding it to the portage tree. I want to test it ! \u00b6 Now that would be helpful to get some feedback about the usability of the current packaging. So far the ebuild features what I think should cover a lot of use cases : full build from sources customizable consul agent init script with reload, telemetry and graceful stop support web UI built from sources and installation for easy deployment # layman -a ultrabug emerge -av consul \u00b6 Hope this interests some of you folks !","title":"Consul on Gentoo Linux"},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/#wtf-is-consul","text":"Consul is a few months' old project ( and already available on Gentoo ! ) from the guys making Vagrant . I especially like its datacenter centric architecture, intuitive deployment and its DNS + HTTP API query mecanisms. This sounds promising so far ! This is a descripion taken from the Hashicorp's blog : Consul is a solution for service discovery and configuration. Consul is completely distributed, highly available, and scales to thousands of nodes and services across multiple datacenters. Some concrete problems Consul solves: finding the services applications need (database, queue, mail server, etc.), configuring services with key/value information such as enabling maintenance mode for a web application, and health checking services so that unhealthy services aren\u2019t used. These are just a handful of important problems Consul addresses. Consul solves the problem of service discovery and configuration. Built on top of a foundation of rigorous academic research, Consul keeps your data safe and works with the largest of infrastructures. Consul embraces modern practices and is friendly to existing DevOps tooling.","title":"WTF is consul ?"},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/#app-adminconsul","text":"This is a RFC and interest call about the packaging and availability of consul for Gentoo Linux. The latest version and live ebuilds are present in my overlay so if you are interested, please tell me (here, IRC, email, whatever) and I'll consider adding it to the portage tree.","title":"app-admin/consul ?"},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/#i-want-to-test-it","text":"Now that would be helpful to get some feedback about the usability of the current packaging. So far the ebuild features what I think should cover a lot of use cases : full build from sources customizable consul agent init script with reload, telemetry and graceful stop support web UI built from sources and installation for easy deployment # layman -a ultrabug","title":"I want to test it !"},{"location":"Tech%20Blog/2014/2014-06-04-consul-on-gentoo-linux/#emerge-av-consul","text":"Hope this interests some of you folks !","title":"emerge -av consul"},{"location":"Tech%20Blog/2014/2014-06-15-iran-yazd/","text":"We took our first bus trip to reach Yazd from Shiraz using the Hamsafar company. Booking a bus trip is as easy as it is cheap in Iran so this is by far the best way to get around even tho it's a bit slow mostly due to the police controls along the road. Yazd was a shock as it's a small and beautiful desert town with an unique athmosphere. This city still haunts me and remains my favorite of the trip. The baazar, the covered streets and the mud walls gives you a feeling which is difficult to describe. We stayed at the very pleasant Orient Hotel and spent one night in a caravanserai where I tried my luck and succeeded to rent a motorcycle for one day ! That was a fun and incredible experience and we'll always remember the look on the amuzed face of the Iranian people when they realized some tourists where riding a motorcycle among them. Yazd is of course not only about desert and features some beautiful and peaceful gardens.","title":"Iran : Yazd"},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/","text":"I had the chance to participate to europython 2014 as my company was sponsoring the event. This was a great week where I got to meet some very interesting people and hear about some neat python use cases, libraries and new technologies so I thought I'd write a quick summary of my biased point of view. ZeroMQ \u00b6 I had the chance to meet Pieter Hintjens and participate in a 3 hours workshop on ZeroMQ . This was very interesting and refreshing as to go in more depth into this technology which I've been using in production for several years now. Pieter is also quite a philosophical person and I strongly encourage you to listen to his keynote . I ended up pinging him in real life for an issue I've been waiting for bug correction on the libzmq and it got answered nicely. uWSGI \u00b6 Another big thing in our python stack is the uWSGI application container which I love and follow closely even if my lack of knowledge in C++ prevents me for going too deep in the source code... I got the chance to speak with Roberto De Ioris about the next 2.1 release and propose him two new features. Allow message spooling directly from a gevent greenlet : I'm glad to say it's been committed today and I successfully test it, huge feature ! Consul.io integration in uWSGI : I wrote a RFC about it and it's going to be done for the great good of uWSGI users and distributed architecture lovers ! Thanks a lot for your consideration Roberto ! Trends \u00b6 Not tested = broken ! Python is strong and very lively in the Big Data world Asynchronous and distributed architectures get more and more traction and interest Videos \u00b6 All the talks videos are already online , you should check them out !","title":"Europython 2014"},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/#zeromq","text":"I had the chance to meet Pieter Hintjens and participate in a 3 hours workshop on ZeroMQ . This was very interesting and refreshing as to go in more depth into this technology which I've been using in production for several years now. Pieter is also quite a philosophical person and I strongly encourage you to listen to his keynote . I ended up pinging him in real life for an issue I've been waiting for bug correction on the libzmq and it got answered nicely.","title":"ZeroMQ"},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/#uwsgi","text":"Another big thing in our python stack is the uWSGI application container which I love and follow closely even if my lack of knowledge in C++ prevents me for going too deep in the source code... I got the chance to speak with Roberto De Ioris about the next 2.1 release and propose him two new features. Allow message spooling directly from a gevent greenlet : I'm glad to say it's been committed today and I successfully test it, huge feature ! Consul.io integration in uWSGI : I wrote a RFC about it and it's going to be done for the great good of uWSGI users and distributed architecture lovers ! Thanks a lot for your consideration Roberto !","title":"uWSGI"},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/#trends","text":"Not tested = broken ! Python is strong and very lively in the Big Data world Asynchronous and distributed architectures get more and more traction and interest","title":"Trends"},{"location":"Tech%20Blog/2014/2014-08-01-europython-2014/#videos","text":"All the talks videos are already online , you should check them out !","title":"Videos"},{"location":"Tech%20Blog/2014/2014-08-12-hd-daymaker-led-headlamp/","text":"Short post to share my experience with the Harley-Davidson Daymaker LED Headlamp . I came to buy it because I was not satisfied with the standard lamp fitted on my sportster and I guess whoever has to drive by night would feel that unpleasant feeling to not actually be able to properly see what's going on in front of you. The LED Headlamp is worth the few hundred bucks it costs if at least for the sake of your own life but furthermore for the incredible improvement from the standard lamp. Don't hesitate a second just go for it and it's dead simple to mount yourself ! See the difference (passing lights) : Now I feel way safer to drive on unlitten roads.","title":"HD Daymaker LED Headlamp"},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/","text":"Foreword \u00b6 Let's say we have to design an application that should span across multiple datacenters while being able to scale as easily as firing up a new vm/container without the need to update any kind of configuration. Facing this kind of challenge is exciting and requires us to address a few key scaffolding points before actually starting to code something : having a robust and yet versatile application container to run our application having a datacenter aware, fault detecting and service discovery service Seeing the title of this article, the two components I'll demonstrate are obviously uWSGI and Consul which can now work together thanks to the uwsgi-consul plugin . While this article example is written in python, you can benefit from the same features in all the languages supported by uWSGI which includes go, ruby, perl ad php ! Our first service discovering application \u00b6 The application will demonstrate how simple it is for a client to discover all the available servers running a specific service on a given port . The best part is that the services will be registered and deregistered automatically by uWSGI as they're loaded and unloaded. The demo application logic is as follows : uWSGI will load two server applications which are each responsible for providing the specified service on the given port uWSGI will automatically register the configured service into Consul uWSGI will also automatically register a health check for the configured service into Consul so that Consul will also be able to detect any failure of the service Consul will then respond to any client requesting the list of the available servers (nodes) providing the specified service The client will query Consul for the service and get either an empty response (no server available / loaded) or the list of the available servers Et voil\u00e0, the client can dynamically detect new/obsolete servers and start working ! Setting up uWSGI and its Consul plugin \u00b6 On Gentoo Linux, you'll just have to run the following commands to get started (other users refer to the uWSGI documentation or your distro's package manager). The plugin will be built by hand as I'm still not sure how I'll package the uWSGI external plugins... $ sudo ACCEPT_KEYWORDS=\"~amd64\" emerge uwsgi $ cd /usr/lib/uwsgi/ $ sudo uwsgi --build-plugin https://github.com/unbit/uwsgi-consul $ cd - You'll have installed the uwsgi-consul plugin which you should see here : $ ls /usr/lib/uwsgi/consul_plugin.so /usr/lib/uwsgi/consul_plugin.so That's all we need to have uWSGI working with Consul. Setting up a Consul server \u00b6 Gentoo users will need to add the ultrabug overlay (use layman) and then install consul (other users refer to the Consul documentation or your distro's package manager). $ sudo layman -a ultrabug $ sudo ACCEPT_KEYWORDS=\"~amd64\" USE=\"web\" emerge consul Running the server and its UI is also quite straightforward. For this example, we will run it directly from a dedicated terminal so you can also enjoy the logs and see what's going on (Gentoo users have an init script and conf.d ready for them shall they wish to go further). Open a new terminal and run : $ consul agent -data-dir=/tmp/consul-agent -server -bootstrap -ui-dir=/var/lib/consul/ui -client=0.0.0.0 You'll see consul running and waiting for work. You can already enjoy the web UI by pointing your browser to http://127.0.0.1:8500/ui/ . Running the application \u00b6 To get this example running, we'll use the uwsgi-consul-demo code that I prepared. First of all we'll need the consulate python library (available on pypi via pip). Gentoo users can just install it (also from the ultrabug overlay added before) : $ sudo ACCEPT_KEYWORDS=\"~amd64\" emerge consulate Now let's clone the demo repository and get into the project's directory. $ git clone git@github.com:ultrabug/uwsgi-consul-demo.git $ cd uwsgi-consul-demo First, we'll run the client which should report that no server is available yet. We will keep this terminal open to see the client detecting in real time the appearance and disappearance of the servers as we start and stop uwsgi : $ python client.py no consul-demo-server available [...] no consul-demo-server available Open a new terminal and get inside the project's directory. Let's have uWSGI load the two servers and register them in Consul : $ uwsgi --ini uwsgi-consul-demo.ini --ini uwsgi-consul-demo.ini:server1 --ini uwsgi-consul-demo.ini:server2 [...] * server #1 is up on port 2001 * server #2 is up on port 2002 [consul] workers ready, let's register the service to the agent [consul] service consul-demo-server registered succesfully [consul] workers ready, let's register the service to the agent [consul] service consul-demo-server registered succesfully Now let's check back our client terminal, hooray it has discovered the two servers on the host named drakar (that's my local box) ! consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2002 consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2001 Expanding our application \u00b6 Ok it works great on our local machine but we want to see how to add more servers to the fun and scale dynamically. Let's add another machine (named cheetah here) to the fun and have servers running there also while our client is still running on our local machine. On cheetah : install uWSGI as described earlier install Consul as described earlier Run a Consul agent (no need of a server) and tell him to work with your already running consul server on your box ( drakar in my case) : $ /usr/bin/consul agent -data-dir=/tmp/consul-agent -join drakar -ui-dir=/var/lib/consul/ui -client=0.0.0.0 The -join is the important part. Now run uWSGI so it starts and registers two new servers on cheetah : $ uwsgi --ini uwsgi-consul-demo.ini --ini uwsgi-consul-demo.ini:server1 --ini uwsgi-consul-demo.ini:server2 And check the miracle on your client terminal still running on your local box, the new servers have appeared and will disappear if you stop uwsgi on the cheetah node : consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2001 consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2002 consul-demo-server found on node cheetah (yy.yy.yy.yy) using port 2001 consul-demo-server found on node cheetah (yy.yy.yy.yy) using port 2002 Go mad \u00b6 Check the source code, it's so simple and efficient you'll cry ;) I hope this example has given you some insights and ideas for your current or future application designs !","title":"Using uWSGI and Consul to design a distributed application"},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#foreword","text":"Let's say we have to design an application that should span across multiple datacenters while being able to scale as easily as firing up a new vm/container without the need to update any kind of configuration. Facing this kind of challenge is exciting and requires us to address a few key scaffolding points before actually starting to code something : having a robust and yet versatile application container to run our application having a datacenter aware, fault detecting and service discovery service Seeing the title of this article, the two components I'll demonstrate are obviously uWSGI and Consul which can now work together thanks to the uwsgi-consul plugin . While this article example is written in python, you can benefit from the same features in all the languages supported by uWSGI which includes go, ruby, perl ad php !","title":"Foreword"},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#our-first-service-discovering-application","text":"The application will demonstrate how simple it is for a client to discover all the available servers running a specific service on a given port . The best part is that the services will be registered and deregistered automatically by uWSGI as they're loaded and unloaded. The demo application logic is as follows : uWSGI will load two server applications which are each responsible for providing the specified service on the given port uWSGI will automatically register the configured service into Consul uWSGI will also automatically register a health check for the configured service into Consul so that Consul will also be able to detect any failure of the service Consul will then respond to any client requesting the list of the available servers (nodes) providing the specified service The client will query Consul for the service and get either an empty response (no server available / loaded) or the list of the available servers Et voil\u00e0, the client can dynamically detect new/obsolete servers and start working !","title":"Our first service discovering application"},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#setting-up-uwsgi-and-its-consul-plugin","text":"On Gentoo Linux, you'll just have to run the following commands to get started (other users refer to the uWSGI documentation or your distro's package manager). The plugin will be built by hand as I'm still not sure how I'll package the uWSGI external plugins... $ sudo ACCEPT_KEYWORDS=\"~amd64\" emerge uwsgi $ cd /usr/lib/uwsgi/ $ sudo uwsgi --build-plugin https://github.com/unbit/uwsgi-consul $ cd - You'll have installed the uwsgi-consul plugin which you should see here : $ ls /usr/lib/uwsgi/consul_plugin.so /usr/lib/uwsgi/consul_plugin.so That's all we need to have uWSGI working with Consul.","title":"Setting up uWSGI and its Consul plugin"},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#setting-up-a-consul-server","text":"Gentoo users will need to add the ultrabug overlay (use layman) and then install consul (other users refer to the Consul documentation or your distro's package manager). $ sudo layman -a ultrabug $ sudo ACCEPT_KEYWORDS=\"~amd64\" USE=\"web\" emerge consul Running the server and its UI is also quite straightforward. For this example, we will run it directly from a dedicated terminal so you can also enjoy the logs and see what's going on (Gentoo users have an init script and conf.d ready for them shall they wish to go further). Open a new terminal and run : $ consul agent -data-dir=/tmp/consul-agent -server -bootstrap -ui-dir=/var/lib/consul/ui -client=0.0.0.0 You'll see consul running and waiting for work. You can already enjoy the web UI by pointing your browser to http://127.0.0.1:8500/ui/ .","title":"Setting up a Consul server"},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#running-the-application","text":"To get this example running, we'll use the uwsgi-consul-demo code that I prepared. First of all we'll need the consulate python library (available on pypi via pip). Gentoo users can just install it (also from the ultrabug overlay added before) : $ sudo ACCEPT_KEYWORDS=\"~amd64\" emerge consulate Now let's clone the demo repository and get into the project's directory. $ git clone git@github.com:ultrabug/uwsgi-consul-demo.git $ cd uwsgi-consul-demo First, we'll run the client which should report that no server is available yet. We will keep this terminal open to see the client detecting in real time the appearance and disappearance of the servers as we start and stop uwsgi : $ python client.py no consul-demo-server available [...] no consul-demo-server available Open a new terminal and get inside the project's directory. Let's have uWSGI load the two servers and register them in Consul : $ uwsgi --ini uwsgi-consul-demo.ini --ini uwsgi-consul-demo.ini:server1 --ini uwsgi-consul-demo.ini:server2 [...] * server #1 is up on port 2001 * server #2 is up on port 2002 [consul] workers ready, let's register the service to the agent [consul] service consul-demo-server registered succesfully [consul] workers ready, let's register the service to the agent [consul] service consul-demo-server registered succesfully Now let's check back our client terminal, hooray it has discovered the two servers on the host named drakar (that's my local box) ! consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2002 consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2001","title":"Running the application"},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#expanding-our-application","text":"Ok it works great on our local machine but we want to see how to add more servers to the fun and scale dynamically. Let's add another machine (named cheetah here) to the fun and have servers running there also while our client is still running on our local machine. On cheetah : install uWSGI as described earlier install Consul as described earlier Run a Consul agent (no need of a server) and tell him to work with your already running consul server on your box ( drakar in my case) : $ /usr/bin/consul agent -data-dir=/tmp/consul-agent -join drakar -ui-dir=/var/lib/consul/ui -client=0.0.0.0 The -join is the important part. Now run uWSGI so it starts and registers two new servers on cheetah : $ uwsgi --ini uwsgi-consul-demo.ini --ini uwsgi-consul-demo.ini:server1 --ini uwsgi-consul-demo.ini:server2 And check the miracle on your client terminal still running on your local box, the new servers have appeared and will disappear if you stop uwsgi on the cheetah node : consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2001 consul-demo-server found on node drakar (xx.xx.xx.xx) using port 2002 consul-demo-server found on node cheetah (yy.yy.yy.yy) using port 2001 consul-demo-server found on node cheetah (yy.yy.yy.yy) using port 2002","title":"Expanding our application"},{"location":"Tech%20Blog/2014/2014-08-14-using-uwsgi-and-consul-to-design-a-distributed-application/#go-mad","text":"Check the source code, it's so simple and efficient you'll cry ;) I hope this example has given you some insights and ideas for your current or future application designs !","title":"Go mad"},{"location":"Tech%20Blog/2014/2014-10-08-py3status-v1-6/","text":"Back from holidays, this new version of py3status was due for a long time now as it features a lot of great contributions ! This version is dedicated to the amazing @ShadowPrince who contributed 6 new modules :) Changelog \u00b6 core : rename the 'examples' folder to 'modules' core : Fix include_paths default wrt issue #38, by Frank Haun new vnstat module, by Vasiliy Horbachenko new net_rate module, alternative module for tracking network rate, by Vasiliy Horbachenko new scratchpad-counter module and window-title module for displaying current windows title, by Vasiliy Horbachenko new keyboard-layout module, by Vasiliy Horbachenko new mpd_status module, by Vasiliy Horbachenko new clementine module displaying the current \"artist - title\" playing in Clementine, by Fran\u00e7ois LASSERRE module clementine.py: Make python3 compatible, by Frank Haun add optional CPU temperature to the sysdata module, by Rayeshman Contributors \u00b6 Huge thanks to this release's contributors : @ChoiZ @fhaun @rayeshman @ShadowPrince What's next ? \u00b6 The next 1.7 release of py3status will bring a neat and cool feature which I'm sure you'll love, stay tuned !","title":"py3status v1.6"},{"location":"Tech%20Blog/2014/2014-10-08-py3status-v1-6/#changelog","text":"core : rename the 'examples' folder to 'modules' core : Fix include_paths default wrt issue #38, by Frank Haun new vnstat module, by Vasiliy Horbachenko new net_rate module, alternative module for tracking network rate, by Vasiliy Horbachenko new scratchpad-counter module and window-title module for displaying current windows title, by Vasiliy Horbachenko new keyboard-layout module, by Vasiliy Horbachenko new mpd_status module, by Vasiliy Horbachenko new clementine module displaying the current \"artist - title\" playing in Clementine, by Fran\u00e7ois LASSERRE module clementine.py: Make python3 compatible, by Frank Haun add optional CPU temperature to the sysdata module, by Rayeshman","title":"Changelog"},{"location":"Tech%20Blog/2014/2014-10-08-py3status-v1-6/#contributors","text":"Huge thanks to this release's contributors : @ChoiZ @fhaun @rayeshman @ShadowPrince","title":"Contributors"},{"location":"Tech%20Blog/2014/2014-10-08-py3status-v1-6/#whats-next","text":"The next 1.7 release of py3status will bring a neat and cool feature which I'm sure you'll love, stay tuned !","title":"What's next ?"},{"location":"Tech%20Blog/2014/2014-10-14-one-month-in-turkey/","text":"Our latest roadtrip was as amazing as it was challenging because we decided that we'd spend an entire month in Turkey and use our own motorbike to get there from Paris. Transportation \u00b6 Our main idea was to spare ourselves from the long hours of road riding to Turkey so we decided from the start to use ferries to get there. Turns out that it's pretty easy as you have to go through Italy and Greece before you set foot in Bodrum, Turkey. Paris -> Nice : train Nice -> Parma (IT) -> Ancona : road, (~7h drive) Ancona -> Patras (GR) : ferry (21h) Patras -> Piraeus (Athens) : road (~4h drive, constructions) Piraeus -> Kos : ferry (~11h by night) Kos -> Bodrum (TR) : ferry (1h) Turkish customs are very friendly and polite, it's really easy to get in with your own vehicle. Tribute to the Nightster \u00b6 This roadtrip added 6000 kms to our brave and astonishing Harley-Davidson Nightster. We encountered no problem at all with the bike even though we clearly didn't go easy on her. We rode on gravels, dirt and mud without her complaining, not to mention the weight of our luggages and the passengers ;) That's why this post will be dedicated to our bike and I'll share some of the photos I took of it during the trip. The real photos will come in some other posts. A quick photo tour \u00b6 I can't describe well enough the pleasure and freedom feeling you get when travelling in motorbike so I hope those first photos will give you an idea. I have to admit that it's really impressive to leave your bike alone between the numerous trucks parking, loading/unloading their stuff a few centimeters from it. We arrived in Piraeus easily, time to buy tickets for the next boat to Kos. Kos is quite a big island that you can discover best by ... riding around ! After Bodrum, where we only spent the night, you quickly discover the true nature of Turkish roads and scenery. Animals are everywhere and sometimes on the road such as those donkeys below. This is a view from the Bozburun bay. Two photos for two bike layouts : beach version and fully loaded version ;) On the way to Cappadocia, near Karapinar : The amazing landscapes of Cappadocia, after two weeks by the sea it felt cold up there. Our last picture from the bike next to the trail leading to our favorite and lonely \"private\" beach on the Dat\u00e7a peninsula.","title":"One month in Turkey"},{"location":"Tech%20Blog/2014/2014-10-14-one-month-in-turkey/#transportation","text":"Our main idea was to spare ourselves from the long hours of road riding to Turkey so we decided from the start to use ferries to get there. Turns out that it's pretty easy as you have to go through Italy and Greece before you set foot in Bodrum, Turkey. Paris -> Nice : train Nice -> Parma (IT) -> Ancona : road, (~7h drive) Ancona -> Patras (GR) : ferry (21h) Patras -> Piraeus (Athens) : road (~4h drive, constructions) Piraeus -> Kos : ferry (~11h by night) Kos -> Bodrum (TR) : ferry (1h) Turkish customs are very friendly and polite, it's really easy to get in with your own vehicle.","title":"Transportation"},{"location":"Tech%20Blog/2014/2014-10-14-one-month-in-turkey/#tribute-to-the-nightster","text":"This roadtrip added 6000 kms to our brave and astonishing Harley-Davidson Nightster. We encountered no problem at all with the bike even though we clearly didn't go easy on her. We rode on gravels, dirt and mud without her complaining, not to mention the weight of our luggages and the passengers ;) That's why this post will be dedicated to our bike and I'll share some of the photos I took of it during the trip. The real photos will come in some other posts.","title":"Tribute to the Nightster"},{"location":"Tech%20Blog/2014/2014-10-14-one-month-in-turkey/#a-quick-photo-tour","text":"I can't describe well enough the pleasure and freedom feeling you get when travelling in motorbike so I hope those first photos will give you an idea. I have to admit that it's really impressive to leave your bike alone between the numerous trucks parking, loading/unloading their stuff a few centimeters from it. We arrived in Piraeus easily, time to buy tickets for the next boat to Kos. Kos is quite a big island that you can discover best by ... riding around ! After Bodrum, where we only spent the night, you quickly discover the true nature of Turkish roads and scenery. Animals are everywhere and sometimes on the road such as those donkeys below. This is a view from the Bozburun bay. Two photos for two bike layouts : beach version and fully loaded version ;) On the way to Cappadocia, near Karapinar : The amazing landscapes of Cappadocia, after two weeks by the sea it felt cold up there. Our last picture from the bike next to the trail leading to our favorite and lonely \"private\" beach on the Dat\u00e7a peninsula.","title":"A quick photo tour"},{"location":"Tech%20Blog/2014/2014-11-20-rip-ns2/","text":"Today we did shutdown our now oldest running Gentoo Linux production server : ns2 . Obviously this machine was happily spreading our DNS records around the world but what's remarkable about it is that it has been doing so for 2717 straight days ! $ uptime 13:00:45 up 2717 days, 2:20, 1 user, load average: 0.13, 0.04, 0.01 As I mentioned when we did shutdown stabber , our beloved firewall, our company has been running Gentoo Linux servers in production for a long time now and we're always a bit sad when we have to power off one of them. As usual, I want to take this chance to thank everyone contributing to Gentoo Linux ! Without our collective work, none of this would have been possible.","title":"RIP ns2"},{"location":"Tech%20Blog/2014/2014-12-20-gentoo-linux-pxe-builder/","text":"Due to a bad hardware failure a few weeks ago at work, I had to rebuild a good part of our PXE stack and I ended up once again looking for the steps to build a PXE-ready Gentoo initramfs . Then I realized that, while I was at it, I wanted this PXE initramfs to feature more than a Live CD like boot because I use PXE to actually install my servers automatically using ansible. So why not embed all my needs straight into the PXE initramfs and automate the whole boring creation process of it ? That what the gentoo-pxe-builder project is about and I thought I'd open source it in case it could help and spare some time to anyone else. The main idea is to provide a simple bash script which bases itself on the latest Gentoo liveCD kernel/initramfs to prepare a PXE suitable version which you can easily hack into without having to handle all the squashfs/cpio hassle to rebuild it. Quick steps it does for you : download the latest live CD extract the kernel / initramfs from it patch the embedded squashfs to make it PXE ready setup SSH and a default root password so you can connect to your PXE booted machine directly add a hackable local.d start script which will be executed at the end of the PXE boot The provided local.d start script provides IP address display so you can actually see the IP address being setup on your PXE host and it will also display the real name of the network interfaces detected on the host based on udev deterministic naming . You can read everything in more details on the project's README . Of course it's mainly oriented to my use case and I'm sure the process / patching could be even more elegant so anyone feel free to contribute or ask/propose some features, I'll happily follow them up !","title":"Gentoo Linux PXE builder"},{"location":"Tech%20Blog/2015/2015-01-04-py3status-v2-0/","text":"I'm very pleased to announce the release of py3status v2.0 which I'd like to dedicate to the person who's behind all the nice improvements this release features : @tablet-mode ! His idea on issue #44 was to make py3status modules configurable. After some thoughts and merges of my own plans of development, we ended up with what I believe are the most ambitious features py3status provides so far. Features \u00b6 The logic behind this release is that py3status now wraps and extends your i3status.conf which allows all the following crazy features : Handle all click events directly from your i3status config For all your i3bar modules i3status and py3status alike thanks to the new on_click parameter which you can use like any other i3status.conf parameter on all modules. It has never been so easy to handle click events ! This is a quick and small example of what it looks like : # run thunar when I left click on the / disk info module disk / { format = \"/ %free\" on_click 1 = \"exec thunar /\" } All py3status contributed modules are now shipped and usable directly without the need to copy them to your local folder . They also get to be configurable directly from your i3status config (see below) No need to copy and edit the contributed py3status modules you like and wish to use, you can now load and configure them directly from your i3status.conf . Load, configure and order py3status modules directly from your i3status config just like any other i3status module All py3status modules (contributed ones and user loaded ones) are now loaded and ordered using the usual syntax order += in your i3status.conf ! All modules have been improved, cleaned up and some of them got some love from contributors. Every click event now triggers a refresh of the clicked module, even for i3status modules. This makes your i3bar more responsive than ever ! Contributors \u00b6 @AdamBSteele @obb @scotte @tablet-mode Thank you \u00b6 Jakub Jedelsky : py3status is now packaged on Fedora Linux. All of you users : py3status has broken the 100 stars on github, I'm still amazed by this. @Lujeni's prophecy has come true :) I still have some nice ideas in stock for even more functionalities, stay tuned !","title":"py3status v2.0"},{"location":"Tech%20Blog/2015/2015-01-04-py3status-v2-0/#features","text":"The logic behind this release is that py3status now wraps and extends your i3status.conf which allows all the following crazy features : Handle all click events directly from your i3status config For all your i3bar modules i3status and py3status alike thanks to the new on_click parameter which you can use like any other i3status.conf parameter on all modules. It has never been so easy to handle click events ! This is a quick and small example of what it looks like : # run thunar when I left click on the / disk info module disk / { format = \"/ %free\" on_click 1 = \"exec thunar /\" } All py3status contributed modules are now shipped and usable directly without the need to copy them to your local folder . They also get to be configurable directly from your i3status config (see below) No need to copy and edit the contributed py3status modules you like and wish to use, you can now load and configure them directly from your i3status.conf . Load, configure and order py3status modules directly from your i3status config just like any other i3status module All py3status modules (contributed ones and user loaded ones) are now loaded and ordered using the usual syntax order += in your i3status.conf ! All modules have been improved, cleaned up and some of them got some love from contributors. Every click event now triggers a refresh of the clicked module, even for i3status modules. This makes your i3bar more responsive than ever !","title":"Features"},{"location":"Tech%20Blog/2015/2015-01-04-py3status-v2-0/#contributors","text":"@AdamBSteele @obb @scotte @tablet-mode","title":"Contributors"},{"location":"Tech%20Blog/2015/2015-01-04-py3status-v2-0/#thank-you","text":"Jakub Jedelsky : py3status is now packaged on Fedora Linux. All of you users : py3status has broken the 100 stars on github, I'm still amazed by this. @Lujeni's prophecy has come true :) I still have some nice ideas in stock for even more functionalities, stay tuned !","title":"Thank you"},{"location":"Tech%20Blog/2015/2015-02-27-mongodb-2-6-8-2-4-13-the-upcoming-3-0-0/","text":"I'm a bit slacking on those follow-up posts but with the upcoming mongoDB 3.x series and the recent new releases I guess it was about time I talked a bit about what was going on. mongodb-3.0.0_rcX \u00b6 Thanks to the help of Tomas Mozes , we might get a release candidate version of the 3.0.0 version of mongoDB pretty soon in tree shall you want to test it on Gentoo. Feel free to contribute or give feedback in the bug, I'll do my best to keep up. What Tomas proposes matches what I had in mind so for now the plan is to : split the mongo tools (mongodump/export etc) to a new package : dev-db/mongo-tools or app-admin/mongo-tools ? split the MMS monitoring agent to its own package : app-admin/mms-monitoring-agent have a look at the MMS backup agent and maybe propose its own package if someone is interested in this ? after the first release, have a look at the MMS deployment automation to see how it could integrate with Gentoo mongodb-2.6.8 & 2.4.13 \u00b6 Released 2 days ago, they are already on portage ! The 2.4.13 is mostly a security (SSL v3) and tiny backport release whereas the 2.6.8 fixes quite a bunch of bugs . Please note that I will drop the 2.4.x releases when 3.0.0 hits the tree ! I will keep the latest 2.4.13 in my overlay if someone asks for it.","title":"mongoDB 2.6.8, 2.4.13 & the upcoming 3.0.0"},{"location":"Tech%20Blog/2015/2015-02-27-mongodb-2-6-8-2-4-13-the-upcoming-3-0-0/#mongodb-300_rcx","text":"Thanks to the help of Tomas Mozes , we might get a release candidate version of the 3.0.0 version of mongoDB pretty soon in tree shall you want to test it on Gentoo. Feel free to contribute or give feedback in the bug, I'll do my best to keep up. What Tomas proposes matches what I had in mind so for now the plan is to : split the mongo tools (mongodump/export etc) to a new package : dev-db/mongo-tools or app-admin/mongo-tools ? split the MMS monitoring agent to its own package : app-admin/mms-monitoring-agent have a look at the MMS backup agent and maybe propose its own package if someone is interested in this ? after the first release, have a look at the MMS deployment automation to see how it could integrate with Gentoo","title":"mongodb-3.0.0_rcX"},{"location":"Tech%20Blog/2015/2015-02-27-mongodb-2-6-8-2-4-13-the-upcoming-3-0-0/#mongodb-268-2413","text":"Released 2 days ago, they are already on portage ! The 2.4.13 is mostly a security (SSL v3) and tiny backport release whereas the 2.6.8 fixes quite a bunch of bugs . Please note that I will drop the 2.4.x releases when 3.0.0 hits the tree ! I will keep the latest 2.4.13 in my overlay if someone asks for it.","title":"mongodb-2.6.8 &amp; 2.4.13"},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/","text":"This is a quite awaited version bump coming to portage and I'm glad to announce it's made its way to the tree today ! I'll right away thank a lot Tomas Mozes and Darko Luketic for their amazing help, feedback and patience ! mongodb-3.0.1 \u00b6 I introduced quite some changes in this ebuild which I wanted to share with you and warn you about. MongoDB upstream have stripped quite a bunch of things out of the main mongo core repository which I have in turn split into ebuilds. Major changes : respect upstream's optimization flags : unless in debug build, user's optimization flags will be ignored to prevent crashes and weird behaviour . shared libraries for C/C++ are not built by the core mongo respository anymore, so I removed the static-libs USE flag. various dependencies optimization to trigger a rebuild of mongoDB when one of its linked dependency changes. app-admin/mongo-tools \u00b6 The new tools USE flag allows you to pull a new ebuild named app-admin/mongo-tools which installs the commands listed below. Obviously, you can now just install this package if you only need those tools on your machine. mongodump / mongorestore mongoexport / mongoimport mongotop mongofiles mongooplog mongostat bsondump app-admin/mms-agent \u00b6 The MMS agent has now some real version numbers and I don't have to host their source on Gentoo's infra woodpecker. At the moment there is only the monitoring agent available, shall anyone request the backup one, I'll be glad to add its support too. dev-libs/mongo-c(xx)-driver \u00b6 I took this opportunity to add the dev-libs/mongo-cxx-driver to the tree and bump the mongo-c-driver one. Thank you to Balint SZENTE for his insight on this.","title":"mongoDB 3.0.1"},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/#mongodb-301","text":"I introduced quite some changes in this ebuild which I wanted to share with you and warn you about. MongoDB upstream have stripped quite a bunch of things out of the main mongo core repository which I have in turn split into ebuilds. Major changes : respect upstream's optimization flags : unless in debug build, user's optimization flags will be ignored to prevent crashes and weird behaviour . shared libraries for C/C++ are not built by the core mongo respository anymore, so I removed the static-libs USE flag. various dependencies optimization to trigger a rebuild of mongoDB when one of its linked dependency changes.","title":"mongodb-3.0.1"},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/#app-adminmongo-tools","text":"The new tools USE flag allows you to pull a new ebuild named app-admin/mongo-tools which installs the commands listed below. Obviously, you can now just install this package if you only need those tools on your machine. mongodump / mongorestore mongoexport / mongoimport mongotop mongofiles mongooplog mongostat bsondump","title":"app-admin/mongo-tools"},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/#app-adminmms-agent","text":"The MMS agent has now some real version numbers and I don't have to host their source on Gentoo's infra woodpecker. At the moment there is only the monitoring agent available, shall anyone request the backup one, I'll be glad to add its support too.","title":"app-admin/mms-agent"},{"location":"Tech%20Blog/2015/2015-03-17-mongodb-3-0-1/#dev-libsmongo-cxx-driver","text":"I took this opportunity to add the dev-libs/mongo-cxx-driver to the tree and bump the mongo-c-driver one. Thank you to Balint SZENTE for his insight on this.","title":"dev-libs/mongo-c(xx)-driver"},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/","text":"I'm very pleased to announce this new release of py3status because it is by far the most contributed one with a total of 33 files changed, 1625 insertions and 509 deletions ! I'll start by thanking this release's contributors with a special mention for Federico Ceratto for his precious insights, his CLI idea and implementation and other modules contributions. Thank you \u00b6 Federico Ceratto @rixx (and her amazing reactivity) J.M. Dana @Gamonics @guilbep @lujeni @obb @shankargopal @thomas- IMPORTANT \u00b6 In order to keep a clean and efficient code base, this is the last version of py3status supporting the legacy modules loading and ordering , this behavior will be dropped on the next 2.5 version ! CLI commands \u00b6 py3status now supports some CLI commands which allows you to get information about all the available modules and their documentation . list all available modules if you specify your own inclusion folder(s) with the -i parameter, your modules will be listed too ! $ py3status modules list Available modules: battery_level Display the battery level. bitcoin_price Display bitcoin prices using bitcoincharts.com. bluetooth Display bluetooth status. clementine Display the current \"artist - title\" playing in Clementine. dpms Activate or deactivate DPMS and screen blanking. glpi Display the total number of open tickets from GLPI. imap Display the unread messages count from your IMAP account. keyboard_layout Display the current keyboard layout. mpd_status Display information from mpd. net_rate Display the current network transfer rate. netdata Display network speed and bandwidth usage. ns_checker Display DNS resolution success on a configured domain. online_status Display if a connection to the internet is established. pingdom Display the latest response time of the configured Pingdom checks. player_control Control music/video players. pomodoro Display and control a Pomodoro countdown. scratchpad_counter Display the amount of windows in your i3 scratchpad. spaceapi Display if your favorite hackerspace is open or not. spotify Display information about the current song playing on Spotify. sysdata Display system RAM and CPU utilization. vnstat Display vnstat statistics. weather_yahoo Display Yahoo! Weather forecast as icons. whoami Display the currently logged in user. window_title Display the current window title. xrandr Control your screen(s) layout easily. get available modules details and configuration $ py3status modules details Available modules: battery_level Display the battery level. Configuration parameters: - color\\_\\* : None means - get it from i3status config - format : text with \"text\" mode. percentage with % replaces {} - hide\\_when\\_full : hide any information when battery is fully charged - mode : for primitive-one-char bar, or \"text\" for text percentage ouput Requires: - the 'acpi' command line @author shadowprince, AdamBSteele @license Eclipse Public License --- [...] Modules changelog \u00b6 new bluetooth module by J.M. Dana new online_status module by @obb new player_control module, by Federico Ceratto new spotify module, by Pierre Guilbert new xrandr module to handle your screens layout from your bar dpms module activate/deactivate the screensaver as well imap module various configuration and optimizations pomodoro module can use DBUS notify, play sounds and be paused spaceapi module bugfix for space APIs without 'lastchange' field keyboard_layout module incorrect parsing of \"setxkbmap -query\" battery_level module better python3 compatibility Other highlights \u00b6 Full changelog here . catch daylight savings time change ensure modules methods are always iterated alphabetically refactor default config file detection rename and move the empty_class example module to the doc/ folder remove obsolete i3bar_click_events module py3status will soon be available on debian thx to Federico Ceratto !","title":"py3status v2.4"},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#thank-you","text":"Federico Ceratto @rixx (and her amazing reactivity) J.M. Dana @Gamonics @guilbep @lujeni @obb @shankargopal @thomas-","title":"Thank you"},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#important","text":"In order to keep a clean and efficient code base, this is the last version of py3status supporting the legacy modules loading and ordering , this behavior will be dropped on the next 2.5 version !","title":"IMPORTANT"},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#cli-commands","text":"py3status now supports some CLI commands which allows you to get information about all the available modules and their documentation . list all available modules if you specify your own inclusion folder(s) with the -i parameter, your modules will be listed too ! $ py3status modules list Available modules: battery_level Display the battery level. bitcoin_price Display bitcoin prices using bitcoincharts.com. bluetooth Display bluetooth status. clementine Display the current \"artist - title\" playing in Clementine. dpms Activate or deactivate DPMS and screen blanking. glpi Display the total number of open tickets from GLPI. imap Display the unread messages count from your IMAP account. keyboard_layout Display the current keyboard layout. mpd_status Display information from mpd. net_rate Display the current network transfer rate. netdata Display network speed and bandwidth usage. ns_checker Display DNS resolution success on a configured domain. online_status Display if a connection to the internet is established. pingdom Display the latest response time of the configured Pingdom checks. player_control Control music/video players. pomodoro Display and control a Pomodoro countdown. scratchpad_counter Display the amount of windows in your i3 scratchpad. spaceapi Display if your favorite hackerspace is open or not. spotify Display information about the current song playing on Spotify. sysdata Display system RAM and CPU utilization. vnstat Display vnstat statistics. weather_yahoo Display Yahoo! Weather forecast as icons. whoami Display the currently logged in user. window_title Display the current window title. xrandr Control your screen(s) layout easily. get available modules details and configuration $ py3status modules details Available modules: battery_level Display the battery level. Configuration parameters: - color\\_\\* : None means - get it from i3status config - format : text with \"text\" mode. percentage with % replaces {} - hide\\_when\\_full : hide any information when battery is fully charged - mode : for primitive-one-char bar, or \"text\" for text percentage ouput Requires: - the 'acpi' command line @author shadowprince, AdamBSteele @license Eclipse Public License --- [...]","title":"CLI commands"},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#modules-changelog","text":"new bluetooth module by J.M. Dana new online_status module by @obb new player_control module, by Federico Ceratto new spotify module, by Pierre Guilbert new xrandr module to handle your screens layout from your bar dpms module activate/deactivate the screensaver as well imap module various configuration and optimizations pomodoro module can use DBUS notify, play sounds and be paused spaceapi module bugfix for space APIs without 'lastchange' field keyboard_layout module incorrect parsing of \"setxkbmap -query\" battery_level module better python3 compatibility","title":"Modules changelog"},{"location":"Tech%20Blog/2015/2015-03-31-py3status-v2-4/#other-highlights","text":"Full changelog here . catch daylight savings time change ensure modules methods are always iterated alphabetically refactor default config file detection rename and move the empty_class example module to the doc/ folder remove obsolete i3bar_click_events module py3status will soon be available on debian thx to Federico Ceratto !","title":"Other highlights"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/","text":"We've been running a nice mongoDB cluster in production for several years now in my company. This cluster suits quite a wide range of use cases from very simple configuration collections to complex queried ones and real time analytics. This versatility has been the strong point of mongoDB for us since the start as it allows different teams to address their different problems using the same technology. We also run some dedicated replica sets for other purposes and network segmentation reasons. We've waited a long time to see the latest 3.0 release features happening. The new WiredTiger storage engine hit the fan at the right time for us since we had reached the limits of our main production cluster and were considering alternatives. So as surprising it may seem, it's the first of our mongoDB architecture we're upgrading to v3.0 as it has become a real necessity. This post is about sharing our first experience about an ongoing and carefully planned major upgrade of a production cluster and does not claim to be a definitive migration guide. Upgrade plan and hardware \u00b6 The upgrade process is well covered in the mongoDB documentation already but I will list the pre-migration base specs of every node of our cluster. mongodb v2.6.8 RAID1 spinning HDD 15k rpm for the OS (Gentoo Linux) RAID10 4x SSD for mongoDB files under LVM 64 GB RAM Our overall philosophy is to keep most of the configuration parameters to their default values to start with. We will start experimenting with them when we have sufficient metrics to compare with later. Disk (re)partitioning considerations \u00b6 The master-get-all-the-writes architecture is still one of the main limitation of mongoDB and this does not change with v3.0 so you obviously need to challenge your current disk layout to take advantage of the new WiredTiger engine. mongoDB 2.6 MMAPv1 \u00b6 Considering our cluster data size, we were forced to use our 4 SSD in a RAID10 as it was the best compromise to preserve performance while providing sufficient data storage capacity. We often reached the limits of our I/O and moved the journal out of the RAID10 to the mostly idle OS RAID1 with no significant improvements. mongoDB 3.0 WiredTiger \u00b6 The main consideration point for us is the new feature allowing to store the indexes in a separate directory. So we anticipated the data storage consumption reduction thanks to the snappy compression and decided to split our RAID10 in two dedicated RAID1. Our test layout so far is : RAID1 SSD for the data RAID1 SSD for the indexes and journal Our first node migration \u00b6 After migrating our mongos and config servers to 3.0, we picked our worst performing secondary node to test the actual migration to WiredTiger. After all, we couldn't do worse right ? We are aware that the strong suit of WiredTiger is actually about having the writes directed to it and will surely share our experience of this aspect later. compression is bliss \u00b6 To make this comparison accurate, we resynchronized this node totally before migrating to WiredTiger so we could compare a non fragmented MMAPv1 disk usage with the WiredTiger compressed one. While I can't disclose the actual values, compression worked like a charm for us with a gain ratio of 3,2 on disk usage (data + indexes) which is way beyond our expectations ! This is the DB Storage graph from MMS, showing a gain ratio of 4 surely due to indexes being in a separate disk now. memory usage \u00b6 As with the disk usage, the node had been running hot on MMAPv1 before the actual migration so we can compare memory allocation/consumption of both engines. There again the memory management of WiredTiger and its cache shows great improvement. For now, we left the default setting which has WiredTiger limit its cache to half the available memory of the system. We'll experiment with this setting later on. connections \u00b6 This I'm still not sure of the actual cause yet but the connections count is higher and more steady than before on this node. First impressions \u00b6 The node is running smooth for several hours now. We are getting acquainted to the new metrics and statistics from WiredTiger. The overall node and I/O load is better than before ! While all the above graphs show huge improvements there is no major change from our applications point of view. We didn't expect any since this is only one node in a whole cluster and that the main benefits will also come from master node migrations. I'll continue to share our experience and progress about our mongoDB 3.0 upgrade.","title":"MongoDB 3.0 upgrade in production : first steps"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#upgrade-plan-and-hardware","text":"The upgrade process is well covered in the mongoDB documentation already but I will list the pre-migration base specs of every node of our cluster. mongodb v2.6.8 RAID1 spinning HDD 15k rpm for the OS (Gentoo Linux) RAID10 4x SSD for mongoDB files under LVM 64 GB RAM Our overall philosophy is to keep most of the configuration parameters to their default values to start with. We will start experimenting with them when we have sufficient metrics to compare with later.","title":"Upgrade plan and hardware"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#disk-repartitioning-considerations","text":"The master-get-all-the-writes architecture is still one of the main limitation of mongoDB and this does not change with v3.0 so you obviously need to challenge your current disk layout to take advantage of the new WiredTiger engine.","title":"Disk (re)partitioning considerations"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#mongodb-26-mmapv1","text":"Considering our cluster data size, we were forced to use our 4 SSD in a RAID10 as it was the best compromise to preserve performance while providing sufficient data storage capacity. We often reached the limits of our I/O and moved the journal out of the RAID10 to the mostly idle OS RAID1 with no significant improvements.","title":"mongoDB 2.6 MMAPv1"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#mongodb-30-wiredtiger","text":"The main consideration point for us is the new feature allowing to store the indexes in a separate directory. So we anticipated the data storage consumption reduction thanks to the snappy compression and decided to split our RAID10 in two dedicated RAID1. Our test layout so far is : RAID1 SSD for the data RAID1 SSD for the indexes and journal","title":"mongoDB 3.0 WiredTiger"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#our-first-node-migration","text":"After migrating our mongos and config servers to 3.0, we picked our worst performing secondary node to test the actual migration to WiredTiger. After all, we couldn't do worse right ? We are aware that the strong suit of WiredTiger is actually about having the writes directed to it and will surely share our experience of this aspect later.","title":"Our first node migration"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#compression-is-bliss","text":"To make this comparison accurate, we resynchronized this node totally before migrating to WiredTiger so we could compare a non fragmented MMAPv1 disk usage with the WiredTiger compressed one. While I can't disclose the actual values, compression worked like a charm for us with a gain ratio of 3,2 on disk usage (data + indexes) which is way beyond our expectations ! This is the DB Storage graph from MMS, showing a gain ratio of 4 surely due to indexes being in a separate disk now.","title":"compression is bliss"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#memory-usage","text":"As with the disk usage, the node had been running hot on MMAPv1 before the actual migration so we can compare memory allocation/consumption of both engines. There again the memory management of WiredTiger and its cache shows great improvement. For now, we left the default setting which has WiredTiger limit its cache to half the available memory of the system. We'll experiment with this setting later on.","title":"memory usage"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#connections","text":"This I'm still not sure of the actual cause yet but the connections count is higher and more steady than before on this node.","title":"connections"},{"location":"Tech%20Blog/2015/2015-05-07-mongodb-3-0-upgrade-in-production-first-steps/#first-impressions","text":"The node is running smooth for several hours now. We are getting acquainted to the new metrics and statistics from WiredTiger. The overall node and I/O load is better than before ! While all the above graphs show huge improvements there is no major change from our applications point of view. We didn't expect any since this is only one node in a whole cluster and that the main benefits will also come from master node migrations. I'll continue to share our experience and progress about our mongoDB 3.0 upgrade.","title":"First impressions"},{"location":"Tech%20Blog/2015/2015-05-13-uwsgi-gevent-and-pymongo-3-threads-mayhem/","text":"This is a quick heads-up post about a behaviour change when running a gevent based application using the new pymongo 3 driver under uWSGI and its gevent loop. I was naturally curious about testing this brand new and major update of the python driver for mongoDB so I just played it dumb : update and give a try on our existing code base. The first thing I noticed instantly is that a vast majority of our applications were suddenly unable to reload gracefully and were force killed by uWSGI after some time ! worker 1 (pid: 9839) is taking too much time to die...NO MERCY !!! uWSGI's gevent-wait-for-hub \u00b6 All our applications must be able to be gracefully reloaded at any time. Some of them are spawning quite a few greenlets on their own so as an added measure of making sure we never loose any running greenlet we use the gevent-wait-for-hub option, which is described as follow : wait for gevent hub's death instead of the control greenlet ... which does not mean a lot but is explained in a previous uWSGI changelog : During shutdown only the greenlets spawned by uWSGI are taken in account, and after all of them are destroyed the process will exit. This is different from the old approach where the process wait for ALL the currently available greenlets (and monkeypatched threads). If you prefer the old behaviour just specify the option gevent-wait-for-hub pymongo 3 \u00b6 Compared to its previous 2.x versions, one of the overall key aspect of the new pymongo 3 driver is its intensive usage of threads to handle server discovery and connection pools. Now we can relate this very fact to the gevent-wait-for-hub behaviour explained above : the process wait for ALL the currently available greenlets ( and monkeypatched threads ) This explained why our applications were hanging until the reload-mercy (force kill) timeout option of uWSGI hit the fan ! conclusion \u00b6 When using pymongo 3 with the gevent-wait-for-hub option, you have to keep in mind that all of pymongo's threads (so monkey patched threads) are considered as active greenlets and will thus be waited for termination before uWSGI recycles the worker ! Two options come in mind to handle this properly : stop using the gevent-wait-for-hub option and change your code to use a gevent pool group to make sure that all of your important greenlets are taken care of when a graceful reload happens (this is how we do it today, the gevent-wait-for-hub option usage was just over protective for us). modify your code to properly close all your pymongo connections on graceful reloads. Hope this will save some people the trouble of debugging this ;)","title":"uWSGI, gevent and pymongo 3 threads mayhem"},{"location":"Tech%20Blog/2015/2015-05-13-uwsgi-gevent-and-pymongo-3-threads-mayhem/#uwsgis-gevent-wait-for-hub","text":"All our applications must be able to be gracefully reloaded at any time. Some of them are spawning quite a few greenlets on their own so as an added measure of making sure we never loose any running greenlet we use the gevent-wait-for-hub option, which is described as follow : wait for gevent hub's death instead of the control greenlet ... which does not mean a lot but is explained in a previous uWSGI changelog : During shutdown only the greenlets spawned by uWSGI are taken in account, and after all of them are destroyed the process will exit. This is different from the old approach where the process wait for ALL the currently available greenlets (and monkeypatched threads). If you prefer the old behaviour just specify the option gevent-wait-for-hub","title":"uWSGI's gevent-wait-for-hub"},{"location":"Tech%20Blog/2015/2015-05-13-uwsgi-gevent-and-pymongo-3-threads-mayhem/#pymongo-3","text":"Compared to its previous 2.x versions, one of the overall key aspect of the new pymongo 3 driver is its intensive usage of threads to handle server discovery and connection pools. Now we can relate this very fact to the gevent-wait-for-hub behaviour explained above : the process wait for ALL the currently available greenlets ( and monkeypatched threads ) This explained why our applications were hanging until the reload-mercy (force kill) timeout option of uWSGI hit the fan !","title":"pymongo 3"},{"location":"Tech%20Blog/2015/2015-05-13-uwsgi-gevent-and-pymongo-3-threads-mayhem/#conclusion","text":"When using pymongo 3 with the gevent-wait-for-hub option, you have to keep in mind that all of pymongo's threads (so monkey patched threads) are considered as active greenlets and will thus be waited for termination before uWSGI recycles the worker ! Two options come in mind to handle this properly : stop using the gevent-wait-for-hub option and change your code to use a gevent pool group to make sure that all of your important greenlets are taken care of when a graceful reload happens (this is how we do it today, the gevent-wait-for-hub option usage was just over protective for us). modify your code to properly close all your pymongo connections on graceful reloads. Hope this will save some people the trouble of debugging this ;)","title":"conclusion"},{"location":"Tech%20Blog/2015/2015-05-23-gevent-ssl-support-fixed-for-python-2-7-9/","text":"Good news for gevent users blocked on python < 2.7.9 due to broken SSL support since python upstream dropped the private API _ssl.sslwrap that eventlet was using. This issue was starting to get old and problematic since GLSA 2015-0310 but I'm happy to say that almost 6 hours after the gevent-1.0.2 release , it is already available on portage ! We were also affected by this issue at work so I'm glad that the tension between ops and devs this issue was causing will finally be over ;)","title":"Gevent : SSL support fixed for python 2.7.9"},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/","text":"In my previous post regarding the migration of our production cluster to mongoDB 3.0 WiredTiger, we successfully upgraded all the secondaries of our replica-sets with decent performances and (almost, read on) no breakage. Step 2 plan \u00b6 The next step of our migration was to test our work load on WiredTiger primaries. After all, this is where the new engine would finally demonstrate all its capabilities. We thus scheduled a step down from our 3.0 MMAPv1 primary servers so that our WiredTiger secondaries would take over. Not migrating the primaries was a safety net in case something went wrong... And boy it went so wrong we're glad we played it safe that way ! We rolled back after 10 minutes of utter bitterness. The failure \u00b6 After all the wait and expectation, I can't express our level of disappointment at work when we saw that the WiredTiger engine could not handle our work load. Our application started immediately to throw 50 to 250 WriteConflict errors per minute ! Turns out that we are affected by this bug and that, of course, we're not the only ones . So far it seems that it affects collections with : heavy insert / update work loads an unique index (or compound index) The breakages \u00b6 We also discovered that we're affected by a weird mongodump new behaviour where the dumped BSON file does not contain the number of documents that mongodump said it was exporting. This is clearly a new problem because it happened right after all our secondaries switched to WiredTiger . Since we have to ensure a strong consistency of our exports and that the mongoDB guys don't seem so keen on moving on the bug (which I surely can understand) there is a large possibility that we'll have to roll back even the WiredTiger secondaries altogether. Not to mention that since the 3.0 version, we experience some CPU overloads crashing the entire server on our MMAPv1 primaries that we're still trying to tackle before opening another JIRA bug... Sad panda \u00b6 Of course, any new major release such as 3.0 causes its headaches and brings its lot of bugs. We were ready for this hence the safety steps we took to ensure that we could roll back on any problem. But as a long time advocate of mongoDB I must admit my frustration, even more after the time it took to get this 3.0 out and all the expectations that came with it. I hope I can share some better news on the next blog post.","title":"MongoDB 3.0 upgrade in production : step 2 failed"},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/#step-2-plan","text":"The next step of our migration was to test our work load on WiredTiger primaries. After all, this is where the new engine would finally demonstrate all its capabilities. We thus scheduled a step down from our 3.0 MMAPv1 primary servers so that our WiredTiger secondaries would take over. Not migrating the primaries was a safety net in case something went wrong... And boy it went so wrong we're glad we played it safe that way ! We rolled back after 10 minutes of utter bitterness.","title":"Step 2 plan"},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/#the-failure","text":"After all the wait and expectation, I can't express our level of disappointment at work when we saw that the WiredTiger engine could not handle our work load. Our application started immediately to throw 50 to 250 WriteConflict errors per minute ! Turns out that we are affected by this bug and that, of course, we're not the only ones . So far it seems that it affects collections with : heavy insert / update work loads an unique index (or compound index)","title":"The failure"},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/#the-breakages","text":"We also discovered that we're affected by a weird mongodump new behaviour where the dumped BSON file does not contain the number of documents that mongodump said it was exporting. This is clearly a new problem because it happened right after all our secondaries switched to WiredTiger . Since we have to ensure a strong consistency of our exports and that the mongoDB guys don't seem so keen on moving on the bug (which I surely can understand) there is a large possibility that we'll have to roll back even the WiredTiger secondaries altogether. Not to mention that since the 3.0 version, we experience some CPU overloads crashing the entire server on our MMAPv1 primaries that we're still trying to tackle before opening another JIRA bug...","title":"The breakages"},{"location":"Tech%20Blog/2015/2015-05-26-mongodb-3-0-upgrade-in-production-step-2-failed/#sad-panda","text":"Of course, any new major release such as 3.0 causes its headaches and brings its lot of bugs. We were ready for this hence the safety steps we took to ensure that we could roll back on any problem. But as a long time advocate of mongoDB I must admit my frustration, even more after the time it took to get this 3.0 out and all the expectations that came with it. I hope I can share some better news on the next blog post.","title":"Sad panda"},{"location":"Tech%20Blog/2015/2015-07-09-mongodb-3-0-upgrade-in-production-step-3-hope/","text":"In our previous attempt to upgrade our production cluster to 3.0, we had to roll back from the WiredTiger engine on primary servers. Since then, we switched back our whole cluster to 3.0 MMAPv1 which has brought us some better performances than 2.6 with no instability. Production checklist \u00b6 We decided to use this increase in performance to allow us some time to fulfil the entire production checklist from MongoDB, especially the migration to XFS . We're slowly upgrading our servers kernels and resynchronising our data set after migrating from ext4 to XFS. Ironically, the strong recommendation of XFS in the production checklist appeared 3 days after our failed attempt at WiredTiger... This is frustrating but gives some kind of hope. I'll keep on posting on our next steps and results. Our hero WiredTiger Replica Set \u00b6 While we were battling with our production cluster, we got a spontaneous major increase in the daily volumes from another platform which was running on a single Replica Set. This application is write intensive and very disk I/O bound. We were killing the disk I/O with almost a continuous 100% usage on the disk write queue. Despite our frustration with WiredTiger so far, we decided to give it a chance considering that this time we were talking about a single Replica Set. We were very happy to see WiredTiger keep up to its promises with an almost shocking serenity . Disk I/O went down dramatically, almost as if nothing was happening any more. Compression did magic on our disk usage and our application went Roarrr !","title":"MongoDB 3.0 upgrade in production : step 3 hope"},{"location":"Tech%20Blog/2015/2015-07-09-mongodb-3-0-upgrade-in-production-step-3-hope/#production-checklist","text":"We decided to use this increase in performance to allow us some time to fulfil the entire production checklist from MongoDB, especially the migration to XFS . We're slowly upgrading our servers kernels and resynchronising our data set after migrating from ext4 to XFS. Ironically, the strong recommendation of XFS in the production checklist appeared 3 days after our failed attempt at WiredTiger... This is frustrating but gives some kind of hope. I'll keep on posting on our next steps and results.","title":"Production checklist"},{"location":"Tech%20Blog/2015/2015-07-09-mongodb-3-0-upgrade-in-production-step-3-hope/#our-hero-wiredtiger-replica-set","text":"While we were battling with our production cluster, we got a spontaneous major increase in the daily volumes from another platform which was running on a single Replica Set. This application is write intensive and very disk I/O bound. We were killing the disk I/O with almost a continuous 100% usage on the disk write queue. Despite our frustration with WiredTiger so far, we decided to give it a chance considering that this time we were talking about a single Replica Set. We were very happy to see WiredTiger keep up to its promises with an almost shocking serenity . Disk I/O went down dramatically, almost as if nothing was happening any more. Compression did magic on our disk usage and our application went Roarrr !","title":"Our hero WiredTiger Replica Set"},{"location":"Tech%20Blog/2015/2015-07-22-designing-a-scalable-and-distributed-application/","text":"These are the slides of my EuroPython 2015 talk . The source code and ansible playbooks are available on github !","title":"Designing a scalable and distributed application"},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/","text":"This new py3status comes with an amazing number of contributions and new modules ! 24 files changed, 1900 insertions(+), 263 deletions(-) I'm also glad to say that py3status becomes my first commit in the new git repository of Gentoo Linux ! IMPORTANT \u00b6 Please note that this version has deprecated the legacy implicit module loading support to favour and focus on the generic i3status order += module loading/ordering ! New modules \u00b6 new aws_bill module, by Anthony Brodard new dropboxd_status module, by Tjaart van der Walt new external_script module, by Dominik new nvidia_temp module for displaying NVIDIA GPUs' temperature, by J.M. Dana new rate_counter module, by Amaury Brisou new screenshot module, by Amaury Brisou new static_string module, by Dominik new taskwarrior module, by James Smith new volume_status module, by Jan T. new whatismyip module displaying your public/external IP as well as your online status Changelog \u00b6 As usual, full changelog is available here . Contributors \u00b6 Along with all those who reported issues and helped fixed them, quick and surely not exhaustive list: Anthony Brodard Tjaart van der Walt Dominik J.M. Dana Amaury Brisou James Smith Jan T. Zopieux Horgix hlmtre What's next ? \u00b6 Well something tells me @Horgix is working hard on some standardization and on the core of py3status ! I'm sure some very interesting stuff will emerge from this, so thank you !","title":"py3status v2.5"},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#important","text":"Please note that this version has deprecated the legacy implicit module loading support to favour and focus on the generic i3status order += module loading/ordering !","title":"IMPORTANT"},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#new-modules","text":"new aws_bill module, by Anthony Brodard new dropboxd_status module, by Tjaart van der Walt new external_script module, by Dominik new nvidia_temp module for displaying NVIDIA GPUs' temperature, by J.M. Dana new rate_counter module, by Amaury Brisou new screenshot module, by Amaury Brisou new static_string module, by Dominik new taskwarrior module, by James Smith new volume_status module, by Jan T. new whatismyip module displaying your public/external IP as well as your online status","title":"New modules"},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#changelog","text":"As usual, full changelog is available here .","title":"Changelog"},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#contributors","text":"Along with all those who reported issues and helped fixed them, quick and surely not exhaustive list: Anthony Brodard Tjaart van der Walt Dominik J.M. Dana Amaury Brisou James Smith Jan T. Zopieux Horgix hlmtre","title":"Contributors"},{"location":"Tech%20Blog/2015/2015-08-17-py3status-v2-5/#whats-next","text":"Well something tells me @Horgix is working hard on some standardization and on the core of py3status ! I'm sure some very interesting stuff will emerge from this, so thank you !","title":"What's next ?"},{"location":"Tech%20Blog/2015/2015-08-27-py3status-v2-6/","text":"Ok I was a bit too hasty in my legacy module support code clean up and I broke quite a few things on the latest version 2.5 release sorry ! :( highlights \u00b6 make use of pkgutil to detect properly installed modules even when they are zipped in egg files (manual install) add back legacy modules output support (tuple of position / response) new uname module inspired from issue 117 thanks to @ndalliard remove dead code thanks ! \u00b6 @ coelebs on IRC for reporting, testing and the good spirit :) @ ndalliard on github for the issue, debug and for inspiring the uname module @ Horgix for responding to issues faster than me !","title":"py3status v2.6"},{"location":"Tech%20Blog/2015/2015-08-27-py3status-v2-6/#highlights","text":"make use of pkgutil to detect properly installed modules even when they are zipped in egg files (manual install) add back legacy modules output support (tuple of position / response) new uname module inspired from issue 117 thanks to @ndalliard remove dead code","title":"highlights"},{"location":"Tech%20Blog/2015/2015-08-27-py3status-v2-6/#thanks","text":"@ coelebs on IRC for reporting, testing and the good spirit :) @ ndalliard on github for the issue, debug and for inspiring the uname module @ Horgix for responding to issues faster than me !","title":"thanks !"},{"location":"Tech%20Blog/2015/2015-09-14-mongodb-3-0-upgrade-in-production-step-4-victory/","text":"In my last post , I explained the new hope we had in following some newly added recommended steps before trying to migrate our production cluster to mongoDB 3.0 WiredTiger. The most demanding step was migrating all our production servers data storage filesystems to XFS which obviously required a resync of each node... But we ended up being there pretty fast and were about to try again as 3.0.5 was getting ready, until we saw this bug coming ! I guess you can understand why we decided to wait for 3.0.6... which eventually got released with a more peaceful changelog this time. The 3.0.6 crash test \u00b6 We decided as usual to test the migration to WiredTiger in two phases. Migrate all our secondaries to the WiredTiger engine (full resync). Wait a week to see if this has any effect on our cluster. Switch all the MMapv1 primary nodes to secondary and let our WiredTiger secondary nodes become the primary nodes of our cluster. Pray hard that this time it will not break under our workload. Step 1 results were good, nothing major changed and even our mongo dumps were still working this time (yay!). One week later, everything was still working smoothly. Step 2 was the big challenge which failed horribly last time . Needless to say that we were quite stressed when doing the switch. But it worked smoothly and nothing broke + performances gains were huge ! The results \u00b6 Nothing speaks better than metrics, so I'll just comment them quickly as they speak by themselves. I obviously can't disclose the scales sorry. Insert-only operations gained 25x performance Upsert-heavy operations gained 5x performance Disk I/O also showed mercy to the disk overall usage. This is due to WiredTiger superior caching and disk flushing mechanisms. Disk usage decreased dramatically thanks to WiredTiger compression The last and next step \u00b6 As of today, we still run our secondaries with the MMapv1 engine and are waiting a few weeks to see if anything goes wrong in the long run. Shall we need to roll back, we'd be able to do so very easily. Then when we get enough uptime using WiredTiger, we will make the final switch to a full Roarring production cluster !","title":"MongoDB 3.0 upgrade in production : step 4 victory !"},{"location":"Tech%20Blog/2015/2015-09-14-mongodb-3-0-upgrade-in-production-step-4-victory/#the-306-crash-test","text":"We decided as usual to test the migration to WiredTiger in two phases. Migrate all our secondaries to the WiredTiger engine (full resync). Wait a week to see if this has any effect on our cluster. Switch all the MMapv1 primary nodes to secondary and let our WiredTiger secondary nodes become the primary nodes of our cluster. Pray hard that this time it will not break under our workload. Step 1 results were good, nothing major changed and even our mongo dumps were still working this time (yay!). One week later, everything was still working smoothly. Step 2 was the big challenge which failed horribly last time . Needless to say that we were quite stressed when doing the switch. But it worked smoothly and nothing broke + performances gains were huge !","title":"The 3.0.6 crash test"},{"location":"Tech%20Blog/2015/2015-09-14-mongodb-3-0-upgrade-in-production-step-4-victory/#the-results","text":"Nothing speaks better than metrics, so I'll just comment them quickly as they speak by themselves. I obviously can't disclose the scales sorry. Insert-only operations gained 25x performance Upsert-heavy operations gained 5x performance Disk I/O also showed mercy to the disk overall usage. This is due to WiredTiger superior caching and disk flushing mechanisms. Disk usage decreased dramatically thanks to WiredTiger compression","title":"The results"},{"location":"Tech%20Blog/2015/2015-09-14-mongodb-3-0-upgrade-in-production-step-4-victory/#the-last-and-next-step","text":"As of today, we still run our secondaries with the MMapv1 engine and are waiting a few weeks to see if anything goes wrong in the long run. Shall we need to roll back, we'd be able to do so very easily. Then when we get enough uptime using WiredTiger, we will make the final switch to a full Roarring production cluster !","title":"The last and next step"},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/","text":"When working on distributed systems, we often have to distribute some kind of workload on different machines (nodes) of a cluster so we have to rely on a predictable and reliable key/value mapping algorithm . If you're not sure about what that means, just consider the following questions when working on a cluster with a lot of machines (nodes): how could I make sure that all the data for a given user always gets delivered and processed on the same machine ? how could I make sure that I store and query the same cache server for a given key ? how do I split and distribute chunks of a large file across multiple storage machines and then make sure I can still access it through all those machines at once ? A lot of technologies answer those kind of mapping questions by implementing a hashing based distribution of their workload (be it a distributed processing, file or cache). Consistent hashing is one of those implementations and I felt like taking some time to explain it further. Example use case \u00b6 Here is my attempt to explain what is consistent hashing and why it is needed on distributed systems. To make things fun, we'll take this simple use case: I have 5 broken cars There are 4 repair factories nearby I have to implement a way to figure out where to send each car to get it fixed I want to ensure that an even number of cars will get fixed on each factory \u00b6 This gets down to two major questions to solve: what is my selection criteria ? this will be my key . what is the expected answer ? this is my value . Static mapping \u00b6 The first approach we could implement is to manually distribute the car based on their colour. key = car's colour value = factory number To implement this, you use what we usually call dictionaries on various languages : those are static data structures where you assign a value to a key . We would then write a mapping of \"car color\" -> \"factory n\" and apply this simple rule to decide where to ship a broken car. { \"yellow\": \"factory 1\", \"orange\": \"factory 2\", \"red\": \"factory 3\", \"green\": \"factory 4\", \"black\": \"factory 1\" } This way we could indeed distribute the car repairs, but we can already see that with that an uneven number of colours ends up in over provisioning the factory 1. But there's worse: What if I start getting only yellow broken cars ? I would end up sending all of them to the factory 1 and the other factories would remain almost empty ! This is a serious limitation. We need a dynamic way to calculate the car distribution between the factories, for this we will use a hash algorithm ! Hash tables \u00b6 A hash table is a data structure where we apply a hash function (algorithm) on the key to compute an index (pointer) into an array of buckets (values) from which we get the value . MD5 gives very good hashing distribution and is widely available so this makes it a very good candidate for a hashing algorithm. We can relate it like this to our example : key = car's plate number hash function = md5 array of values = [factory 1, factory 2, factory 3, factory 4] To find out where we send a car we just could do: hash = md5(car plate number) index = int(hash) % size_of(array) index = 0 if index > size_of(array) factory = array[index] In python ? okay ! import md5 factories = [1, 2, 3, 4] def get_factory(plate): hash = int(md5.new(plate).hexdigest(), 16) index = hash % len(factories) if index > len(factories): index = 0 return factories[index] get_factory('ah-993-xx') 3 get_factory('zz-6793-kh') 3 Wow it's amazingly simple right ? :) Now we have a way better car repair distribution !... until something bad happens: What if a factory burns ? Our algorithm is based on the number of available factories so removing a factory from our array means that we will redistribute a vast majority of the key mappings from our hash table ! Keep in mind that the more values (factories) you have in your array the worse this problem gets. In our case, given a car's plate number we are sure that we wouldn't be able to figure out where a vast majority of them were sent any more. factories = [1, 2, 4] get_factory('ah-993-xx') 2 (was 3 before) get_factory('zz-6793-kh') 1 (was 3 before) Even worse is that when factory 3 gets repaired and back in my hash table, I will once again loose track of all my dispatched cars... What we need is a more consistent way of sorting this out. Consistent hashing \u00b6 The response to this kind of problem is to implement a consistent hashing algorithm. The goal of this technique is to limit the number of remapped keys when the hash table is resized . This is possible by imagining our factories as a circle (ring) and the hash of our keys as points on the same circle. We would then select the next factory (value) by going through the circle, always on the same way, until we find a factory. Red hashed plate number would go to factory 1 Blue hashed plate number would go to factory 3 This way, when a factory gets added or removed from the ring, we loose only a limited portion of the key/value mappings ! Of course on a real world example we would implement a ring with a lot more of slots by adding the same factories on the ring multiple times. This way the affected range of mappings would be smaller and the impact even more balanced ! For instance, uhashring being fully compatible and defaulting to ketama's ring distribution, you get 160 points per node on the ring ! I hope I got this little example right and gave you some insight on this very interesting topic !","title":"Consistent Hashing 101"},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#example-use-case","text":"Here is my attempt to explain what is consistent hashing and why it is needed on distributed systems. To make things fun, we'll take this simple use case: I have 5 broken cars There are 4 repair factories nearby I have to implement a way to figure out where to send each car to get it fixed I want to ensure that an even number of cars will get fixed on each factory","title":"Example use case"},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#_1","text":"This gets down to two major questions to solve: what is my selection criteria ? this will be my key . what is the expected answer ? this is my value .","title":""},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#static-mapping","text":"The first approach we could implement is to manually distribute the car based on their colour. key = car's colour value = factory number To implement this, you use what we usually call dictionaries on various languages : those are static data structures where you assign a value to a key . We would then write a mapping of \"car color\" -> \"factory n\" and apply this simple rule to decide where to ship a broken car. { \"yellow\": \"factory 1\", \"orange\": \"factory 2\", \"red\": \"factory 3\", \"green\": \"factory 4\", \"black\": \"factory 1\" } This way we could indeed distribute the car repairs, but we can already see that with that an uneven number of colours ends up in over provisioning the factory 1. But there's worse: What if I start getting only yellow broken cars ? I would end up sending all of them to the factory 1 and the other factories would remain almost empty ! This is a serious limitation. We need a dynamic way to calculate the car distribution between the factories, for this we will use a hash algorithm !","title":"Static mapping"},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#hash-tables","text":"A hash table is a data structure where we apply a hash function (algorithm) on the key to compute an index (pointer) into an array of buckets (values) from which we get the value . MD5 gives very good hashing distribution and is widely available so this makes it a very good candidate for a hashing algorithm. We can relate it like this to our example : key = car's plate number hash function = md5 array of values = [factory 1, factory 2, factory 3, factory 4] To find out where we send a car we just could do: hash = md5(car plate number) index = int(hash) % size_of(array) index = 0 if index > size_of(array) factory = array[index] In python ? okay ! import md5 factories = [1, 2, 3, 4] def get_factory(plate): hash = int(md5.new(plate).hexdigest(), 16) index = hash % len(factories) if index > len(factories): index = 0 return factories[index] get_factory('ah-993-xx') 3 get_factory('zz-6793-kh') 3 Wow it's amazingly simple right ? :) Now we have a way better car repair distribution !... until something bad happens: What if a factory burns ? Our algorithm is based on the number of available factories so removing a factory from our array means that we will redistribute a vast majority of the key mappings from our hash table ! Keep in mind that the more values (factories) you have in your array the worse this problem gets. In our case, given a car's plate number we are sure that we wouldn't be able to figure out where a vast majority of them were sent any more. factories = [1, 2, 4] get_factory('ah-993-xx') 2 (was 3 before) get_factory('zz-6793-kh') 1 (was 3 before) Even worse is that when factory 3 gets repaired and back in my hash table, I will once again loose track of all my dispatched cars... What we need is a more consistent way of sorting this out.","title":"Hash tables"},{"location":"Tech%20Blog/2015/2015-10-19-consistent-hashing-101/#consistent-hashing","text":"The response to this kind of problem is to implement a consistent hashing algorithm. The goal of this technique is to limit the number of remapped keys when the hash table is resized . This is possible by imagining our factories as a circle (ring) and the hash of our keys as points on the same circle. We would then select the next factory (value) by going through the circle, always on the same way, until we find a factory. Red hashed plate number would go to factory 1 Blue hashed plate number would go to factory 3 This way, when a factory gets added or removed from the ring, we loose only a limited portion of the key/value mappings ! Of course on a real world example we would implement a ring with a lot more of slots by adding the same factories on the ring multiple times. This way the affected range of mappings would be smaller and the impact even more balanced ! For instance, uhashring being fully compatible and defaulting to ketama's ring distribution, you get 160 points per node on the ring ! I hope I got this little example right and gave you some insight on this very interesting topic !","title":"Consistent hashing"},{"location":"Tech%20Blog/2015/2015-10-20-uhashring-consistent-hashing-in-python/","text":"It's been quite some time since I wanted to use a consistent hashing based distribution of my workload in a quite big cluster at work. So when we finally reached the point where this became critical for some job processing I rushed to find out what python library I could use to implement this easily and efficiently. I was surprised not to find a clear \"winner\" for such a library. The more \"popular\" named hash_ring has a rather unoptimized source code and is dead (old issues, no answer). Some others stepped up since but with no clear interest for contributions and almost no added features for real world applications. So I packaged the ketama C library and its python binding on my overlay to get intimate with its algorithm. Then I started working on my own pure python library and released uhashring on Pypi and on Gentoo portage ! Features \u00b6 instance-oriented usage so you can use your consistent hash ring object directly in your code (see advanced usage ). a lot of convenient methods to use your consistent hash ring in real world applications. simple integration with other libs such as memcache through monkey patching. all the missing functions in the libketama C python binding (which is not even available on pypi). another and more performant consistent hash algorithm if you don't care about the ketama compatibility (see benchmark ). native pypy support . tests of implementation, key distribution and ketama compatibility. WTF ? \u00b6 Not so sure about hash tables and consistent hashing ? Read my consistent hashing 101 explanation attempt !","title":"uhashring : consistent hashing in python"},{"location":"Tech%20Blog/2015/2015-10-20-uhashring-consistent-hashing-in-python/#features","text":"instance-oriented usage so you can use your consistent hash ring object directly in your code (see advanced usage ). a lot of convenient methods to use your consistent hash ring in real world applications. simple integration with other libs such as memcache through monkey patching. all the missing functions in the libketama C python binding (which is not even available on pypi). another and more performant consistent hash algorithm if you don't care about the ketama compatibility (see benchmark ). native pypy support . tests of implementation, key distribution and ketama compatibility.","title":"Features"},{"location":"Tech%20Blog/2015/2015-10-20-uhashring-consistent-hashing-in-python/#wtf","text":"Not so sure about hash tables and consistent hashing ? Read my consistent hashing 101 explanation attempt !","title":"WTF ?"},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/","text":"I'm more than two weeks late but I'm very glad to announce the release of py3status v2.7 which features a lot of interesting stuff ! For this release I want to salute the significant work and help of Daniel Foerster (@pydsigner), who discovered and fixed a bug in the event detection loop. The result is a greatly improved click event detection and bar update speed with a largely reduced CPU consumption and less code ! Highlights \u00b6 major performance and click event detection improvements by Daniel Foerster support of %z on time and tztime modules fixes #110 and #123 thx to @derekdreery and @olhotak directive %Z and any other failure in parsing the time and tztime modules format will result in using i3status date output add ethernet, wireless and battery _first_ instance detection and support. thx to @rekoil for reporting on IRC i3status.conf parser handles configuration values with the = char New modules \u00b6 new rt module: display ongoing tickets from RT queues new xsel module: display xsel buffers, by umbsublime new window_title_async module, by Anon1234 Modules enhancements \u00b6 battery_level module: major improvements, documentation, add format option, by Maxim Baz keyboard_layout module: color customisation, add format option, by Ali Mousavi mpd_status module: fix connection leak, by Thomas Sanchez pomodoro module: implement format option and add additional display features, by Christoph Schober spotify module: fix support for new versions, by Jimmy Garpeh\u00e4ll spotify module: add support for colors output based on the playback status, by Sondre Lefsaker sysdata module: trim spaces in `cpu_temp`, by beetleman whatismyip module: change default check URL and make it configurable Thanks ! \u00b6 Once again, thanks to all contributors listed above !","title":"py3status v2.7"},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/#highlights","text":"major performance and click event detection improvements by Daniel Foerster support of %z on time and tztime modules fixes #110 and #123 thx to @derekdreery and @olhotak directive %Z and any other failure in parsing the time and tztime modules format will result in using i3status date output add ethernet, wireless and battery _first_ instance detection and support. thx to @rekoil for reporting on IRC i3status.conf parser handles configuration values with the = char","title":"Highlights"},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/#new-modules","text":"new rt module: display ongoing tickets from RT queues new xsel module: display xsel buffers, by umbsublime new window_title_async module, by Anon1234","title":"New modules"},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/#modules-enhancements","text":"battery_level module: major improvements, documentation, add format option, by Maxim Baz keyboard_layout module: color customisation, add format option, by Ali Mousavi mpd_status module: fix connection leak, by Thomas Sanchez pomodoro module: implement format option and add additional display features, by Christoph Schober spotify module: fix support for new versions, by Jimmy Garpeh\u00e4ll spotify module: add support for colors output based on the playback status, by Sondre Lefsaker sysdata module: trim spaces in `cpu_temp`, by beetleman whatismyip module: change default check URL and make it configurable","title":"Modules enhancements"},{"location":"Tech%20Blog/2015/2015-11-26-py3status-v2-7/#thanks","text":"Once again, thanks to all contributors listed above !","title":"Thanks !"},{"location":"Tech%20Blog/2015/2015-12-31-uwsgi-v2-0-12/","text":"It's been a long time since I made a blog post about a uWSGI release but this one is special to me because it contains some features I asked for to a colleague of mine. For his first contributions to a big Open Source project, our fellow @ shir0kamii added two features (spooler_get_task and -if-hostname-match) which were backported in this release and that we needed at work for quite a long time : congratulations again :) Highlights \u00b6 official PHP 7 support uwsgi. spooler_get_task API to easily read and inspect a spooler file from your code -if-hostname-match regexp on uWSGI configuration files to allow a more flexible configuration based on the hostname of the machine Of course, all of this is already available on Gentoo Linux ! Full changelog here as usual.","title":"uWSGI v2.0.12"},{"location":"Tech%20Blog/2015/2015-12-31-uwsgi-v2-0-12/#highlights","text":"official PHP 7 support uwsgi. spooler_get_task API to easily read and inspect a spooler file from your code -if-hostname-match regexp on uWSGI configuration files to allow a more flexible configuration based on the hostname of the machine Of course, all of this is already available on Gentoo Linux ! Full changelog here as usual.","title":"Highlights"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/","text":"As I found little help about this online I figured I'd write a summary piece about my recent experience in installing Gentoo Linux on a DELL XPS 13 9350 . EDIT: TL;DR if you need a working kernel config, you can also get my 4.12.5 kernel config here . UEFI or MBR ? \u00b6 This machine ships with a NVME SSD so don't think twice : UEFI is the only sane way to go. BIOS configuration \u00b6 I advise to use the pre-installed Windows 10 to update the XPS to the latest BIOS (1.1.7 at the time of writing). Then you need to change some stuff to boot and get the NVME SSD disk discovered by the live CD. Turn off Secure Boot Set SATA Operation to AHCI (will break your Windows boot but who cares) Live CD \u00b6 Go for the latest SystemRescueCD (it's Gentoo based, you won't be lost) as it's quite more up to date and supports booting on UEFI. Make it a Live USB for example using unetbootin and the ISO on a vfat formatted USB stick . NVME SSD disk partitioning \u00b6 We'll be using GPT with UEFI . I found that using gdisk was the easiest. The disk itself is found on /dev/nvme0n1 . Here it is the partition table I used : 500Mo UEFI boot partition (type EF00) 16Go Swap partition 60Go Linux root partition 400Go home partition The corresponding gdisk commands : # gdisk /dev/nvme0n1 Command: o \u21b5 This option deletes all partitions and creates a new protective MBR. Proceed? (Y/N): y \u21b5 Command: n \u21b5 Partition Number: 1 \u21b5 First sector: \u21b5 Last sector: +500M \u21b5 Hex Code: EF00 \u21b5 Command: n \u21b5 Partition Number: 2 \u21b5 First sector: \u21b5 Last sector: +16G \u21b5 Hex Code: 8200 \u21b5 Command: n \u21b5 Partition Number: 3 \u21b5 First sector: \u21b5 Last sector: +60G \u21b5 Hex Code: \u21b5 Command: n \u21b5 Partition Number: 4 \u21b5 First sector: \u21b5 Last sector: \u21b5 (for rest of disk) Hex Code: \u21b5 Command: w \u21b5 Do you want to proceed? (Y/N): Y \u21b5 No WiFi on Live CD ? no panic \u00b6 If your live CD is old (pre 4.4 kernel), the integrated broadcom 4350 wifi card won't be available ! My trick was to use my Android phone connected to my local WiFi as a USB modem which was detected directly by the live CD. get your Android phone connected on your local WiFi (unless you want to use your cellular data) plug in your phone using USB to your XPS on your phone, go to Settings / More / Tethering & portable hotspot enable USB tethering Running ip addr will show the network card enp0s20f0u2 (for me at least), then if no IP address is set on the card, just ask for one : # dhcpcd enp0s20f0u2 Et voil\u00e0, you have now access to the internet. Proceed with installation \u00b6 The only thing to worry about is to format the UEFI boot partition as FAT32. # mkfs.vfat -F 32 /dev/nvme0n1p1 Then follow the Gentoo handbook as usual for the next steps of the installation process until you arrive to the kernel and the bootloader / grub part. From this moment I can already say that NO we won't be using GRUB at all so don't bother installing it. Why ? Because at the time of writing, the efi-64 support of GRUB was totally not working at all as it failed to discover the NVME SSD disk on boot. Kernel sources and consideration \u00b6 The trick here is that we'll setup the boot ourselves directly from the BIOS later so we only need to build a standalone kernel (meaning able to boot without an initramfs). EDIT: as of Jan. 10 2016, kernel 4.4 is available on portage so you don't need the patching below any more ! Make sure you install and use at least a 4.3.x kernel (4.3.3 at the time of writing). Add sys-kernel/gentoo-sources to your /etc/portage/package.keywords file if needed. If you have a 4.4 kernel available, you can skip patching it below. Patching 4.3.x kernels for Broadcom 4350 WiFi support \u00b6 To get the broadcom 4350 WiFi card working on 4.3.x, we need to patch the kernel sources. This is very easy to do thanks to Gentoo's user patches support. Do this before installing gentoo-sources (or reinstall it afterwards) . This example is for gentoo-sources-4.3.3, adjust your version accordingly : (chroot) # mkdir -p /etc/portage/patches/sys-kernel/gentoo-sources-4.3.3 (chroot) # cd /etc/portage/patches/sys-kernel/gentoo-sources-4.3.3 (chroot) # wget http://ultrabug.fr/gentoo/xps9350/0001-bcm4350.patch When emerging the gentoo-sources package, you should see the patch being applied. Check that it worked by issuing : (chroot) # grep BRCM_CC_4350 /usr/src/linux/drivers/net/wireless/brcm80211/brcmfmac/chip.c case BRCM_CC_4350_CHIP_ID: The resulting kernel module will be called brcmfmac , make sure to load it on boot by adding it in your /etc/conf.d/modules : modules=\"brcmfmac\" EDIT: as of Jan. 7 2016, version 20151207 of linux-firmware ships with the needed files so you don't need to download those any more ! Then we need to download the WiFi card's firmware files which are not part of the linux-firmware package at the time of writing (20150012). (chroot) # emerge '>=sys-kernel/linux-firmware-20151207' DO THIS ONLY IF YOU DONT HAVE >=sys-kernel/linux-firmware-20151207 available ! \u00b6 (chroot) # cd /lib/firmware/brcm/ (chroot) # wget http://ultrabug.fr/gentoo/xps9350/BCM-0a5c-6412.hcd (chroot) # wget http://ultrabug.fr/gentoo/xps9350/brcmfmac4350-pcie.bin Kernel config & build \u00b6 I used genkernel to build my kernel. I've done a very few adjustments but these are the things to mind in this pre-built kernel : support for NVME SSD added as builtin it is builtin for ext4 only (other FS are not compiled in) support for DM_CRYPT and LUKS ciphers for encrypted /home the root partition is hardcoded in the kernel as /dev/nvme0n1p3 so if yours is different, you'll need to change CONFIG_CMDLINE and compile it yourself the CONFIG_CMDLINE above is needed because you can't pass kernel parameters using UEFI so you have to hardcode them in the kernel itself support for the intel graphic card DRM and framebuffer (there's a kernel bug with skylake CPUs which will spam the logs but it still works good) Get the kernel config and compile it : EDIT: updated kernel config to 4.4.4 with SD Card support. (chroot) # mkdir -p /etc/kernels (chroot) # cd /etc/kernels (chroot) # wget http://ultrabug.fr/gentoo/xps9350/kernel-config-x86_64-4.4.4-gentoo (chroot) # genkernel kernel The proposed kernel config here is for gentoo-sources-4.4.4 so make sure to rename the file for your current version . EDIT: you can use and download a recent working kernel config at the beginning of the article. This kernel is far from perfect but it works very good so far, sound, webcam and suspend work smoothly ! make.conf settings for intel graphics \u00b6 I can recommend using the following on your /etc/portage/make.conf : INPUT_DRIVERS=\"evdev synaptics\" VIDEO_CARDS=\"intel i965\" fstab for SSD \u00b6 Don't forget to make sure the noatime option is used on your fstab for / and /home ! /dev/nvme0n1p1 /boot vfat noauto,noatime 1 2 /dev/nvme0n1p2 none swap sw 0 0 /dev/nvme0n1p3 / ext4 noatime 0 1 /dev/nvme0n1p4 /home ext4 noatime 0 1 As pointed out by stefantalpalaru on comments, it is recommended to schedule a SSD TRIM on your crontab once in a while, see Gentoo Wiki on SSD for more details. encrypted /home auto-mounted at login \u00b6 I advise adding the cryptsetup to your USE variable in /etc/portage/make.conf and then updating your @world with a emerge -NDuq @world . I assume you don't have created your user yet so your unmounted /home is empty. Make sure that : your /dev/nvme0n1p4 home partition is not mounted you removed the corresponding /home line from your /etc/fstab (we'll configure pam_mount to get it auto-mounted on login) AFAIK, the LUKS password you'll set on the first slot when issuing luksFormat below should be the same as your user's password ! (chroot) # cryptsetup luksFormat -s 512 /dev/nvme0n1p4 (chroot) # cryptsetup luksOpen /dev/nvme0n1p4 crypt_home (chroot) # mkfs.ext4 /dev/mapper/crypt_home (chroot) # mount /dev/mapper/crypt_home /home (chroot) # useradd -m -G wheel,audio,video,plugdev,portage,users USERNAME (chroot) # passwd USERNAME (chroot) # umount /home (chroot) # cryptsetup luksClose crypt_home We'll use sys-auth/ pam_mount to manage the mounting of our /home partition when a user logs in successfully, so make sure you emerge pam_mount first, then configure the following files : /etc/security/pam_mount.conf.xml (only line added is the volume one) <!-- debug should come before everything else, since this file is still processed in a single pass from top-to-bottom --> <!-- Volume definitions --> <!-- pam\\_mount parameters: General tunables --> <!-- pam\\_mount parameters: Volume-related --> /etc/pam.d/system-auth (only lines added are the ones with pam_mount.so ) auth required pam_env.so auth required pam_unix.so try_first_pass likeauth nullok auth optional pam_mount.so auth optional pam_permit.so account required pam_unix.so account optional pam_permit.so password optional pam_mount.so password required pam_cracklib.so difok=2 minlen=8 dcredit=2 ocredit=2 retry=3 password required pam_unix.so try_first_pass use_authtok nullok sha512 shadow password optional pam_permit.so session optional pam_mount.so session required pam_limits.so session required pam_env.so session required pam_unix.so session optional pam_permit.so That's it, easy heh ?! When you login as your user, pam_mount will decrypt your home partition using your user's password and mount it on /home ! UEFI booting your Gentoo Linux \u00b6 The best (and weird ?) way I found for booting the installed Gentoo Linux and its kernel is to configure the UEFI boot directly from the XPS BIOS. The idea is that the BIOS can read the files from the EFI boot partition since it is formatted as FAT32. All we have to do is create a new boot option from the BIOS and configure it to use the kernel file stored in the EFI boot partition. reboot your machine get on the BIOS (hit F2) get on the General / Boot Sequence menu click Add set a name (like Gentoo 4.3.3) and find + select the kernel file (use the integrated file finder) remove all unwanted boot options save it and reboot Your Gentoo kernel and OpenRC will be booting now ! Suggestions, corrections, enhancements ? \u00b6 As I said, I wrote all this quickly to spare some time to whoever it could help. I'm sure there are a lot of improvements to be done still so I'll surely update this article later on.","title":"Gentoo Linux on DELL XPS 13 9350"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#uefi-or-mbr","text":"This machine ships with a NVME SSD so don't think twice : UEFI is the only sane way to go.","title":"UEFI or MBR ?"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#bios-configuration","text":"I advise to use the pre-installed Windows 10 to update the XPS to the latest BIOS (1.1.7 at the time of writing). Then you need to change some stuff to boot and get the NVME SSD disk discovered by the live CD. Turn off Secure Boot Set SATA Operation to AHCI (will break your Windows boot but who cares)","title":"BIOS configuration"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#live-cd","text":"Go for the latest SystemRescueCD (it's Gentoo based, you won't be lost) as it's quite more up to date and supports booting on UEFI. Make it a Live USB for example using unetbootin and the ISO on a vfat formatted USB stick .","title":"Live CD"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#nvme-ssd-disk-partitioning","text":"We'll be using GPT with UEFI . I found that using gdisk was the easiest. The disk itself is found on /dev/nvme0n1 . Here it is the partition table I used : 500Mo UEFI boot partition (type EF00) 16Go Swap partition 60Go Linux root partition 400Go home partition The corresponding gdisk commands : # gdisk /dev/nvme0n1 Command: o \u21b5 This option deletes all partitions and creates a new protective MBR. Proceed? (Y/N): y \u21b5 Command: n \u21b5 Partition Number: 1 \u21b5 First sector: \u21b5 Last sector: +500M \u21b5 Hex Code: EF00 \u21b5 Command: n \u21b5 Partition Number: 2 \u21b5 First sector: \u21b5 Last sector: +16G \u21b5 Hex Code: 8200 \u21b5 Command: n \u21b5 Partition Number: 3 \u21b5 First sector: \u21b5 Last sector: +60G \u21b5 Hex Code: \u21b5 Command: n \u21b5 Partition Number: 4 \u21b5 First sector: \u21b5 Last sector: \u21b5 (for rest of disk) Hex Code: \u21b5 Command: w \u21b5 Do you want to proceed? (Y/N): Y \u21b5","title":"NVME SSD disk partitioning"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#no-wifi-on-live-cd-no-panic","text":"If your live CD is old (pre 4.4 kernel), the integrated broadcom 4350 wifi card won't be available ! My trick was to use my Android phone connected to my local WiFi as a USB modem which was detected directly by the live CD. get your Android phone connected on your local WiFi (unless you want to use your cellular data) plug in your phone using USB to your XPS on your phone, go to Settings / More / Tethering & portable hotspot enable USB tethering Running ip addr will show the network card enp0s20f0u2 (for me at least), then if no IP address is set on the card, just ask for one : # dhcpcd enp0s20f0u2 Et voil\u00e0, you have now access to the internet.","title":"No WiFi on Live CD ? no panic"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#proceed-with-installation","text":"The only thing to worry about is to format the UEFI boot partition as FAT32. # mkfs.vfat -F 32 /dev/nvme0n1p1 Then follow the Gentoo handbook as usual for the next steps of the installation process until you arrive to the kernel and the bootloader / grub part. From this moment I can already say that NO we won't be using GRUB at all so don't bother installing it. Why ? Because at the time of writing, the efi-64 support of GRUB was totally not working at all as it failed to discover the NVME SSD disk on boot.","title":"Proceed with installation"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#kernel-sources-and-consideration","text":"The trick here is that we'll setup the boot ourselves directly from the BIOS later so we only need to build a standalone kernel (meaning able to boot without an initramfs). EDIT: as of Jan. 10 2016, kernel 4.4 is available on portage so you don't need the patching below any more ! Make sure you install and use at least a 4.3.x kernel (4.3.3 at the time of writing). Add sys-kernel/gentoo-sources to your /etc/portage/package.keywords file if needed. If you have a 4.4 kernel available, you can skip patching it below.","title":"Kernel sources and consideration"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#patching-43x-kernels-for-broadcom-4350-wifi-support","text":"To get the broadcom 4350 WiFi card working on 4.3.x, we need to patch the kernel sources. This is very easy to do thanks to Gentoo's user patches support. Do this before installing gentoo-sources (or reinstall it afterwards) . This example is for gentoo-sources-4.3.3, adjust your version accordingly : (chroot) # mkdir -p /etc/portage/patches/sys-kernel/gentoo-sources-4.3.3 (chroot) # cd /etc/portage/patches/sys-kernel/gentoo-sources-4.3.3 (chroot) # wget http://ultrabug.fr/gentoo/xps9350/0001-bcm4350.patch When emerging the gentoo-sources package, you should see the patch being applied. Check that it worked by issuing : (chroot) # grep BRCM_CC_4350 /usr/src/linux/drivers/net/wireless/brcm80211/brcmfmac/chip.c case BRCM_CC_4350_CHIP_ID: The resulting kernel module will be called brcmfmac , make sure to load it on boot by adding it in your /etc/conf.d/modules : modules=\"brcmfmac\" EDIT: as of Jan. 7 2016, version 20151207 of linux-firmware ships with the needed files so you don't need to download those any more ! Then we need to download the WiFi card's firmware files which are not part of the linux-firmware package at the time of writing (20150012). (chroot) # emerge '>=sys-kernel/linux-firmware-20151207'","title":"Patching 4.3.x kernels for Broadcom 4350 WiFi support"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#do-this-only-if-you-dont-have-sys-kernellinux-firmware-20151207-available","text":"(chroot) # cd /lib/firmware/brcm/ (chroot) # wget http://ultrabug.fr/gentoo/xps9350/BCM-0a5c-6412.hcd (chroot) # wget http://ultrabug.fr/gentoo/xps9350/brcmfmac4350-pcie.bin","title":"DO THIS ONLY IF YOU DONT HAVE &gt;=sys-kernel/linux-firmware-20151207 available !"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#kernel-config-build","text":"I used genkernel to build my kernel. I've done a very few adjustments but these are the things to mind in this pre-built kernel : support for NVME SSD added as builtin it is builtin for ext4 only (other FS are not compiled in) support for DM_CRYPT and LUKS ciphers for encrypted /home the root partition is hardcoded in the kernel as /dev/nvme0n1p3 so if yours is different, you'll need to change CONFIG_CMDLINE and compile it yourself the CONFIG_CMDLINE above is needed because you can't pass kernel parameters using UEFI so you have to hardcode them in the kernel itself support for the intel graphic card DRM and framebuffer (there's a kernel bug with skylake CPUs which will spam the logs but it still works good) Get the kernel config and compile it : EDIT: updated kernel config to 4.4.4 with SD Card support. (chroot) # mkdir -p /etc/kernels (chroot) # cd /etc/kernels (chroot) # wget http://ultrabug.fr/gentoo/xps9350/kernel-config-x86_64-4.4.4-gentoo (chroot) # genkernel kernel The proposed kernel config here is for gentoo-sources-4.4.4 so make sure to rename the file for your current version . EDIT: you can use and download a recent working kernel config at the beginning of the article. This kernel is far from perfect but it works very good so far, sound, webcam and suspend work smoothly !","title":"Kernel config &amp; build"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#makeconf-settings-for-intel-graphics","text":"I can recommend using the following on your /etc/portage/make.conf : INPUT_DRIVERS=\"evdev synaptics\" VIDEO_CARDS=\"intel i965\"","title":"make.conf settings for intel graphics"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#fstab-for-ssd","text":"Don't forget to make sure the noatime option is used on your fstab for / and /home ! /dev/nvme0n1p1 /boot vfat noauto,noatime 1 2 /dev/nvme0n1p2 none swap sw 0 0 /dev/nvme0n1p3 / ext4 noatime 0 1 /dev/nvme0n1p4 /home ext4 noatime 0 1 As pointed out by stefantalpalaru on comments, it is recommended to schedule a SSD TRIM on your crontab once in a while, see Gentoo Wiki on SSD for more details.","title":"fstab for SSD"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#encrypted-home-auto-mounted-at-login","text":"I advise adding the cryptsetup to your USE variable in /etc/portage/make.conf and then updating your @world with a emerge -NDuq @world . I assume you don't have created your user yet so your unmounted /home is empty. Make sure that : your /dev/nvme0n1p4 home partition is not mounted you removed the corresponding /home line from your /etc/fstab (we'll configure pam_mount to get it auto-mounted on login) AFAIK, the LUKS password you'll set on the first slot when issuing luksFormat below should be the same as your user's password ! (chroot) # cryptsetup luksFormat -s 512 /dev/nvme0n1p4 (chroot) # cryptsetup luksOpen /dev/nvme0n1p4 crypt_home (chroot) # mkfs.ext4 /dev/mapper/crypt_home (chroot) # mount /dev/mapper/crypt_home /home (chroot) # useradd -m -G wheel,audio,video,plugdev,portage,users USERNAME (chroot) # passwd USERNAME (chroot) # umount /home (chroot) # cryptsetup luksClose crypt_home We'll use sys-auth/ pam_mount to manage the mounting of our /home partition when a user logs in successfully, so make sure you emerge pam_mount first, then configure the following files : /etc/security/pam_mount.conf.xml (only line added is the volume one) <!-- debug should come before everything else, since this file is still processed in a single pass from top-to-bottom --> <!-- Volume definitions --> <!-- pam\\_mount parameters: General tunables --> <!-- pam\\_mount parameters: Volume-related --> /etc/pam.d/system-auth (only lines added are the ones with pam_mount.so ) auth required pam_env.so auth required pam_unix.so try_first_pass likeauth nullok auth optional pam_mount.so auth optional pam_permit.so account required pam_unix.so account optional pam_permit.so password optional pam_mount.so password required pam_cracklib.so difok=2 minlen=8 dcredit=2 ocredit=2 retry=3 password required pam_unix.so try_first_pass use_authtok nullok sha512 shadow password optional pam_permit.so session optional pam_mount.so session required pam_limits.so session required pam_env.so session required pam_unix.so session optional pam_permit.so That's it, easy heh ?! When you login as your user, pam_mount will decrypt your home partition using your user's password and mount it on /home !","title":"encrypted /home auto-mounted at login"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#uefi-booting-your-gentoo-linux","text":"The best (and weird ?) way I found for booting the installed Gentoo Linux and its kernel is to configure the UEFI boot directly from the XPS BIOS. The idea is that the BIOS can read the files from the EFI boot partition since it is formatted as FAT32. All we have to do is create a new boot option from the BIOS and configure it to use the kernel file stored in the EFI boot partition. reboot your machine get on the BIOS (hit F2) get on the General / Boot Sequence menu click Add set a name (like Gentoo 4.3.3) and find + select the kernel file (use the integrated file finder) remove all unwanted boot options save it and reboot Your Gentoo kernel and OpenRC will be booting now !","title":"UEFI booting your Gentoo Linux"},{"location":"Tech%20Blog/2016/2016-01-02-gentoo-linux-on-dell-xps-13-9350/#suggestions-corrections-enhancements","text":"As I said, I wrote all this quickly to spare some time to whoever it could help. I'm sure there are a lot of improvements to be done still so I'll surely update this article later on.","title":"Suggestions, corrections, enhancements ?"},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/","text":"py3status v2.9 is out with a good bunch of new modules, exciting improvements and fixes ! Thanks \u00b6 This release is made of their stuff, thank you contributors ! @4iar @AnwariasEu @cornerman Alexandre Bonnetain Alexis 'Horgix' Chotard Andrwe Lord Weber Ben Oswald Daniel Foerster Iain Tatch Johannes Karoff Markus Weimar Rail Aliiev Themistokle Benetatos New modules \u00b6 arch_updates module, by Iain Tatch deadbeef module to show current track playing, by Themistokle Benetatos icinga2 module, by Ben Oswald scratchpad_async module, by johannes karoff wifi module, by Markus Weimar Fixes and enhancements \u00b6 Rail Aliiev implement flake8 check via travis-ci, we now have a new build-passing badge fix: handle format_time tztime parameter thx to @cornerman, fix issue #177 fix: respect ordering of the ipv6 i3status module even on empty configuration, fix #158 as reported by @nazco battery_level module: add multiple battery support, by 4iar battery_level module: added formatting options, by Alexandre Bonnetain battery_level module: added option hide_seconds, by Andrwe Lord Weber dpms module: added color support, by Andrwe Lord Weber spotify module: added format_down option, by Andrwe Lord Weber spotify module: fixed color & playbackstatus check, by Andrwe Lord Weber spotify module: workaround broken dbus, removed PlaybackStatus query, by christian weather_yahoo module: support woeid, add more configuration parameters, by Rail Aliiev What's next ? \u00b6 Some major core enhancements and code clean up are coming up thanks to @cornerman, @Horgix and @pydsigner. The next release will be faster than ever and even less CPU consuming ! Meanwhile, this 2.9 release is available on pypi and Gentoo portage, have fun !","title":"py3status v2.9"},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/#thanks","text":"This release is made of their stuff, thank you contributors ! @4iar @AnwariasEu @cornerman Alexandre Bonnetain Alexis 'Horgix' Chotard Andrwe Lord Weber Ben Oswald Daniel Foerster Iain Tatch Johannes Karoff Markus Weimar Rail Aliiev Themistokle Benetatos","title":"Thanks"},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/#new-modules","text":"arch_updates module, by Iain Tatch deadbeef module to show current track playing, by Themistokle Benetatos icinga2 module, by Ben Oswald scratchpad_async module, by johannes karoff wifi module, by Markus Weimar","title":"New modules"},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/#fixes-and-enhancements","text":"Rail Aliiev implement flake8 check via travis-ci, we now have a new build-passing badge fix: handle format_time tztime parameter thx to @cornerman, fix issue #177 fix: respect ordering of the ipv6 i3status module even on empty configuration, fix #158 as reported by @nazco battery_level module: add multiple battery support, by 4iar battery_level module: added formatting options, by Alexandre Bonnetain battery_level module: added option hide_seconds, by Andrwe Lord Weber dpms module: added color support, by Andrwe Lord Weber spotify module: added format_down option, by Andrwe Lord Weber spotify module: fixed color & playbackstatus check, by Andrwe Lord Weber spotify module: workaround broken dbus, removed PlaybackStatus query, by christian weather_yahoo module: support woeid, add more configuration parameters, by Rail Aliiev","title":"Fixes and enhancements"},{"location":"Tech%20Blog/2016/2016-03-02-py3status-v2-9/#whats-next","text":"Some major core enhancements and code clean up are coming up thanks to @cornerman, @Horgix and @pydsigner. The next release will be faster than ever and even less CPU consuming ! Meanwhile, this 2.9 release is available on pypi and Gentoo portage, have fun !","title":"What's next ?"},{"location":"Tech%20Blog/2016/2016-06-25-py3status-v3-0/","text":"Oh boy, this new version is so amazing in terms of improvements and contributions that it's hard to sum it up ! Before going into more explanations I want to dedicate this release to tobes whose contributions, hard work and patience have permitted this ambitious 3.0 : THANK YOU ! This is the graph of contributed commits since 2.9 just so you realise how much this version is thanks to him: I can't continue on without also thanking Horgix who started this madness by splitting the code base into modular files and pydsigner for his everlasting contributions and code reviews ! The git stat since 2.9 also speaks for itself: 73 files changed, 7600 insertions(+), 3406 deletions(-) So what's new ? \u00b6 the monolithic code base have been split into modules responsible for the given tasks py3status performs major improvements on modules output orchestration and execution resulting in considerable CPU consumption reduction and i3bar responsiveness refactoring of user notifications with added dbus support and rate limiting improved modules error reporting py3status can now survive an i3status crash and will try to respawn it a new 'container' module output type gives the ability to group modules together refactoring of the time and tztime modules support brings the support of all the time macros (%d, %Z etc) support for stopping py3status and its modules when i3bar hide mode is used refactoring of general, contribution and most noticeably modules documentation more details on the rest of the changelog Modules \u00b6 Along with a cool list of improvements on the existing modules, these are the new modules: new group module to cycle display of several modules (check it out, it's insanely handy !) new fedora_updates module to check for your Fedora packages updates new github module to check a github repository and notifications new graphite module to check metrics from graphite new insync module to check your current insync status new timer module to have a simple countdown displayed new twitch_streaming module to check is a Twitch Streamer is online new vpn_status module to check your VPN status new xrandr_rotate module to rotate your screens new yandexdisk_status module to display Yandex.Disk status Contributors \u00b6 And of course thank you to all the others who made this version possible ! @egeskow Alex Caswell Johannes Karoff Joshua Pratt Maxim Baz Nathan Smith Themistokle Benetatos Vladimir Potapev Yongming Lai","title":"py3status v3.0"},{"location":"Tech%20Blog/2016/2016-06-25-py3status-v3-0/#so-whats-new","text":"the monolithic code base have been split into modules responsible for the given tasks py3status performs major improvements on modules output orchestration and execution resulting in considerable CPU consumption reduction and i3bar responsiveness refactoring of user notifications with added dbus support and rate limiting improved modules error reporting py3status can now survive an i3status crash and will try to respawn it a new 'container' module output type gives the ability to group modules together refactoring of the time and tztime modules support brings the support of all the time macros (%d, %Z etc) support for stopping py3status and its modules when i3bar hide mode is used refactoring of general, contribution and most noticeably modules documentation more details on the rest of the changelog","title":"So what's new ?"},{"location":"Tech%20Blog/2016/2016-06-25-py3status-v3-0/#modules","text":"Along with a cool list of improvements on the existing modules, these are the new modules: new group module to cycle display of several modules (check it out, it's insanely handy !) new fedora_updates module to check for your Fedora packages updates new github module to check a github repository and notifications new graphite module to check metrics from graphite new insync module to check your current insync status new timer module to have a simple countdown displayed new twitch_streaming module to check is a Twitch Streamer is online new vpn_status module to check your VPN status new xrandr_rotate module to rotate your screens new yandexdisk_status module to display Yandex.Disk status","title":"Modules"},{"location":"Tech%20Blog/2016/2016-06-25-py3status-v3-0/#contributors","text":"And of course thank you to all the others who made this version possible ! @egeskow Alex Caswell Johannes Karoff Joshua Pratt Maxim Baz Nathan Smith Themistokle Benetatos Vladimir Potapev Yongming Lai","title":"Contributors"},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/","text":"It was about time I added a new package to portage and I'm very glad it to be RethinkDB and its python driver ! dev-db/rethinkdb dev-python/python-rethinkdb For those of you who never heard about this database, I urge you to go about their excellent website and have a good read. Packaging RethinkDB \u00b6 RethinkDB has been under my radar for quite a long time now and when they finally got serious enough about high availability I also got serious about using it at work... and obviously \"getting serious\" + \"work\" means packaging it for Gentoo Linux :) Quick notes on packaging for Gentoo Linux: This is a C++ project so it feels natural and easy to grasp The configure script already offers a way of using system libraries instead of the bundled ones which is in line with Gentoo's QA policy The only grey zone about the above statement is the web UI which is used precompiled RethinkDB has a few QA violations which the ebuild is addressing by modifying the sources: There is a configure.default which tries to force some configure options The configure is missing some options to avoid auto installing some docs and init scripts The build system does its best to guess the CXX compiler but it should offer an option to set it directly The build system does not respect users' CXXFLAGS and tries to force the usage of -03 Getting our hands into RethinkDB \u00b6 At work, we finally found the excuse to get our hands into RethinkDB when we challenged ourselves with developing a quizz game for our booth as a sponsor of Europython 2016. It was a simple game where you were presented a question and four possible answers and you had 60 seconds to answer as much of them as you could. The trick is that we wanted to create an interactive game where the participant had to play on a tablet but the rest of the audience got to see who was currently playing and follow their score progression + their ranking for the day and the week in real time on another screen ! Another challenge for us in the creation of this game is that we only used technologies that were new to us and even switched jobs so the backend python guys would be doing the frontend javascript et vice et versa. The stack finally went like this : Game quizz frontend : Angular2 (TypeScript) Game questions API : Go Real time scores frontend : Angular2 + autobahn Real time scores API : python 3.5 asyncio + autobahn Database : RethinkDB As you can see on the stack we chose RethinkDB for its main strength : real time updates pushed to the connected clients . The real time scores frontend and API were bonded together using autobahn while the API was using the changefeeds (realtime updates coming from the database) and broadcasting them to the frontend. What we learnt about RethinkDB \u00b6 We're sure that we want to use it in production ! The ReQL query language is a pipeline so its syntax is quite tricky to get familiar with (even more when coming from mongoDB like us), it is as powerful as it can be disconcerting Realtime changefeeds have limitations which are sometimes not so easy to understand/find out (especially the order_by / secondary index part) Changefeeds limitations is a constraint you have to take into account in your data modeling ! Changefeeds + order_by can do the ordering for you when using the include_offsets option, this is amazing The administration web UI is awesome The python 3.5 asyncio proper support is still not merged , this is a pain ! Try it out \u00b6 Now that you can emerge rethinkdb I encourage you to try this awesome database. Be advised that the ebuild also provides a way of configuring your rethinkdb instance by running emerge --config dev-db/rethinkdb ! I'll now try to get in touch with upstream to get Gentoo listed on their website .","title":"RethinkDB on Gentoo Linux"},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/#packaging-rethinkdb","text":"RethinkDB has been under my radar for quite a long time now and when they finally got serious enough about high availability I also got serious about using it at work... and obviously \"getting serious\" + \"work\" means packaging it for Gentoo Linux :) Quick notes on packaging for Gentoo Linux: This is a C++ project so it feels natural and easy to grasp The configure script already offers a way of using system libraries instead of the bundled ones which is in line with Gentoo's QA policy The only grey zone about the above statement is the web UI which is used precompiled RethinkDB has a few QA violations which the ebuild is addressing by modifying the sources: There is a configure.default which tries to force some configure options The configure is missing some options to avoid auto installing some docs and init scripts The build system does its best to guess the CXX compiler but it should offer an option to set it directly The build system does not respect users' CXXFLAGS and tries to force the usage of -03","title":"Packaging RethinkDB"},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/#getting-our-hands-into-rethinkdb","text":"At work, we finally found the excuse to get our hands into RethinkDB when we challenged ourselves with developing a quizz game for our booth as a sponsor of Europython 2016. It was a simple game where you were presented a question and four possible answers and you had 60 seconds to answer as much of them as you could. The trick is that we wanted to create an interactive game where the participant had to play on a tablet but the rest of the audience got to see who was currently playing and follow their score progression + their ranking for the day and the week in real time on another screen ! Another challenge for us in the creation of this game is that we only used technologies that were new to us and even switched jobs so the backend python guys would be doing the frontend javascript et vice et versa. The stack finally went like this : Game quizz frontend : Angular2 (TypeScript) Game questions API : Go Real time scores frontend : Angular2 + autobahn Real time scores API : python 3.5 asyncio + autobahn Database : RethinkDB As you can see on the stack we chose RethinkDB for its main strength : real time updates pushed to the connected clients . The real time scores frontend and API were bonded together using autobahn while the API was using the changefeeds (realtime updates coming from the database) and broadcasting them to the frontend.","title":"Getting our hands into RethinkDB"},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/#what-we-learnt-about-rethinkdb","text":"We're sure that we want to use it in production ! The ReQL query language is a pipeline so its syntax is quite tricky to get familiar with (even more when coming from mongoDB like us), it is as powerful as it can be disconcerting Realtime changefeeds have limitations which are sometimes not so easy to understand/find out (especially the order_by / secondary index part) Changefeeds limitations is a constraint you have to take into account in your data modeling ! Changefeeds + order_by can do the ordering for you when using the include_offsets option, this is amazing The administration web UI is awesome The python 3.5 asyncio proper support is still not merged , this is a pain !","title":"What we learnt about RethinkDB"},{"location":"Tech%20Blog/2016/2016-09-02-rethinkdb-on-gentoo-linux/#try-it-out","text":"Now that you can emerge rethinkdb I encourage you to try this awesome database. Be advised that the ebuild also provides a way of configuring your rethinkdb instance by running emerge --config dev-db/rethinkdb ! I'll now try to get in touch with upstream to get Gentoo listed on their website .","title":"Try it out"},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/","text":"Ok I slacked by not posting for v3.1 and v3.2 and I should have since those previous versions were awesome and feature rich. But v3.3 is another major milestone which was made possible by tremendous contributions from @ tobes as usual and also greatly thanks to the hard work of @ guiniol and @ pferate who I'd like to mention and thank again ! Also, I'd like to mention that @tobes has become the first collaborator of the py3status project ! Instead of doing a changelog review, I'll highlight some of the key features that got introduced and extended during those versions. The py3 helper \u00b6 Writing powerful py3status modules have never been so easy thanks to the py3 helper ! This magical object is added automatically to modules and provides a lot of useful methods to help normalize and enhance modules capabilities. This is a non exhaustive list of such methods: format_units : to pretty format units (KB, MB etc) notify_user: send a notification to the user time_in : to handle module cache expiration easily safe_format : use the extended formatter to handle the module's output in a powerful way (see below) check_commands: check if the listed commands are available on the system command_run: execute the given command command_output: execute the command and get its output play_sound : sound notifications ! Powerful control over the modules' output \u00b6 Using the self.py3.safe_format helper will unleash a feature rich formatter that one can use to conditionally select the output of a module based on its content. Square brackets [] can be used. The content of them will be removed from the output if there is no valid placeholder contained within. They can also be nested. A pipe (vertical bar) | can be used to divide sections the first valid section only will be shown in the output. A backslash \\ can be used to escape a character eg \\[ will show [ in the output. \\? is special and is used to provide extra commands to the format string, example \\?color=#FF00FF . Multiple commands can be given using an ampersand & as a separator, example \\?color=#FF00FF&show . {<placeholder>} will be converted, or removed if it is None or empty. Formatting can also be applied to the placeholder eg {number:03.2f} . Example format_string: \u00b6 This will show artist - title if artist is present, title if title but no artist, and file if file is present but not artist or title. \"[[{artist} - ]{title}]|{file}\" More code and documentation tests \u00b6 A lot of efforts have been put into py3status automated CI and feature testing allowing more confidence in the advanced features we develop while keeping a higher standard on code quality. This is such as even modules' docstrings are now tested for bad formatting :) Colouring and thresholds A special effort have been put in normalizing modules' output colouring with the added refinement of normalized thresholds to give users more power over their output. See the colouring documentation See the thresholds documentation New modules, on and on ! \u00b6 new clock module to display multiple times and dates informations in a flexible way, by @tobes new coin_balance module to display balances of diverse crypto-currencies, by Felix Morgner new diskdata module to shows both usage data and IO data from disks, by @guiniol new exchange_rate module to check for your favorite currency rates, by @tobes new file_status module to check the presence of a file, by @ritze new frame module to group and display multiple modules inline, by @tobes new gpmdp module for Google Play Music Desktop Player by @Spirotot new kdeconnector module to display information about Android devices, by @ritze new mpris module to control MPRIS enabled music players, by @ritze new net_iplist module to display interfaces and their IPv4 and IPv6 IP addresses, by @guiniol new process_status module to check the presence of a process, by @ritze new rainbow module to enlight your day, by @tobes new tcp_status module to check for a given TCP port on a host, by @ritze Changelog \u00b6 The changelog is very big and the next 3.4 milestone is very promising with amazing new features giving you even more power over your i3bar, stay tuned ! Thank you contributors \u00b6 Still a lot of new timer contributors which I take great pride in as I see it as py3status being an accessible project. @btall @chezstov @coxley Felix Morgner Gabriel F\u00e9ron @guiniol @inclementweather @jakubjedelsky Jan Mr\u00e1zek @m45t3r Maxim Baz @pferate @ritze @rixx @Spirotot @Stautob @tjaartvdwalt Yuli Khodorkovskiy @ZeiP","title":"py3status v3.3"},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#the-py3-helper","text":"Writing powerful py3status modules have never been so easy thanks to the py3 helper ! This magical object is added automatically to modules and provides a lot of useful methods to help normalize and enhance modules capabilities. This is a non exhaustive list of such methods: format_units : to pretty format units (KB, MB etc) notify_user: send a notification to the user time_in : to handle module cache expiration easily safe_format : use the extended formatter to handle the module's output in a powerful way (see below) check_commands: check if the listed commands are available on the system command_run: execute the given command command_output: execute the command and get its output play_sound : sound notifications !","title":"The py3 helper"},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#powerful-control-over-the-modules-output","text":"Using the self.py3.safe_format helper will unleash a feature rich formatter that one can use to conditionally select the output of a module based on its content. Square brackets [] can be used. The content of them will be removed from the output if there is no valid placeholder contained within. They can also be nested. A pipe (vertical bar) | can be used to divide sections the first valid section only will be shown in the output. A backslash \\ can be used to escape a character eg \\[ will show [ in the output. \\? is special and is used to provide extra commands to the format string, example \\?color=#FF00FF . Multiple commands can be given using an ampersand & as a separator, example \\?color=#FF00FF&show . {<placeholder>} will be converted, or removed if it is None or empty. Formatting can also be applied to the placeholder eg {number:03.2f} .","title":"Powerful control over the modules' output"},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#example-format_string","text":"This will show artist - title if artist is present, title if title but no artist, and file if file is present but not artist or title. \"[[{artist} - ]{title}]|{file}\"","title":"Example format_string:"},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#more-code-and-documentation-tests","text":"A lot of efforts have been put into py3status automated CI and feature testing allowing more confidence in the advanced features we develop while keeping a higher standard on code quality. This is such as even modules' docstrings are now tested for bad formatting :) Colouring and thresholds A special effort have been put in normalizing modules' output colouring with the added refinement of normalized thresholds to give users more power over their output. See the colouring documentation See the thresholds documentation","title":"More code and documentation tests"},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#new-modules-on-and-on","text":"new clock module to display multiple times and dates informations in a flexible way, by @tobes new coin_balance module to display balances of diverse crypto-currencies, by Felix Morgner new diskdata module to shows both usage data and IO data from disks, by @guiniol new exchange_rate module to check for your favorite currency rates, by @tobes new file_status module to check the presence of a file, by @ritze new frame module to group and display multiple modules inline, by @tobes new gpmdp module for Google Play Music Desktop Player by @Spirotot new kdeconnector module to display information about Android devices, by @ritze new mpris module to control MPRIS enabled music players, by @ritze new net_iplist module to display interfaces and their IPv4 and IPv6 IP addresses, by @guiniol new process_status module to check the presence of a process, by @ritze new rainbow module to enlight your day, by @tobes new tcp_status module to check for a given TCP port on a host, by @ritze","title":"New modules, on and on !"},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#changelog","text":"The changelog is very big and the next 3.4 milestone is very promising with amazing new features giving you even more power over your i3bar, stay tuned !","title":"Changelog"},{"location":"Tech%20Blog/2016/2016-11-21-py3status-v3-3/#thank-you-contributors","text":"Still a lot of new timer contributors which I take great pride in as I see it as py3status being an accessible project. @btall @chezstov @coxley Felix Morgner Gabriel F\u00e9ron @guiniol @inclementweather @jakubjedelsky Jan Mr\u00e1zek @m45t3r Maxim Baz @pferate @ritze @rixx @Spirotot @Stautob @tjaartvdwalt Yuli Khodorkovskiy @ZeiP","title":"Thank you contributors"},{"location":"Tech%20Blog/2017/2017-01-27-py3status-v3-4/","text":"Another community driven and incredible update of py3status has been released ! Our contributor star for this release is without doubt @lasers who is showing some amazing energy with challenging ideas and some impressive modules QA clean ups ! Thanks a lot as usual to @tobes who is basically leading the development of py3status now days with me being in a merge button mode most of the time. By looking at the issues and pull requests I can already say that the 3.5 release will be grand ! Highlights \u00b6 support of python 3.6 thanks to @tobes a major effort in modules standardization, almost all of them support the format parameter now thanks to @lasers modules documentation has been cleaned up new do_not_disturb module to toggle notifications, by @maximbaz new rss_aggregator module to display your unread feed items, by @raspbeguy whatsmyip module: added geolocation support using ip-api.com, by @vicyap with original code from @neutronst4r See the full changelog here. Thank you guys !","title":"py3status v3.4"},{"location":"Tech%20Blog/2017/2017-01-27-py3status-v3-4/#highlights","text":"support of python 3.6 thanks to @tobes a major effort in modules standardization, almost all of them support the format parameter now thanks to @lasers modules documentation has been cleaned up new do_not_disturb module to toggle notifications, by @maximbaz new rss_aggregator module to display your unread feed items, by @raspbeguy whatsmyip module: added geolocation support using ip-api.com, by @vicyap with original code from @neutronst4r See the full changelog here. Thank you guys !","title":"Highlights"},{"location":"Tech%20Blog/2017/2017-04-10-py3status-v3-5/","text":"Howdy folks, I'm obviously slacking a bit on my blog and I'm ashamed to say that it's not the only place where I do. py3status is another of them and it wouldn't be the project it is today without @tobes. In fact, this new 3.5 release has witnessed his takeover on the top contributions on the project, so I want to extend a warm thank you and lots of congratulations on this my friend :) Also, an amazing new contributor from the USA has come around in the nickname of @lasers . He has been doing a tremendous job on module normalization, code review and feedbacks. His high energy is amazing and more than welcome. This release is mainly his, so thank you @lasers ! What's new ? \u00b6 Well the changelog has never been so large that I even don't know where to start. I guess the most noticeable change is the gorgeous and brand new documentation of py3status on readthedocs ! Apart from the enhanced guides and sections, what's amazing behind this new documentation is the level of automation efforts that @lasers and @tobes put into it. They even generate modules' screenshots programmatically ! I would never have thought of it possible :D The other main efforts on this release is about modules normalization where @lasers put so much energy in taking advantage of the formatter features and bringing all the modules to a new level of standardization . This long work brought to light some lack of features or bugs which got corrected along the way. Last but not least, the way py3status notifies you when modules fail to load/execute got changed . Now modules which fail to load or execute will not pop up a notification (i3 nagbar or dbus) but display directly in the bar where they belong. Users can left click to show the error and right click to discard them from their bar ! New modules \u00b6 Once again, new and recurring contributors helped the project get better and offer a cool set of modules, thank you contributors ! air_quality module, to display the air quality of your place, by @ beetleman and @lasers getjson module to display fields from a json url, by @ vicyap keyboard_locks module to display keyboard locks states, by @ lasers systemd module to check the status of a systemd unit, by @ adrianlzt tor_rate module to display the incoming and outgoing data rates of a Tor daemon instance, by @ fmorgner xscreensaver module, by @lasers and @ neutronst4r Special mention to @ maximbaz for his continuous efforts and help. And also a special community mention to @ valdur55 for his responsiveness and help for other users on IRC ! What's next ? \u00b6 The 3.6 version will focus on the following ideas, some sane and some crazy :) we will continue to work on the ability to add/remove/move modules in the bar at runtime i3blocks and i3pystatus support, to embed their configurations and modules inside py3status formatter optimizations finish modules normalization write more documentation and clean up the old ones Stay tuned","title":"py3status v3.5"},{"location":"Tech%20Blog/2017/2017-04-10-py3status-v3-5/#whats-new","text":"Well the changelog has never been so large that I even don't know where to start. I guess the most noticeable change is the gorgeous and brand new documentation of py3status on readthedocs ! Apart from the enhanced guides and sections, what's amazing behind this new documentation is the level of automation efforts that @lasers and @tobes put into it. They even generate modules' screenshots programmatically ! I would never have thought of it possible :D The other main efforts on this release is about modules normalization where @lasers put so much energy in taking advantage of the formatter features and bringing all the modules to a new level of standardization . This long work brought to light some lack of features or bugs which got corrected along the way. Last but not least, the way py3status notifies you when modules fail to load/execute got changed . Now modules which fail to load or execute will not pop up a notification (i3 nagbar or dbus) but display directly in the bar where they belong. Users can left click to show the error and right click to discard them from their bar !","title":"What's new ?"},{"location":"Tech%20Blog/2017/2017-04-10-py3status-v3-5/#new-modules","text":"Once again, new and recurring contributors helped the project get better and offer a cool set of modules, thank you contributors ! air_quality module, to display the air quality of your place, by @ beetleman and @lasers getjson module to display fields from a json url, by @ vicyap keyboard_locks module to display keyboard locks states, by @ lasers systemd module to check the status of a systemd unit, by @ adrianlzt tor_rate module to display the incoming and outgoing data rates of a Tor daemon instance, by @ fmorgner xscreensaver module, by @lasers and @ neutronst4r Special mention to @ maximbaz for his continuous efforts and help. And also a special community mention to @ valdur55 for his responsiveness and help for other users on IRC !","title":"New modules"},{"location":"Tech%20Blog/2017/2017-04-10-py3status-v3-5/#whats-next","text":"The 3.6 version will focus on the following ideas, some sane and some crazy :) we will continue to work on the ability to add/remove/move modules in the bar at runtime i3blocks and i3pystatus support, to embed their configurations and modules inside py3status formatter optimizations finish modules normalization write more documentation and clean up the old ones Stay tuned","title":"What's next ?"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/","text":"I recently worked a bit at how we could secure better our SSH connections to our servers at work. So far we are using the OpenSSH public key only mechanism which means that there is no password set on the servers for our users. While this was satisfactory for a time we think that this still suffers some disadvantages such as: we cannot enforce SSH private keys to have a passphrase on the user side the security level of the whole system is based on the protection of the private key which means that it's directly tied to the security level of the desktop of the users This lead us to think about adding a 2nd factor authentication to SSH and about the usage of security keys . Meet the Yubikey \u00b6 Yubikeys are security keys made by Yubico. They can support multiple modes and work with the U2F open authentication standard which is why they got my attention. I decided to try the Yubikey 4 because it can act as a smartcard while offering these interesting features: Challenge-Response OTP GPG PIV Method 1 - SSH using pam_ssh + pam_yubico \u00b6 The first method I found satisfactory was to combine pam_ssh authentication module along with pam_yubico as a 2nd factor. This allows server side passphrase enforcement on SSH and the usage of the security key to login. TL;DR: two gotchas before we begin This method is still quite complex to setup but allows you to use the cheaper U2F only Yubikeys . The second method based on PIV is the solution we chose as it fits better our use cases and ecosystem. ADVISE: keep a root SSH session to your servers while deploying/testing this so you can revert any change you make and avoid to lock yourself out of your servers. Setup pam_ssh \u00b6 Use pam_ssh on the servers to force usage of a passphrase on a private key. The idea behind pam_ssh is that the passphrase of your SSH key serves as your SSH password. Generate your SSH key pair with a passphrase on your local machine . ssh-keygen -f identity Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in identity. Your public key has been saved in identity.pub. The key fingerprint is: SHA256:a2/HNCe28+bpMZ2dIf9bodnBwnmD7stO5sdBOV6teP8 alexys@yazd The key's randomart image is: +---[RSA 2048]----+ | | | | | o| | . ++o| | S BoOo| | . B %+O| | o + %+*=| | . .. @ .*+| | ....%B.E| +----[SHA256]-----+ You then must copy your private key (named identity with no extension) to your servers under the ~/.ssh/login-keys.d/ folder. In your $HOME on the servers, you will get something like this: .ssh/ \u251c\u2500\u2500 known_hosts \u2514\u2500\u2500 login-keys.d \u2514\u2500\u2500 identity Then you can enable the pam_ssh authentication. Gentoo users should enable the pam_ssh USE flag for sys-auth/pambase and re-install. Add this at the beginning of the file /etc/pam.d/ssh auth required pam_ssh.so debug The debug flag can be removed after you tested it correctly. Disable public key authentication \u00b6 Because it takes precedence over the PAM authentication mechanism, you have to disable OpenSSH PubkeyAuthentication authentication on /etc/ssh/sshd_config: PubkeyAuthentication no Enable PAM authentication on /etc/ssh/sshd_config ChallengeResponseAuthentication no PasswordAuthentication no UsePAM yes Test pam_ssh \u00b6 Now you should be prompted for your SSH passphrase to login through SSH. \u279c ~ ssh cheetah SSH passphrase: Setup 2nd factor using pam_yubico \u00b6 Now we will make use of our Yubikey security key to add a 2nd factor authentication to login through SSH on our servers. Because the Yubikey is not physically plugged on the server, we cannot use an offline Challenge-Response mechanism, so we will have to use a third party to validate the challenge. Yubico gracefully provide an API for this and the pam_yubico module is meant to use it easily. Preparing your account using your Yubikey (on your machine) \u00b6 First of all, you need to get your Yubico API key ID from the following URL: https://upgrade.yubico.com/getapikey/ enter your email address tap your Yubikey to generate an OTP in the Yubikey OTP field You will get a Client ID (this you will use) and Secret Key (this you will keep safe). Then you will need to create an authorization mapping file which basically link your account to a Yubikey fingerprint (modhex). This is equivalent to saying \"this Yubikey belongs to this user and can authenticate him\". First, get your modhex: go to https://demo.yubico.com/php-yubico/Modhex_Calculator.php select OTP tap your key click \"convert all formats\" the modhex is listed as Modhex encoded: xxccccxxuuxx Using this modhex, create your mapping file named authorized_yubikeys which will be copied to ~/.yubico/authorized_yubikeys on the servers (replace LOGIN_USERNAME with your actual account login name). LOGIN_USERNAME:xxccccxxuuxx NOTE: this mapping file can be a centralized one (in /etc for example) to handle all the users from a server. See the the authfile option on the doc . Setting up OpenSSH (on your servers) \u00b6 You must install pam_yubico on the servers . For Gentoo, it's as simple as: emerge sys-auth/pam_yubico Copy your authentication mapping file to your home under the .yubico folder on all servers . You should get this: .yubico/ \u2514\u2500\u2500 authorized_yubikey Configure pam to use pam_yubico. Add this after the pam_ssh on the file /etc/pam.d/ssh which should look like this now: auth required pam_ssh.so auth required pam_yubico.so id=YOUR_API_ID debug debug_file=/var/log/auth.log The debug and debug_file flags can be removed after you tested it correctly. Testing pam_yubico \u00b6 Now you should be prompted for your SSH passphrase and then for your Yubikey OTP to login through SSH. \u279c ~ ssh cheetah SSH passphrase: YubiKey for `ultrabug': About the Yubico API dependency \u00b6 Careful readers will notice that using pam_yubico introduces a strong dependency on the Yubico API availability. If the API becomes unreachable or your internet connection goes down then your servers would be unable to authenticate you! The solution I found to this problem is to instruct pam to ignore the Yubikey authentication when pam_yubico is unable to contact the API. In this case, the module will return a AUTHINFO_UNAVAIL code to PAM which we can act upon using the following syntax. The /etc/pam.d/ssh first lines should be changed to this: auth required pam_ssh.so auth [success=done authinfo_unavail=ignore new_authtok_reqd=done default=die] pam_yubico.so id=YOUR_API_ID debug debug_file=/var/log/auth.log Now you can be sure to be able to use your Yubikey even if the API is down or unreachable ;)","title":"Hardening SSH authentication using Yubikey (1/2)"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#meet-the-yubikey","text":"Yubikeys are security keys made by Yubico. They can support multiple modes and work with the U2F open authentication standard which is why they got my attention. I decided to try the Yubikey 4 because it can act as a smartcard while offering these interesting features: Challenge-Response OTP GPG PIV","title":"Meet the Yubikey"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#method-1-ssh-using-pam_ssh-pam_yubico","text":"The first method I found satisfactory was to combine pam_ssh authentication module along with pam_yubico as a 2nd factor. This allows server side passphrase enforcement on SSH and the usage of the security key to login. TL;DR: two gotchas before we begin This method is still quite complex to setup but allows you to use the cheaper U2F only Yubikeys . The second method based on PIV is the solution we chose as it fits better our use cases and ecosystem. ADVISE: keep a root SSH session to your servers while deploying/testing this so you can revert any change you make and avoid to lock yourself out of your servers.","title":"Method 1 - SSH using pam_ssh + pam_yubico"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#setup-pam_ssh","text":"Use pam_ssh on the servers to force usage of a passphrase on a private key. The idea behind pam_ssh is that the passphrase of your SSH key serves as your SSH password. Generate your SSH key pair with a passphrase on your local machine . ssh-keygen -f identity Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in identity. Your public key has been saved in identity.pub. The key fingerprint is: SHA256:a2/HNCe28+bpMZ2dIf9bodnBwnmD7stO5sdBOV6teP8 alexys@yazd The key's randomart image is: +---[RSA 2048]----+ | | | | | o| | . ++o| | S BoOo| | . B %+O| | o + %+*=| | . .. @ .*+| | ....%B.E| +----[SHA256]-----+ You then must copy your private key (named identity with no extension) to your servers under the ~/.ssh/login-keys.d/ folder. In your $HOME on the servers, you will get something like this: .ssh/ \u251c\u2500\u2500 known_hosts \u2514\u2500\u2500 login-keys.d \u2514\u2500\u2500 identity Then you can enable the pam_ssh authentication. Gentoo users should enable the pam_ssh USE flag for sys-auth/pambase and re-install. Add this at the beginning of the file /etc/pam.d/ssh auth required pam_ssh.so debug The debug flag can be removed after you tested it correctly.","title":"Setup pam_ssh"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#disable-public-key-authentication","text":"Because it takes precedence over the PAM authentication mechanism, you have to disable OpenSSH PubkeyAuthentication authentication on /etc/ssh/sshd_config: PubkeyAuthentication no Enable PAM authentication on /etc/ssh/sshd_config ChallengeResponseAuthentication no PasswordAuthentication no UsePAM yes","title":"Disable public key authentication"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#test-pam_ssh","text":"Now you should be prompted for your SSH passphrase to login through SSH. \u279c ~ ssh cheetah SSH passphrase:","title":"Test pam_ssh"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#setup-2nd-factor-using-pam_yubico","text":"Now we will make use of our Yubikey security key to add a 2nd factor authentication to login through SSH on our servers. Because the Yubikey is not physically plugged on the server, we cannot use an offline Challenge-Response mechanism, so we will have to use a third party to validate the challenge. Yubico gracefully provide an API for this and the pam_yubico module is meant to use it easily.","title":"Setup 2nd factor using pam_yubico"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#preparing-your-account-using-your-yubikey-on-your-machine","text":"First of all, you need to get your Yubico API key ID from the following URL: https://upgrade.yubico.com/getapikey/ enter your email address tap your Yubikey to generate an OTP in the Yubikey OTP field You will get a Client ID (this you will use) and Secret Key (this you will keep safe). Then you will need to create an authorization mapping file which basically link your account to a Yubikey fingerprint (modhex). This is equivalent to saying \"this Yubikey belongs to this user and can authenticate him\". First, get your modhex: go to https://demo.yubico.com/php-yubico/Modhex_Calculator.php select OTP tap your key click \"convert all formats\" the modhex is listed as Modhex encoded: xxccccxxuuxx Using this modhex, create your mapping file named authorized_yubikeys which will be copied to ~/.yubico/authorized_yubikeys on the servers (replace LOGIN_USERNAME with your actual account login name). LOGIN_USERNAME:xxccccxxuuxx NOTE: this mapping file can be a centralized one (in /etc for example) to handle all the users from a server. See the the authfile option on the doc .","title":"Preparing your account using your Yubikey (on your machine)"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#setting-up-openssh-on-your-servers","text":"You must install pam_yubico on the servers . For Gentoo, it's as simple as: emerge sys-auth/pam_yubico Copy your authentication mapping file to your home under the .yubico folder on all servers . You should get this: .yubico/ \u2514\u2500\u2500 authorized_yubikey Configure pam to use pam_yubico. Add this after the pam_ssh on the file /etc/pam.d/ssh which should look like this now: auth required pam_ssh.so auth required pam_yubico.so id=YOUR_API_ID debug debug_file=/var/log/auth.log The debug and debug_file flags can be removed after you tested it correctly.","title":"Setting up OpenSSH (on your servers)"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#testing-pam_yubico","text":"Now you should be prompted for your SSH passphrase and then for your Yubikey OTP to login through SSH. \u279c ~ ssh cheetah SSH passphrase: YubiKey for `ultrabug':","title":"Testing pam_yubico"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-12/#about-the-yubico-api-dependency","text":"Careful readers will notice that using pam_yubico introduces a strong dependency on the Yubico API availability. If the API becomes unreachable or your internet connection goes down then your servers would be unable to authenticate you! The solution I found to this problem is to instruct pam to ignore the Yubikey authentication when pam_yubico is unable to contact the API. In this case, the module will return a AUTHINFO_UNAVAIL code to PAM which we can act upon using the following syntax. The /etc/pam.d/ssh first lines should be changed to this: auth required pam_ssh.so auth [success=done authinfo_unavail=ignore new_authtok_reqd=done default=die] pam_yubico.so id=YOUR_API_ID debug debug_file=/var/log/auth.log Now you can be sure to be able to use your Yubikey even if the API is down or unreachable ;)","title":"About the Yubico API dependency"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/","text":"In my previous blog post, I demonstrated how to use a Yubikey to add a 2nd factor (2FA) authentication to SSH using pam_ssh and pam_yubico . In this article, I will go further and demonstrate another method using Yubikey's Personal Identity Verification (PIV) capability . This one has the huge advantage to allow a 2nd factor authentication while using the public key authentication mechanism of OpenSSH and thus does not need any kind of setup on the servers. Method 2 - SSH using Yubikey and PIV \u00b6 Yubikey 4 and NEO also act as smartcards supporting the PIV standard which allows you to store a private key on your security key through PKCS#11. This is an amazing feature which is also very good for our use case. Tools installation \u00b6 For this to work, we will need some tools on our local machines to setup our Yubikey correctly. Gentoo users should install those packages: emerge dev-libs/opensc sys-auth/ykpers sys-auth/yubico-piv-tool sys-apps/pcsc-lite app-crypt/ccid sys-apps/pcsc-tools sys-auth/yubikey-personalization-gui Gentoo users should also allow the pcscd service to be hotplugged (started automatically upon key insertion) by modifying their /etc/rc.conf and having: rc_hotplug=\"pcscd\" Yubikey setup \u00b6 The idea behind the Yubikey setup is to generate and store a private key in our Yubikey and to secure it via a PIN code. First, insert your Yubikey and let's change its USB operating mode to OTP+CCID. ykpersonalize -m2 Firmware version 4.3.4 Touch level 783 Program sequence 3 The USB mode will be set to: 0x2 Commit? (y/n) [n]: y Then, we will create a new management key: key=`dd if=/dev/random bs=1 count=24 2>/dev/null | hexdump -v -e '/1 \"%02X\"'` echo $key D59E46FE263DDC052A409C68EB71941D8DD0C5915B7C143A Replace the default management key (if prompted, copy/paste the key printed above): yubico-piv-tool -a set-mgm-key -n $key --key 010203040506070801020304050607080102030405060708 Then change the default PIN code and PUK code of your Yubikey yubico-piv-tool -a change-pin -P 123456 -N yubico-piv-tool -a change-puk -P 12345678 -N Now that your Yubikey is secure, let's proceed with the PCKS#11 certificate generation. You will be prompted for your management key that you generated before. yubico-piv-tool -s 9a -a generate -o public.pem -k Then create a self-signed certificate (only used for libpcks11) and import it in the Yubikey: yubico-piv-tool -a verify-pin -a selfsign-certificate -s 9a -S \"/CN=SSH key/\" -i public.pem -o cert.pem yubico-piv-tool -a import-certificate -s 9a -i cert.pem Here you are! You can now export your public key to use with OpenSSH: ssh-keygen -D opensc-pkcs11.so -e ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWtqI37jwxYMJ9XLq9VwHgJlhZViPVAGIUfMm8SAlfs6cka4Cj570lkoGK04r8JAVJFy/iKfhGpL9N9XuartfIoq6Cg/6Qvg3REupuqs51V2cBaC/gnWIQ7qZqlzBulvcOvzNfHFD/lX42J58+E8tWnYg6GzIsImFZQVpmI6SxNfSmVQIqxIufInrbQaI+pKXntdTQC9wyNK5FAA8TXAdff5ZDnmetsOTVble9Ia5m6gqM7MnxNZ56uDpn+6lCxRZSW+Ln2PDE7sivVcST4qpfwY4P4Lrb3vrjCGODFg4xmGNKXsLi2+uZbs5rW7bg4HFO50kKDucPV1M+rBWA9999 Copy to your servers your SSH public key to your usual ~/.ssh/authorized_keys file in your $HOME. Testing PIV secured SSH \u00b6 Plug-in your Yubikey, and then SSH to your remote server using the opensc-pkcs11 library. You will be prompted for your PIN and then successfully logged in :) ssh -I opensc-pkcs11.so cheetah Enter PIN for 'PIV_II (PIV Card Holder pin)': You can then configure SSH to use it by default for all your hosts in your ~/.ssh/config Host=* PKCS11Provider /usr/lib/opensc-pkcs11.so Using PIV with ssh-agent \u00b6 You can also use ssh-agent to avoid typing your PIN every time. When asked for the passphrase, enter your PIN: ssh-add -s /usr/lib/opensc-pkcs11.so Enter passphrase for PKCS#11: Card added: /usr/lib/opensc-pkcs11.so You can verify that it worked by listing the available keys in your ssh agent: ssh-add -L ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWtqI37jwxYMJ9XLq9VwHgJlhZViPVAGIUfMm8SAlfs6cka4Cj570lkoGK04r8JAVJFy/iKfhGpL9N9XuartfIoq6Cg/6Qvg3REupuqs51V2cBaC/gnWIQ7qZqlzBulvcOvzNfHFD/lX42J58+E8tWnYg6GzIsImFZQVpmI6SxNfSmVQIqxIufInrbQaI+pKXntdTQC9wyNK5FAA8TXAdff5ZDnmetsOTVble9Ia5m6gqM7MnxNZ56uDpn+6lCxRZSW+Ln2PDE7sivVcST4qpfwY4P4Lrb3vrjCGODFg4xmGNKXsLi2+uZbs5rW7bg4HFO50kKDucPV1M+rBWA9999 /usr/lib64/opensc-pkcs11.so Enjoy! \u00b6 Now you have a flexible yet robust way to authenticate your users which you can also extend by adding another type of authentication on your servers using PAM.","title":"Hardening SSH authentication using Yubikey (2/2)"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#method-2-ssh-using-yubikey-and-piv","text":"Yubikey 4 and NEO also act as smartcards supporting the PIV standard which allows you to store a private key on your security key through PKCS#11. This is an amazing feature which is also very good for our use case.","title":"Method 2 - SSH using Yubikey and PIV"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#tools-installation","text":"For this to work, we will need some tools on our local machines to setup our Yubikey correctly. Gentoo users should install those packages: emerge dev-libs/opensc sys-auth/ykpers sys-auth/yubico-piv-tool sys-apps/pcsc-lite app-crypt/ccid sys-apps/pcsc-tools sys-auth/yubikey-personalization-gui Gentoo users should also allow the pcscd service to be hotplugged (started automatically upon key insertion) by modifying their /etc/rc.conf and having: rc_hotplug=\"pcscd\"","title":"Tools installation"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#yubikey-setup","text":"The idea behind the Yubikey setup is to generate and store a private key in our Yubikey and to secure it via a PIN code. First, insert your Yubikey and let's change its USB operating mode to OTP+CCID. ykpersonalize -m2 Firmware version 4.3.4 Touch level 783 Program sequence 3 The USB mode will be set to: 0x2 Commit? (y/n) [n]: y Then, we will create a new management key: key=`dd if=/dev/random bs=1 count=24 2>/dev/null | hexdump -v -e '/1 \"%02X\"'` echo $key D59E46FE263DDC052A409C68EB71941D8DD0C5915B7C143A Replace the default management key (if prompted, copy/paste the key printed above): yubico-piv-tool -a set-mgm-key -n $key --key 010203040506070801020304050607080102030405060708 Then change the default PIN code and PUK code of your Yubikey yubico-piv-tool -a change-pin -P 123456 -N yubico-piv-tool -a change-puk -P 12345678 -N Now that your Yubikey is secure, let's proceed with the PCKS#11 certificate generation. You will be prompted for your management key that you generated before. yubico-piv-tool -s 9a -a generate -o public.pem -k Then create a self-signed certificate (only used for libpcks11) and import it in the Yubikey: yubico-piv-tool -a verify-pin -a selfsign-certificate -s 9a -S \"/CN=SSH key/\" -i public.pem -o cert.pem yubico-piv-tool -a import-certificate -s 9a -i cert.pem Here you are! You can now export your public key to use with OpenSSH: ssh-keygen -D opensc-pkcs11.so -e ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWtqI37jwxYMJ9XLq9VwHgJlhZViPVAGIUfMm8SAlfs6cka4Cj570lkoGK04r8JAVJFy/iKfhGpL9N9XuartfIoq6Cg/6Qvg3REupuqs51V2cBaC/gnWIQ7qZqlzBulvcOvzNfHFD/lX42J58+E8tWnYg6GzIsImFZQVpmI6SxNfSmVQIqxIufInrbQaI+pKXntdTQC9wyNK5FAA8TXAdff5ZDnmetsOTVble9Ia5m6gqM7MnxNZ56uDpn+6lCxRZSW+Ln2PDE7sivVcST4qpfwY4P4Lrb3vrjCGODFg4xmGNKXsLi2+uZbs5rW7bg4HFO50kKDucPV1M+rBWA9999 Copy to your servers your SSH public key to your usual ~/.ssh/authorized_keys file in your $HOME.","title":"Yubikey setup"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#testing-piv-secured-ssh","text":"Plug-in your Yubikey, and then SSH to your remote server using the opensc-pkcs11 library. You will be prompted for your PIN and then successfully logged in :) ssh -I opensc-pkcs11.so cheetah Enter PIN for 'PIV_II (PIV Card Holder pin)': You can then configure SSH to use it by default for all your hosts in your ~/.ssh/config Host=* PKCS11Provider /usr/lib/opensc-pkcs11.so","title":"Testing PIV secured SSH"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#using-piv-with-ssh-agent","text":"You can also use ssh-agent to avoid typing your PIN every time. When asked for the passphrase, enter your PIN: ssh-add -s /usr/lib/opensc-pkcs11.so Enter passphrase for PKCS#11: Card added: /usr/lib/opensc-pkcs11.so You can verify that it worked by listing the available keys in your ssh agent: ssh-add -L ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWtqI37jwxYMJ9XLq9VwHgJlhZViPVAGIUfMm8SAlfs6cka4Cj570lkoGK04r8JAVJFy/iKfhGpL9N9XuartfIoq6Cg/6Qvg3REupuqs51V2cBaC/gnWIQ7qZqlzBulvcOvzNfHFD/lX42J58+E8tWnYg6GzIsImFZQVpmI6SxNfSmVQIqxIufInrbQaI+pKXntdTQC9wyNK5FAA8TXAdff5ZDnmetsOTVble9Ia5m6gqM7MnxNZ56uDpn+6lCxRZSW+Ln2PDE7sivVcST4qpfwY4P4Lrb3vrjCGODFg4xmGNKXsLi2+uZbs5rW7bg4HFO50kKDucPV1M+rBWA9999 /usr/lib64/opensc-pkcs11.so","title":"Using PIV with ssh-agent"},{"location":"Tech%20Blog/2017/2017-05-12-hardening-ssh-authentication-using-yubikey-22/#enjoy","text":"Now you have a flexible yet robust way to authenticate your users which you can also extend by adding another type of authentication on your servers using PAM.","title":"Enjoy!"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/","text":"In my previous blog post, I demonstrated how to use the PIV feature of a Yubikey to add a 2nd factor authentication to SSH . Careful readers such as Grzegorz Kulewski pointed out that using the GPG capability of the Yubikey was also a great, more versatile and more secure option on the table (I love those community insights): GPG keys and subkeys are indeed more flexible and can be used for case-specific operations (signing, encryption, authentication) GPG is more widely used and one could use their Yubikey smartcard for SSH, VPN, HTTP auth and code signing The Yubikey 4 GPG feature supports 4096 bit keys (limited to 2048 for PIV) While I initially looked at the GPG feature, its apparent complexity got me to discard it for my direct use case (SSH). But I couldn't resist the good points of Grzegorz and here I got back into testing it. Thank you again Grzegorz for the excuse you provided ;) So let's get through with the GPG feature of the Yubikey to authenticate our SSH connections . Just like the PIV method, this one has the advantage to allow a 2nd factor authentication while using the public key authentication mechanism of OpenSSH and thus does not need any kind of setup on the servers. Method 3 - SSH using Yubikey and GPG \u00b6 Acknowledgement \u00b6 The first choice you have to make is to decide whether you allow your master key to be stored on the Yubikey or not. This choice will be guided by how you plan to use and industrialize your usage of the GPG based SSH authentication. Consider this to choose whether to store the master key on the Yubikey or not: (con) it will not allow the usage of the same GPG key on multiple Yubikeys (con) if you loose your Yubikey, you will have to revoke your entire GPG key and start from scratch (since the secret key is stored on the Yubikey) (pro) by storing everything on the Yubikey, you won't necessary have to have an offline copy of your master key (and all the process that comes with it) (pro) it is easier to generate and store everything on the key and is then a good starting point for new comers or rare GPG users Because I want to demonstrate and enforce the most straightforward way of using it, I will base this article on generating and storing everything on a Yubikey 4. You can find useful links at the end of the article pointing to reference on how to do it differently. Tools installation \u00b6 For this to work, we will need some tools on our local machine to setup our Yubikey correctly. Gentoo users should install those packages: emerge -av dev-libs/opensc sys-auth/ykpers app-crypt/ccid sys-apps/pcsc-tools app-crypt/gnupg Gentoo users should also allow the pcscd service to be hotplugged (started automatically upon key insertion) by modifying their /etc/rc.conf and having: rc_hotplug=\"pcscd\" Yubikey setup \u00b6 The idea behind the Yubikey setup is to generate and store the GPG keys directly on our Yubikey and to secure them via a PIN code (and an admin PIN code). default PIN code: 123456 default admin PIN code: 12345678 First, insert your Yubikey and let's change its USB operating mode to OTP+U2F+CCID with MODE_FLAG_EJECT flag. ykpersonalize -m86 Firmware version 4.3.4 Touch level 783 Program sequence 3 The USB mode will be set to: 0x86 Commit? (y/n) [n]: y NOTE: if you have an older version of Yubikey (before Sept. 2014), use -m82 instead. Then, we can generate a new GPG key on the Yubikey. Let's open the smartcard for edition. gpg --card-edit --expert Reader ...........: Yubico Yubikey 4 OTP U2F CCID (0005435106) 00 00 Application ID ...: A7560001240102010006054351060000 Version ..........: 2.1 Manufacturer .....: Yubico Serial number ....: 75435106 Name of cardholder: [not set] Language prefs ...: [not set] Sex ..............: unspecified URL of public key : [not set] Login data .......: [not set] Signature PIN ....: forced Key attributes ...: rsa2048 rsa2048 rsa2048 Max. PIN lengths .: 127 127 127 PIN retry counter : 3 0 3 Signature counter : 0 Signature key ....: [none] Encryption key....: [none] Authentication key: [none] General key info..: [none] Then switch to admin mode. gpg/card> admin Admin commands are allowed We can start generating the Signature, Encryption and Authentication keys on the Yubikey. During the process, you will be prompted alternatively for the admin PIN and PIN . gpg/card> generate Make off-card backup of encryption key? (Y/n) Please note that the factory settings of the PINs are PIN = '123456' Admin PIN = '12345678' You should change them using the command --change-pin I advise you say Yes to the off-card backup of the encryption key. Yubikey 4 users can choose a 4096 bits key , let's go for it for every key type. What keysize do you want for the Signature key? (2048) 4096 The card will now be re-configured to generate a key of 4096 bits Note: There is no guarantee that the card supports the requested size. If the key generation does not succeed, please check the documentation of your card to see what sizes are allowed. What keysize do you want for the Encryption key? (2048) 4096 The card will now be re-configured to generate a key of 4096 bits What keysize do you want for the Authentication key? (2048) 4096 The card will now be re-configured to generate a key of 4096 bits Then you're asked for the expiration of your key. I choose 1 year but it's up to you (leave 0 for no expiration). Please specify how long the key should be valid. 0 = key does not expire = key expires in n days w = key expires in n weeks m = key expires in n months y = key expires in n years Key is valid for? (0) 1y Key expires at mer. 15 mai 2018 21:42:42 CEST Is this correct? (y/N) y Finally you give GnuPG details about your user ID and you will be prompted for a passphrase (make it strong). GnuPG needs to construct a user ID to identify your key. Real name: Ultrabug Email address: ultrabug@nospam.com Comment: You selected this USER-ID: \"Ultrabug ultrabug@nospam.com \" Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. If you chose to make an off-card backup of your key, you will also get notified of its location as well the revocation certificate . gpg: Note: backup of card key saved to '/home/ultrabug/.gnupg/sk_8E407636C9C32C38.gpg' gpg: key 22A73AED8E766F01 marked as ultimately trusted gpg: revocation certificate stored as '/home/ultrabug/.gnupg/openpgp-revocs.d/A1580FD98C0486D94C1BE63B22A73AED8E766F01.rev' public and secret key created and signed. Make sure to store that backup in a secure and offline location. You can verify that everything went good and take this chance to note the public key ID. gpg/card> verify Reader ...........: Yubico Yubikey 4 OTP U2F CCID (0001435106) 00 00 Application ID ...: A7560001240102010006054351060000 Version ..........: 2.1 Manufacturer .....: Yubico Serial number ....: 75435106 Name of cardholder: [not set] Language prefs ...: [not set] Sex ..............: unspecified URL of public key : [not set] Login data .......: [not set] Signature PIN ....: forced Key attributes ...: rsa4096 rsa4096 rsa4096 Max. PIN lengths .: 127 127 127 PIN retry counter : 3 0 3 Signature counter : 4 Signature key ....: A158 0FD9 8C04 86D9 4C1B E63B 22A7 3AED 8E76 6F01 created ....: 2017-05-16 20:43:17 Encryption key....: E1B6 7009 907D 1D94 B200 37D7 8E40 7636 C9C3 2C38 created ....: 2017-05-16 20:43:17 Authentication key: AAED AB8E E055 41B2 EFFF 62A4 164F 873A 75D2 AD6B created ....: 2017-05-16 20:43:17 General key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug ultrabug@nospam.com sec> rsa4096/22A73AED8E766F01 created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 ssb> rsa4096/164F873A75D2AD6B created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 ssb> rsa4096/8E407636C9C32C38 created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 You'll find the public key ID on the \"General key info\" line (22A73AED8E766F01): General key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug ultrabug@nospam.com Quit the card edition. gpg/card> quit It is then convenient to upload your public key to a key server, whether public or on your own web server (you can also keep it to be used and imported directly from an USB stick). Export the public key : gpg --armor --export 22A73AED8E766F01 > 22A73AED8E766F01.asc Then upload it to your http server or a public server (needed if you want to be able to easily use the key on multiple machines): # Upload it to your http server scp 22A73AED8E766F01.asc user@server:public_html/static/22A73AED8E766F01.asc OR upload it to a public keyserver \u00b6 gpg --keyserver hkps://hkps.pool.sks-keyservers.net --send-key 22A73AED8E766F01 Now we can finish up the Yubikey setup. Let's edit the card again: gpg --card-edit --expert Reader ...........: Yubico Yubikey 4 OTP U2F CCID (0001435106) 00 00 Application ID ...: A7560001240102010006054351060000 Version ..........: 2.1 Manufacturer .....: Yubico Serial number ....: 75435106 Name of cardholder: [not set] Language prefs ...: [not set] Sex ..............: unspecified URL of public key : [not set] Login data .......: [not set] Signature PIN ....: forced Key attributes ...: rsa4096 rsa4096 rsa4096 Max. PIN lengths .: 127 127 127 PIN retry counter : 3 0 3 Signature counter : 4 Signature key ....: A158 0FD9 8C04 86D9 4C1B E63B 22A7 3AED 8E76 6F01 created ....: 2017-05-16 20:43:17 Encryption key....: E1B6 7009 907D 1D94 B200 37D7 8E40 7636 C9C3 2C38 created ....: 2017-05-16 20:43:17 Authentication key: AAED AB8E E055 41B2 EFFF 62A4 164F 873A 75D2 AD6B created ....: 2017-05-16 20:43:17 General key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug ultrabug@nospam.com sec> rsa4096/22A73AED8E766F01 created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 ssb> rsa4096/164F873A75D2AD6B created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 ssb> rsa4096/8E407636C9C32C38 created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 gpg/card> admin Make sure that the Signature PIN is forced to request that your PIN is entered when your key is used. If it is listed as \"not forced\" , you can enforce it by entering the following command: gpg/card> forcesig It is also good practice to set a few more settings on your key. gpg/card> login Login data (account name): ultrabug gpg/card> lang Language preferences: en gpg/card> name Cardholder's surname: Bug Cardholder's given name: Ultra Now we need to setup the PIN and admin PIN on the card . gpg/card> passwd gpg: OpenPGP card no. A7560001240102010006054351060000 detected 1 - change PIN 2 - unblock PIN 3 - change Admin PIN 4 - set the Reset Code Q - quit Your selection? 1 PIN changed. 1 - change PIN 2 - unblock PIN 3 - change Admin PIN 4 - set the Reset Code Q - quit Your selection? 3 PIN changed. 1 - change PIN 2 - unblock PIN 3 - change Admin PIN 4 - set the Reset Code Q - quit Your selection? Q If you uploaded your public key on your web server or a public server, configure it on the key: gpg/card> url URL to retrieve public key: http://ultrabug.fr/keyserver/22A73AED8E766F01.asc gpg/card> quit Now we can quit the gpg card edition, we're done on the Yubikey side! gpg/card> quit SSH client setup \u00b6 This is the setup on the machine(s) where you will be using the GPG key. The idea is to import your key from the card to your local keyring so you can use it on gpg-agent (and its ssh support). You can skip the fetch/import part below if you generated the key on the same machine than you are using it. You should see it listed when executing gpg -k . Plug-in your Yubikey and load the smartcard. gpg --card-edit --expert Then fetch the key from the URL to import it to your local keyring. gpg/card> fetch Then you're done on this part, exit gpg and update/display& your card status. gpg/card> quit gpg --card-status You can verify the presence of the key in your keyring: gpg -K sec> rsa4096 2017-05-16 [SC] [expires: 2018-05-16] A1580FD98C0486D94C1BE63B22A73AED8E766F01 Card serial no. = 0001 05435106 uid [ultimate] Ultrabug ultrabug@nospam.com ssb> rsa4096 2017-05-16 [A] [expires: 2018-05-16] ssb> rsa4096 2017-05-16 [E] [expires: 2018-05-16] Note the \"Card serial no.\" showing that the key is actually stored on a smartcard. Now we need to configure gpg-agent to enable ssh support , edit your ~/.gnupg/gpg-agent.conf configuration file and make sure that the enable-ssh-support is present: default-cache-ttl 7200 max-cache-ttl 86400 enable-ssh-support Then you will need to update your ~/.bashrc file to automatically start gpg-agent and override ssh-agent's environment variables. Add this at the end of your ~/.bashrc file (or equivalent). # start gpg-agent if it's not running then override SSH authentication socket to use gpg-agent \u00b6 pgrep -l gpg-agent &>/dev/null if [[ \"$?\" != \"0\" ]]; then gpg-agent --daemon &>/dev/null fi SSH_AUTH_SOCK=/run/user/$(id -u)/gnupg/S.gpg-agent.ssh export SSH_AUTH_SOCK To simulate a clean slate, unplug your card then kill any running gpg-agent: killall gpg-agent Then plug back your card and source your ~/.bashrc file: source ~/.bashrc Your GPG key is now listed in you ssh identities! ssh-add -l 4096 SHA256:a4vsJM6Sw1Rt8orvPnI8nvNUwHbRQ67ylnoTxruozK9 cardno:000105435106 (RSA) You will now be able to get the SSH public key hash to copy to your remote servers using: ssh-add -L ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCVDq24Ld/bOzc3yNnY6fF7FNfZnb6wRVdN2xMo1YiA5pz20y+2P1ynq0rb6l/wsSip0Owq4G6fzaJtT1pBUAkEJbuMvZBrbYokb2mZ78iFZyzAkCT+C9YQtvPClFxSnqSL38jBpunZuuiFfejM842dWdMNK3BTcjeTQdTbmY+VsVOw7ppepRh7BWslb52cVpg+bHhB/4H0BUUd/KHZ5sor0z6e1OkqIV8UTiLY2laCCL8iWepZcE6n7MH9KItqzX2I9HVIuLCzhIib35IRp6d3Whhw3hXlit4/irqkIw0g7s9jC8OwybEqXiaeQbmosLromY3X6H8++uLnk7eg9RtCwcWfDq0Qg2uNTEarVGgQ1HXFr8WsjWBIneC8iG09idnwZNbfV3ocY5+l1REZZDobo2BbhSZiS7hKRxzoSiwRvlWh9GuIv8RNCDss9yNFxNUiEIi7lyskSgQl3J8znDXHfNiOAA2X5kVH0s6AQx4hQP9Dl1X2Em4zOz+yJEPFnAvE+XvBys1yuUPq1c3WKMWzongZi8JNu51Yfj7Trm74hoFRn+CREUNpELD9JignxlvkoKAJpWVLdEu1bxJ7jh7kcMQfVEflLbfkEPLV4nZS4sC1FJR88DZwQvOudyS69wLrF3azC1Gc/fTgBiXVVQwuAXE7vozZk+K4hdrGq4u7Gw== cardno:000105435106 This is what ends up in ~/.ssh/authorized_keys on your servers. When connecting to your remote server, you will be prompted for the PIN! Conclusion \u00b6 Using the GPG feature of your Yubikey is very convenient and versatile. Even if it is not that hard after all, it is interesting and fair to note that the PIV method is indeed more simple to implement. When you need to maintain a large number of security keys in an organization and that their usage is limited to SSH, you will be inclined to stick with PIV if 2048 bits keys are acceptable for you. However, for power users and developers, usage of GPG is definitely something you need to consider for its versatility and enhanced security. Useful links \u00b6 You may find those articles useful to setup your GPG key differently and avoid having the secret key tied to your Yubikey. https://www.yubico.com/support/knowledge-base/categories/articles/use-yubikey-openpgp/#generatelocal https://www.esev.com/blog/post/2015-01-pgp-ssh-key-on-yubikey-neo/ https://www.jfry.me/articles/2015/gpg-smartcard/","title":"Hardening SSH authentication using Yubikey (3/2)"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#method-3-ssh-using-yubikey-and-gpg","text":"","title":"Method 3 - SSH using Yubikey and GPG"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#acknowledgement","text":"The first choice you have to make is to decide whether you allow your master key to be stored on the Yubikey or not. This choice will be guided by how you plan to use and industrialize your usage of the GPG based SSH authentication. Consider this to choose whether to store the master key on the Yubikey or not: (con) it will not allow the usage of the same GPG key on multiple Yubikeys (con) if you loose your Yubikey, you will have to revoke your entire GPG key and start from scratch (since the secret key is stored on the Yubikey) (pro) by storing everything on the Yubikey, you won't necessary have to have an offline copy of your master key (and all the process that comes with it) (pro) it is easier to generate and store everything on the key and is then a good starting point for new comers or rare GPG users Because I want to demonstrate and enforce the most straightforward way of using it, I will base this article on generating and storing everything on a Yubikey 4. You can find useful links at the end of the article pointing to reference on how to do it differently.","title":"Acknowledgement"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#tools-installation","text":"For this to work, we will need some tools on our local machine to setup our Yubikey correctly. Gentoo users should install those packages: emerge -av dev-libs/opensc sys-auth/ykpers app-crypt/ccid sys-apps/pcsc-tools app-crypt/gnupg Gentoo users should also allow the pcscd service to be hotplugged (started automatically upon key insertion) by modifying their /etc/rc.conf and having: rc_hotplug=\"pcscd\"","title":"Tools installation"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#yubikey-setup","text":"The idea behind the Yubikey setup is to generate and store the GPG keys directly on our Yubikey and to secure them via a PIN code (and an admin PIN code). default PIN code: 123456 default admin PIN code: 12345678 First, insert your Yubikey and let's change its USB operating mode to OTP+U2F+CCID with MODE_FLAG_EJECT flag. ykpersonalize -m86 Firmware version 4.3.4 Touch level 783 Program sequence 3 The USB mode will be set to: 0x86 Commit? (y/n) [n]: y NOTE: if you have an older version of Yubikey (before Sept. 2014), use -m82 instead. Then, we can generate a new GPG key on the Yubikey. Let's open the smartcard for edition. gpg --card-edit --expert Reader ...........: Yubico Yubikey 4 OTP U2F CCID (0005435106) 00 00 Application ID ...: A7560001240102010006054351060000 Version ..........: 2.1 Manufacturer .....: Yubico Serial number ....: 75435106 Name of cardholder: [not set] Language prefs ...: [not set] Sex ..............: unspecified URL of public key : [not set] Login data .......: [not set] Signature PIN ....: forced Key attributes ...: rsa2048 rsa2048 rsa2048 Max. PIN lengths .: 127 127 127 PIN retry counter : 3 0 3 Signature counter : 0 Signature key ....: [none] Encryption key....: [none] Authentication key: [none] General key info..: [none] Then switch to admin mode. gpg/card> admin Admin commands are allowed We can start generating the Signature, Encryption and Authentication keys on the Yubikey. During the process, you will be prompted alternatively for the admin PIN and PIN . gpg/card> generate Make off-card backup of encryption key? (Y/n) Please note that the factory settings of the PINs are PIN = '123456' Admin PIN = '12345678' You should change them using the command --change-pin I advise you say Yes to the off-card backup of the encryption key. Yubikey 4 users can choose a 4096 bits key , let's go for it for every key type. What keysize do you want for the Signature key? (2048) 4096 The card will now be re-configured to generate a key of 4096 bits Note: There is no guarantee that the card supports the requested size. If the key generation does not succeed, please check the documentation of your card to see what sizes are allowed. What keysize do you want for the Encryption key? (2048) 4096 The card will now be re-configured to generate a key of 4096 bits What keysize do you want for the Authentication key? (2048) 4096 The card will now be re-configured to generate a key of 4096 bits Then you're asked for the expiration of your key. I choose 1 year but it's up to you (leave 0 for no expiration). Please specify how long the key should be valid. 0 = key does not expire = key expires in n days w = key expires in n weeks m = key expires in n months y = key expires in n years Key is valid for? (0) 1y Key expires at mer. 15 mai 2018 21:42:42 CEST Is this correct? (y/N) y Finally you give GnuPG details about your user ID and you will be prompted for a passphrase (make it strong). GnuPG needs to construct a user ID to identify your key. Real name: Ultrabug Email address: ultrabug@nospam.com Comment: You selected this USER-ID: \"Ultrabug ultrabug@nospam.com \" Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. If you chose to make an off-card backup of your key, you will also get notified of its location as well the revocation certificate . gpg: Note: backup of card key saved to '/home/ultrabug/.gnupg/sk_8E407636C9C32C38.gpg' gpg: key 22A73AED8E766F01 marked as ultimately trusted gpg: revocation certificate stored as '/home/ultrabug/.gnupg/openpgp-revocs.d/A1580FD98C0486D94C1BE63B22A73AED8E766F01.rev' public and secret key created and signed. Make sure to store that backup in a secure and offline location. You can verify that everything went good and take this chance to note the public key ID. gpg/card> verify Reader ...........: Yubico Yubikey 4 OTP U2F CCID (0001435106) 00 00 Application ID ...: A7560001240102010006054351060000 Version ..........: 2.1 Manufacturer .....: Yubico Serial number ....: 75435106 Name of cardholder: [not set] Language prefs ...: [not set] Sex ..............: unspecified URL of public key : [not set] Login data .......: [not set] Signature PIN ....: forced Key attributes ...: rsa4096 rsa4096 rsa4096 Max. PIN lengths .: 127 127 127 PIN retry counter : 3 0 3 Signature counter : 4 Signature key ....: A158 0FD9 8C04 86D9 4C1B E63B 22A7 3AED 8E76 6F01 created ....: 2017-05-16 20:43:17 Encryption key....: E1B6 7009 907D 1D94 B200 37D7 8E40 7636 C9C3 2C38 created ....: 2017-05-16 20:43:17 Authentication key: AAED AB8E E055 41B2 EFFF 62A4 164F 873A 75D2 AD6B created ....: 2017-05-16 20:43:17 General key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug ultrabug@nospam.com sec> rsa4096/22A73AED8E766F01 created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 ssb> rsa4096/164F873A75D2AD6B created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 ssb> rsa4096/8E407636C9C32C38 created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 You'll find the public key ID on the \"General key info\" line (22A73AED8E766F01): General key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug ultrabug@nospam.com Quit the card edition. gpg/card> quit It is then convenient to upload your public key to a key server, whether public or on your own web server (you can also keep it to be used and imported directly from an USB stick). Export the public key : gpg --armor --export 22A73AED8E766F01 > 22A73AED8E766F01.asc Then upload it to your http server or a public server (needed if you want to be able to easily use the key on multiple machines): # Upload it to your http server scp 22A73AED8E766F01.asc user@server:public_html/static/22A73AED8E766F01.asc","title":"Yubikey setup"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#or-upload-it-to-a-public-keyserver","text":"gpg --keyserver hkps://hkps.pool.sks-keyservers.net --send-key 22A73AED8E766F01 Now we can finish up the Yubikey setup. Let's edit the card again: gpg --card-edit --expert Reader ...........: Yubico Yubikey 4 OTP U2F CCID (0001435106) 00 00 Application ID ...: A7560001240102010006054351060000 Version ..........: 2.1 Manufacturer .....: Yubico Serial number ....: 75435106 Name of cardholder: [not set] Language prefs ...: [not set] Sex ..............: unspecified URL of public key : [not set] Login data .......: [not set] Signature PIN ....: forced Key attributes ...: rsa4096 rsa4096 rsa4096 Max. PIN lengths .: 127 127 127 PIN retry counter : 3 0 3 Signature counter : 4 Signature key ....: A158 0FD9 8C04 86D9 4C1B E63B 22A7 3AED 8E76 6F01 created ....: 2017-05-16 20:43:17 Encryption key....: E1B6 7009 907D 1D94 B200 37D7 8E40 7636 C9C3 2C38 created ....: 2017-05-16 20:43:17 Authentication key: AAED AB8E E055 41B2 EFFF 62A4 164F 873A 75D2 AD6B created ....: 2017-05-16 20:43:17 General key info..: pub rsa4096/22A73AED8E766F01 2017-05-16 Ultrabug ultrabug@nospam.com sec> rsa4096/22A73AED8E766F01 created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 ssb> rsa4096/164F873A75D2AD6B created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 ssb> rsa4096/8E407636C9C32C38 created: 2017-05-16 expires: 2018-05-16 card-no: 0001 05435106 gpg/card> admin Make sure that the Signature PIN is forced to request that your PIN is entered when your key is used. If it is listed as \"not forced\" , you can enforce it by entering the following command: gpg/card> forcesig It is also good practice to set a few more settings on your key. gpg/card> login Login data (account name): ultrabug gpg/card> lang Language preferences: en gpg/card> name Cardholder's surname: Bug Cardholder's given name: Ultra Now we need to setup the PIN and admin PIN on the card . gpg/card> passwd gpg: OpenPGP card no. A7560001240102010006054351060000 detected 1 - change PIN 2 - unblock PIN 3 - change Admin PIN 4 - set the Reset Code Q - quit Your selection? 1 PIN changed. 1 - change PIN 2 - unblock PIN 3 - change Admin PIN 4 - set the Reset Code Q - quit Your selection? 3 PIN changed. 1 - change PIN 2 - unblock PIN 3 - change Admin PIN 4 - set the Reset Code Q - quit Your selection? Q If you uploaded your public key on your web server or a public server, configure it on the key: gpg/card> url URL to retrieve public key: http://ultrabug.fr/keyserver/22A73AED8E766F01.asc gpg/card> quit Now we can quit the gpg card edition, we're done on the Yubikey side! gpg/card> quit","title":"OR upload it to a public keyserver"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#ssh-client-setup","text":"This is the setup on the machine(s) where you will be using the GPG key. The idea is to import your key from the card to your local keyring so you can use it on gpg-agent (and its ssh support). You can skip the fetch/import part below if you generated the key on the same machine than you are using it. You should see it listed when executing gpg -k . Plug-in your Yubikey and load the smartcard. gpg --card-edit --expert Then fetch the key from the URL to import it to your local keyring. gpg/card> fetch Then you're done on this part, exit gpg and update/display& your card status. gpg/card> quit gpg --card-status You can verify the presence of the key in your keyring: gpg -K sec> rsa4096 2017-05-16 [SC] [expires: 2018-05-16] A1580FD98C0486D94C1BE63B22A73AED8E766F01 Card serial no. = 0001 05435106 uid [ultimate] Ultrabug ultrabug@nospam.com ssb> rsa4096 2017-05-16 [A] [expires: 2018-05-16] ssb> rsa4096 2017-05-16 [E] [expires: 2018-05-16] Note the \"Card serial no.\" showing that the key is actually stored on a smartcard. Now we need to configure gpg-agent to enable ssh support , edit your ~/.gnupg/gpg-agent.conf configuration file and make sure that the enable-ssh-support is present: default-cache-ttl 7200 max-cache-ttl 86400 enable-ssh-support Then you will need to update your ~/.bashrc file to automatically start gpg-agent and override ssh-agent's environment variables. Add this at the end of your ~/.bashrc file (or equivalent). # start gpg-agent if it's not running","title":"SSH client setup"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#then-override-ssh-authentication-socket-to-use-gpg-agent","text":"pgrep -l gpg-agent &>/dev/null if [[ \"$?\" != \"0\" ]]; then gpg-agent --daemon &>/dev/null fi SSH_AUTH_SOCK=/run/user/$(id -u)/gnupg/S.gpg-agent.ssh export SSH_AUTH_SOCK To simulate a clean slate, unplug your card then kill any running gpg-agent: killall gpg-agent Then plug back your card and source your ~/.bashrc file: source ~/.bashrc Your GPG key is now listed in you ssh identities! ssh-add -l 4096 SHA256:a4vsJM6Sw1Rt8orvPnI8nvNUwHbRQ67ylnoTxruozK9 cardno:000105435106 (RSA) You will now be able to get the SSH public key hash to copy to your remote servers using: ssh-add -L ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCVDq24Ld/bOzc3yNnY6fF7FNfZnb6wRVdN2xMo1YiA5pz20y+2P1ynq0rb6l/wsSip0Owq4G6fzaJtT1pBUAkEJbuMvZBrbYokb2mZ78iFZyzAkCT+C9YQtvPClFxSnqSL38jBpunZuuiFfejM842dWdMNK3BTcjeTQdTbmY+VsVOw7ppepRh7BWslb52cVpg+bHhB/4H0BUUd/KHZ5sor0z6e1OkqIV8UTiLY2laCCL8iWepZcE6n7MH9KItqzX2I9HVIuLCzhIib35IRp6d3Whhw3hXlit4/irqkIw0g7s9jC8OwybEqXiaeQbmosLromY3X6H8++uLnk7eg9RtCwcWfDq0Qg2uNTEarVGgQ1HXFr8WsjWBIneC8iG09idnwZNbfV3ocY5+l1REZZDobo2BbhSZiS7hKRxzoSiwRvlWh9GuIv8RNCDss9yNFxNUiEIi7lyskSgQl3J8znDXHfNiOAA2X5kVH0s6AQx4hQP9Dl1X2Em4zOz+yJEPFnAvE+XvBys1yuUPq1c3WKMWzongZi8JNu51Yfj7Trm74hoFRn+CREUNpELD9JignxlvkoKAJpWVLdEu1bxJ7jh7kcMQfVEflLbfkEPLV4nZS4sC1FJR88DZwQvOudyS69wLrF3azC1Gc/fTgBiXVVQwuAXE7vozZk+K4hdrGq4u7Gw== cardno:000105435106 This is what ends up in ~/.ssh/authorized_keys on your servers. When connecting to your remote server, you will be prompted for the PIN!","title":"then override SSH authentication socket to use gpg-agent"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#conclusion","text":"Using the GPG feature of your Yubikey is very convenient and versatile. Even if it is not that hard after all, it is interesting and fair to note that the PIV method is indeed more simple to implement. When you need to maintain a large number of security keys in an organization and that their usage is limited to SSH, you will be inclined to stick with PIV if 2048 bits keys are acceptable for you. However, for power users and developers, usage of GPG is definitely something you need to consider for its versatility and enhanced security.","title":"Conclusion"},{"location":"Tech%20Blog/2017/2017-05-16-hardening-ssh-authentication-using-yubikey-32/#useful-links","text":"You may find those articles useful to setup your GPG key differently and avoid having the secret key tied to your Yubikey. https://www.yubico.com/support/knowledge-base/categories/articles/use-yubikey-openpgp/#generatelocal https://www.esev.com/blog/post/2015-01-pgp-ssh-key-on-yubikey-neo/ https://www.jfry.me/articles/2015/gpg-smartcard/","title":"Useful links"},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/","text":"In our quest to a highly available HiveServer2 , we faced so many problems and a clear lack of documentation when it came to do it with F5 BIG-IP load balancers that I think it's worth a blog post to help around. We are using the Cloudera Hadoop distribution but this applies whatever your distribution. Hive HA configuration \u00b6 This appears to be well documented at a first glance but the HiveServer2 (HS2) documentation vanished at the time of writing. Hive Metastore HA HiveServer2 HA = 404 Anyway, using Cloudera Manager to set up HS2 HA is not hard but there are a few gotchas that I want to highlight and that you will need to be careful with: As for every Keberos based service, make sure you have a dedicated IP for the HiveServer2 Load Balancer URL and that it's reverse DNS is setup properly. Else you will get GSSAPI errors. When running a secure cluster with Kerberos, the HiveServer2 Load Balancer URL is to be used as your connection host (obvious) AND in your Kerberos principal connection string (maybe less obvious). Example beeline connection string before HA: !connect jdbc:hive2://hive-server:10000/default;principal=hive/_HOST@REALM.COM and with HA (notice we changed also the _HOST): !connect jdbc:hive2://ha-hive-fqdn:10000/default;principal=hive/ha-hive-fqdn@REALM.COM We found out the kerberos principal gotcha the hard way... The reason behind this is that the _HOST is basically a macro that will get resolved to the client host name which will then be used to validate the kerberos ticket. When running in load balanced/HA mode , the actual source IP will be replaced by the load balancer's IP (SNAT) and the kerberos reverse DNS lookup will then fail! So if you do not use the HS2 HA URL in the kerberos principal string, you will get Kerberos GSSAPI errors when the load balanding SNAT will be used (see next chapter). This will require you to update all your jobs using HS2 to reflect these changes before load balancing HS2 with F5 BIG-IP. Load balancing HiveServer2 with F5 \u00b6 Our pals at Cloudera have brought a good doc for Impala HA with F5 and they instructed we followed it to set up HS2 HA too because they had nothing better. Kerberos GSSAPI problem \u00b6 When we applied it the first time and tried to switch to using the F5, all our jobs failed because of the kerberos _HOST principal problem mentioned on the previous chapter. This one is not that hard to find out and debug with a google search and explained on Cloudera community forums. We then migrated (again) all our jobs to update the principal connection strings before migrating again to the F5 load balancers. Connection Reset problems \u00b6 After our next migration to F5 load balancers, we had most of our jobs running well and the Kerberos problems vanished but we faced a new problem: some of our jobs failed with Connection Reset errors on HiveServer2: java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Connection reset After some debugging and traffic analysis we found out that the F5 were actually responsible for those connection reset but we struggled to understand why. It turned out that the Protocol Profile set up on the Virtual Server was the root cause of the problem and specifically its idle timeout setting default of 300s : Note the Reset on Timeout setting as well which is responsible for the actual Reset packet sent by the F5 to the client. This could also be proven by the Virtual Server statistics showing an increasing Connection Expires count . The solution is to create a new Protocol Profile based on the fastL4 with a higher Idle Timeout setting and update our Virtual Server to use this profile instead of the default one. It seemed sensible in our case to increase the 5 minutes expiration to 1 day, so let's call our new profile fastL4-24h-idle-timeout: Then change the Hive Virtual Server configuration to use this Protocol Profile: You will see no more expired connections on the Virtual Server statistics! Connection mirroring \u00b6 When creating a Virtual Server, there is a hidden but critical option to enable named Connection Mirroring under the Configuration: Advanced drop-down. Make sure you enable this feature so that your Hive queries and applications can survive a failover of your F5 load balancers. If you do not enable this, you will experience stalled connections and jobs which could take up to multiple hours before failing! Job design consideration \u00b6 We could argue whether or not a default 5 minutes idle timeout is reasonable or not for Hive or any other Hadoop component but it is important to point out that the jobs which were affected also had sub-optimal design pattern in the first place. This also explains why most of our jobs (including also long running ones) were not affected. The affected jobs allowed were Talend jobs where the Hive connection was established at the beginning of the job, used at that time and then the job went on doing other things before using the Hive connection again. When those in between computation took more than 300s, the remaining of the job failed because the initial Hive connection got reset by the F5: This is clearly not a good job design for long processing jobs and you should refrain from doing it. Instead open a connection to Hive when you need it, use it and close it properly. Shall you need it later in your job, open a new connection to Hive and use that one. This will also have the benefit of not keeping open idle connections to Hive itself and favour resources allocation fairness across your jobs. I hope this will be of help to anyone facing these kind of issues.","title":"Load balancing Hadoop Hive with F5 BIG-IP"},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#hive-ha-configuration","text":"This appears to be well documented at a first glance but the HiveServer2 (HS2) documentation vanished at the time of writing. Hive Metastore HA HiveServer2 HA = 404 Anyway, using Cloudera Manager to set up HS2 HA is not hard but there are a few gotchas that I want to highlight and that you will need to be careful with: As for every Keberos based service, make sure you have a dedicated IP for the HiveServer2 Load Balancer URL and that it's reverse DNS is setup properly. Else you will get GSSAPI errors. When running a secure cluster with Kerberos, the HiveServer2 Load Balancer URL is to be used as your connection host (obvious) AND in your Kerberos principal connection string (maybe less obvious). Example beeline connection string before HA: !connect jdbc:hive2://hive-server:10000/default;principal=hive/_HOST@REALM.COM and with HA (notice we changed also the _HOST): !connect jdbc:hive2://ha-hive-fqdn:10000/default;principal=hive/ha-hive-fqdn@REALM.COM We found out the kerberos principal gotcha the hard way... The reason behind this is that the _HOST is basically a macro that will get resolved to the client host name which will then be used to validate the kerberos ticket. When running in load balanced/HA mode , the actual source IP will be replaced by the load balancer's IP (SNAT) and the kerberos reverse DNS lookup will then fail! So if you do not use the HS2 HA URL in the kerberos principal string, you will get Kerberos GSSAPI errors when the load balanding SNAT will be used (see next chapter). This will require you to update all your jobs using HS2 to reflect these changes before load balancing HS2 with F5 BIG-IP.","title":"Hive HA configuration"},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#load-balancing-hiveserver2-with-f5","text":"Our pals at Cloudera have brought a good doc for Impala HA with F5 and they instructed we followed it to set up HS2 HA too because they had nothing better.","title":"Load balancing HiveServer2 with F5"},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#kerberos-gssapi-problem","text":"When we applied it the first time and tried to switch to using the F5, all our jobs failed because of the kerberos _HOST principal problem mentioned on the previous chapter. This one is not that hard to find out and debug with a google search and explained on Cloudera community forums. We then migrated (again) all our jobs to update the principal connection strings before migrating again to the F5 load balancers.","title":"Kerberos GSSAPI problem"},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#connection-reset-problems","text":"After our next migration to F5 load balancers, we had most of our jobs running well and the Kerberos problems vanished but we faced a new problem: some of our jobs failed with Connection Reset errors on HiveServer2: java.sql.SQLException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Connection reset After some debugging and traffic analysis we found out that the F5 were actually responsible for those connection reset but we struggled to understand why. It turned out that the Protocol Profile set up on the Virtual Server was the root cause of the problem and specifically its idle timeout setting default of 300s : Note the Reset on Timeout setting as well which is responsible for the actual Reset packet sent by the F5 to the client. This could also be proven by the Virtual Server statistics showing an increasing Connection Expires count . The solution is to create a new Protocol Profile based on the fastL4 with a higher Idle Timeout setting and update our Virtual Server to use this profile instead of the default one. It seemed sensible in our case to increase the 5 minutes expiration to 1 day, so let's call our new profile fastL4-24h-idle-timeout: Then change the Hive Virtual Server configuration to use this Protocol Profile: You will see no more expired connections on the Virtual Server statistics!","title":"Connection Reset problems"},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#connection-mirroring","text":"When creating a Virtual Server, there is a hidden but critical option to enable named Connection Mirroring under the Configuration: Advanced drop-down. Make sure you enable this feature so that your Hive queries and applications can survive a failover of your F5 load balancers. If you do not enable this, you will experience stalled connections and jobs which could take up to multiple hours before failing!","title":"Connection mirroring"},{"location":"Tech%20Blog/2017/2017-06-02-load-balancing-hadoop-hive-with-f5-big-ip/#job-design-consideration","text":"We could argue whether or not a default 5 minutes idle timeout is reasonable or not for Hive or any other Hadoop component but it is important to point out that the jobs which were affected also had sub-optimal design pattern in the first place. This also explains why most of our jobs (including also long running ones) were not affected. The affected jobs allowed were Talend jobs where the Hive connection was established at the beginning of the job, used at that time and then the job went on doing other things before using the Hive connection again. When those in between computation took more than 300s, the remaining of the job failed because the initial Hive connection got reset by the F5: This is clearly not a good job design for long processing jobs and you should refrain from doing it. Instead open a connection to Hive when you need it, use it and close it properly. Shall you need it later in your job, open a new connection to Hive and use that one. This will also have the benefit of not keeping open idle connections to Hive itself and favour resources allocation fairness across your jobs. I hope this will be of help to anyone facing these kind of issues.","title":"Job design consideration"},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/","text":"I am happy to announce that my work on packaging ScyllaDB for Gentoo Linux is complete! Happy or curious users are very welcome to share their thoughts and ping me to get it into portage (which will very likely happen). Why Scylla? \u00b6 Ever heard of the Cassandra NoSQL database and Java GC/Heap space problems?... if you do, you already get it ;) I will not go into the details as their website does this way better than me but I got interested into Scylla because it fits the Gentoo Linux philosophy very well. If you remember my writing about packaging Rethinkdb for Gentoo Linux , I think that we have a great match with Scylla as well! it is written in C++ so it plays very well with emerge the code quality is so great that building it does not require heavy patching on the ebuild (feels good to be a packager) the code relies on system libs instead of bundling them in the sources (hurrah!) performance tuning is handled by smart scripting and automation , allowing the relationship between the project and the hardware is strong I believe that these are good enough points to go further and that such a project can benefit from a source based distribution like Gentoo Linux. Of course compiling on multiple systems is a challenge for such a database but one does not improve by staying in their comfort zone. Upstream & contributions \u00b6 Packaging is a great excuse to get to know the source code of a project but more importantly the people behind it. So here I got to my first contributions to Scylla to get Gentoo Linux as a detected and supported Linux distribution in the different scripts and tools used to automatically setup the machine it will run upon (fear not, I contributed bash & python, not C++)... Even if I expected to contribute using Github PRs and got to change my habits to a git-patch+mailing list combo , I got warmly welcomed and received positive and genuine interest in the contributions. They got merged quickly and thanks to them you can install and experience Scylla in Gentoo Linux without heavy patching on our side. Special shout out to Pekka, Avi and Vlad for their welcoming and insightful code reviews! I've some open contributions about pushing further on the python code QA side to get the tools to a higher level of coding standards. Seeing how upstream is serious about this I have faith that it will get merged and a good base for other contributions. Last note about reaching them is that I am a bit sad that they're not using IRC freenode to communicate (I instinctively joined #scylla and found myself alone) but they're on Slack (those \"modern folks\") and pretty responsive to the mailing lists ;) Java & Scylla \u00b6 Even if scylla is a rewrite of Cassandra in C++, the project still relies on some external tools used by the Cassandra community which are written in Java. When you install the scylla package on Gentoo, you will see that those two packages are Java based dependencies: app-admin/scylla-tools app-admin/scylla-jmx It pained me a lot to package those (thanks to help of @monsieurp) but they are building and working as expected so this gets the packaging of the whole Scylla project pretty solid. emerge dev-db/scylla \u00b6 The scylla packages are located in the ultrabug overlay for now until I test them even more and ultimately put them in production. Then they'll surely reach the portage tree with the approval of the Gentoo java team for the app-admin/ packages listed above. I provide a live ebuild (scylla-9999 with no keywords) and ebuilds for the latest major version (2.0_rc1 at time of writing). It's as simple as: $ sudo layman -a ultrabug $ sudo emerge -a dev-db/scylla $ sudo emerge --config dev-db/scylla Try it out and tell me what you think , I hope you'll start considering and using this awesome database!","title":"ScyllaDB meets Gentoo Linux"},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/#why-scylla","text":"Ever heard of the Cassandra NoSQL database and Java GC/Heap space problems?... if you do, you already get it ;) I will not go into the details as their website does this way better than me but I got interested into Scylla because it fits the Gentoo Linux philosophy very well. If you remember my writing about packaging Rethinkdb for Gentoo Linux , I think that we have a great match with Scylla as well! it is written in C++ so it plays very well with emerge the code quality is so great that building it does not require heavy patching on the ebuild (feels good to be a packager) the code relies on system libs instead of bundling them in the sources (hurrah!) performance tuning is handled by smart scripting and automation , allowing the relationship between the project and the hardware is strong I believe that these are good enough points to go further and that such a project can benefit from a source based distribution like Gentoo Linux. Of course compiling on multiple systems is a challenge for such a database but one does not improve by staying in their comfort zone.","title":"Why Scylla?"},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/#upstream-contributions","text":"Packaging is a great excuse to get to know the source code of a project but more importantly the people behind it. So here I got to my first contributions to Scylla to get Gentoo Linux as a detected and supported Linux distribution in the different scripts and tools used to automatically setup the machine it will run upon (fear not, I contributed bash & python, not C++)... Even if I expected to contribute using Github PRs and got to change my habits to a git-patch+mailing list combo , I got warmly welcomed and received positive and genuine interest in the contributions. They got merged quickly and thanks to them you can install and experience Scylla in Gentoo Linux without heavy patching on our side. Special shout out to Pekka, Avi and Vlad for their welcoming and insightful code reviews! I've some open contributions about pushing further on the python code QA side to get the tools to a higher level of coding standards. Seeing how upstream is serious about this I have faith that it will get merged and a good base for other contributions. Last note about reaching them is that I am a bit sad that they're not using IRC freenode to communicate (I instinctively joined #scylla and found myself alone) but they're on Slack (those \"modern folks\") and pretty responsive to the mailing lists ;)","title":"Upstream &amp; contributions"},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/#java-scylla","text":"Even if scylla is a rewrite of Cassandra in C++, the project still relies on some external tools used by the Cassandra community which are written in Java. When you install the scylla package on Gentoo, you will see that those two packages are Java based dependencies: app-admin/scylla-tools app-admin/scylla-jmx It pained me a lot to package those (thanks to help of @monsieurp) but they are building and working as expected so this gets the packaging of the whole Scylla project pretty solid.","title":"Java &amp; Scylla"},{"location":"Tech%20Blog/2017/2017-08-08-scylladb-meets-gentoo-linux/#emerge-dev-dbscylla","text":"The scylla packages are located in the ultrabug overlay for now until I test them even more and ultimately put them in production. Then they'll surely reach the portage tree with the approval of the Gentoo java team for the app-admin/ packages listed above. I provide a live ebuild (scylla-9999 with no keywords) and ebuilds for the latest major version (2.0_rc1 at time of writing). It's as simple as: $ sudo layman -a ultrabug $ sudo emerge -a dev-db/scylla $ sudo emerge --config dev-db/scylla Try it out and tell me what you think , I hope you'll start considering and using this awesome database!","title":"emerge dev-db/scylla"},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/","text":"After four months of cool contributions and hard work on normalization and modules' clean up, I'm glad to announce the release of py3status v3.6 ! Milestone 3.6 was mainly focused about existing modules , from their documentation to their usage of the py3 helper to streamline their code base. Other improvements were made about error reporting while some sneaky bugs got fixed along the way. Highlights \u00b6 Not an extensive list, check the changelog . LOTS of modules streamlining (mainly the hard work of @lasers) error reporting improvements py3-cmd performance improvements New modules \u00b6 i3blocks support (yes, py3status can now wrap i3blocks thanks to @tobes) cmus module: to control your cmus music player, by @lasers coin_market module: to display custom cryptocurrency data, by @lasers moc module: to control your moc music player, by @lasers Milestone 3.7 \u00b6 This milestone will give a serious kick into py3status performance. We'll do lots of profiling and drastic work to reduce py3status CPU and memory footprints! For now we've been relying a lot on threads, which is simple to operate but not that CPU/memory friendly. Since i3wm users rightly care for their efficiency we think it's about time we address this kind of points in py3status. Stay tuned, we have some nice ideas in stock :) Thanks contributors! \u00b6 This release is their work, thanks a lot guys! aethelz alexoneill armandg Cypher1 docwalter enguerrand fmorgner guiniol lasers markrileybot maximbaz tablet-mode paradoxisme ritze rixx tobes valdur55 vvoland yabbes","title":"py3status v3.6"},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/#highlights","text":"Not an extensive list, check the changelog . LOTS of modules streamlining (mainly the hard work of @lasers) error reporting improvements py3-cmd performance improvements","title":"Highlights"},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/#new-modules","text":"i3blocks support (yes, py3status can now wrap i3blocks thanks to @tobes) cmus module: to control your cmus music player, by @lasers coin_market module: to display custom cryptocurrency data, by @lasers moc module: to control your moc music player, by @lasers","title":"New modules"},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/#milestone-37","text":"This milestone will give a serious kick into py3status performance. We'll do lots of profiling and drastic work to reduce py3status CPU and memory footprints! For now we've been relying a lot on threads, which is simple to operate but not that CPU/memory friendly. Since i3wm users rightly care for their efficiency we think it's about time we address this kind of points in py3status. Stay tuned, we have some nice ideas in stock :)","title":"Milestone 3.7"},{"location":"Tech%20Blog/2017/2017-08-22-py3status-v3-6/#thanks-contributors","text":"This release is their work, thanks a lot guys! aethelz alexoneill armandg Cypher1 docwalter enguerrand fmorgner guiniol lasers markrileybot maximbaz tablet-mode paradoxisme ritze rixx tobes valdur55 vvoland yabbes","title":"Thanks contributors!"},{"location":"Tech%20Blog/2017/2017-10-13-gentoo-linux-listed-rethinkdbs-website/","text":"The rethinkdb 's website has (finally) been updated and Gentoo Linux is now listed on the installation page! Meanwhile, we have bumped the ebuild to version 2.3.6 with fixes for building on gcc-6 thanks to Peter Levine who kindly proposed a nice PR on github.","title":"Gentoo Linux listed RethinkDB's website"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/","text":"Since I received some positive feedback about my previous DELL XPS 9350 post , I am writing this summary about my recent experience in installing Gentoo Linux on a DELL XPS 13 9365 . This installation notes goals: UEFI boot using Grub Provide you with a complete and working kernel configuration Encrypted disk root partition using LUKS Be able to type your LUKS passphrase to decrypt your partition using your local keyboard layout Grub & UEFI & Luks installation \u00b6 This installation is a fully UEFI one using grub and booting an encrypted root partition (and home partition). I was happy to see that since my previous post, everything got smoother. So even if you can have this installation working using MBR, I don't really see a point in avoiding UEFI now. BIOS configuration \u00b6 Just like with its ancestor, you should: Turn off Secure Boot Set SATA Operation to AHCI Live CD \u00b6 Once again, go for the latest SystemRescueCD (it\u2019s Gentoo based, you won\u2019t be lost) as it\u2019s quite more up to date and supports booting on UEFI. Make it a Live USB for example using unetbootin and the ISO on a vfat formatted USB stick . NVME SSD disk partitioning \u00b6 We\u2019ll obviously use GPT with UEFI . I found that using gdisk was the easiest. The disk itself is found on /dev/nvme0n1 . Here it is the partition table I used : 10Mo UEFI BIOS partition (type EF02) 500Mo UEFI boot partition (type EF00) 2Go Swap partition 475Go Linux root partition The corresponding gdisk commands : # gdisk /dev/nvme0n1 Command: o \u21b5 This option deletes all partitions and creates a new protective MBR. Proceed? (Y/N): y \u21b5 Command: n \u21b5 Partition Number: 1 \u21b5 First sector: \u21b5 Last sector: +10M \u21b5 Hex Code: EF02 \u21b5 Command: n \u21b5 Partition Number: 2 \u21b5 First sector: \u21b5 Last sector: +500M \u21b5 Hex Code: EF00 \u21b5 Command: n \u21b5 Partition Number: 3 \u21b5 First sector: \u21b5 Last sector: +2G \u21b5 Hex Code: 8200 \u21b5 Command: n \u21b5 Partition Number: 4 \u21b5 First sector: \u21b5 Last sector: \u21b5 (for rest of disk) Hex Code: \u21b5 Command: p \u21b5 Disk /dev/nvme0n1: 1000215216 sectors, 476.9 GiB Logical sector size: 512 bytes Disk identifier (GUID): A73970B7-FF37-4BA7-92BE-2EADE6DDB66E Partition table holds up to 128 entries First usable sector is 34, last usable sector is 1000215182 Partitions will be aligned on 2048-sector boundaries Total free space is 2014 sectors (1007.0 KiB) Number Start (sector) End (sector) Size Code Name 1 2048 22527 10.0 MiB EF02 BIOS boot partition 2 22528 1046527 500.0 MiB EF00 EFI System 3 1046528 5240831 2.0 GiB 8200 Linux swap 4 5240832 1000215182 474.4 GiB 8300 Linux filesystem Command: w \u21b5 Do you want to proceed? (Y/N): Y \u21b5 No WiFi on Live CD ? no panic \u00b6 Once again on my (old?) SystemRescueCD stick, the integrated Intel 8265/8275 wifi card is not detected. So I used my old trick with my Android phone connected to my local WiFi as a USB modem which was detected directly by the live CD. get your Android phone connected on your local WiFi (unless you want to use your cellular data) plug in your phone using USB to your XPS on your phone, go to Settings / More / Tethering & portable hotspot enable USB tethering Running ip addr will show the network card enp0s20f0u2 (for me at least), then if no IP address is set on the card, just ask for one : # dhcpcd enp0s20f0u2 You have now access to the internet. Proceed with installation \u00b6 The only thing to prepare is to format the UEFI boot partition as FAT32. Do not worry about the UEFI BIOS partition (/dev/nvme0n1p1), grub will take care of it later. # mkfs.vfat -F 32 /dev/nvme0n1p2 Do not forget to use cryptsetup to encrypt your /dev/nvme0n1p4 partition! In the rest of the article, I'll be using its device mapper representation. # cryptsetup luksFormat -s 512 /dev/nvme0n1p4 cryptsetup luksOpen /dev/nvme0n1p4 root \u00b6 mkfs.ext4 /dev/mapper/root \u00b6 Then follow the Gentoo handbook as usual for the stage3 related next steps. Make sure you mount and bind the following to your /mnt/gentoo LiveCD installation folder (the /sys binding is important for grub UEFI): # mount -t proc none /mnt/gentoo/proc mount -o bind /dev /mnt/gentoo/dev \u00b6 mount -o bind /sys /mnt/gentoo/sys \u00b6 make.conf settings \u00b6 I strongly recommend using at least the following on your /etc/portage/make.conf : GRUB_PLATFORM=\"efi-64\" INPUT_DEVICES=\"evdev synaptics\" VIDEO_CARDS=\"intel i965\" USE=\"bindist cryptsetup\" The GRUB_PLATFORM one is important for later grub setup and the cryptsetup USE flag will help you along the way. fstab for SSD \u00b6 Don't forget to make sure the noatime option is used on your fstab for / and /home. /dev/nvme0n1p2 /boot vfat noauto,noatime 1 2 /dev/nvme0n1p3 none swap sw 0 0 /dev/mapper/root / ext4 noatime 0 1 Kernel configuration and compilation \u00b6 I suggest you use a recent >=sys-kernel/gentoo-sources-4.13.0 along with genkernel . You can download my kernel configuration file (iptables, docker, luks & stuff included) Put the kernel configuration file into the /etc/kernels/ directory (with a training s) Rename the configuration file with the exact version of your kernel Then you'll need to configure genkernel to add luks support, firmware files support and keymap support if your keyboard layout is not QWERTY. In your /etc/genkernel.conf , change the following options: LUKS=\"yes\" FIRMWARE=\"yes\" KEYMAP=\"1\" Then run genkernel all to build your kernel and luks+firmware+keymap aware initramfs. Grub UEFI bootloader with LUKS and custom keymap support \u00b6 Now it's time for the grub magic to happen so you can boot your wonderful Gentoo installation using UEFI and type your password using your favourite keyboard layout. make sure your boot vfat partition is mounted on /boot edit your /etc/default/grub configuration file with the following: GRUB_CMDLINE_LINUX=\"crypt_root=/dev/nvme0n1p4 keymap=fr\" This will allow your initramfs to know it has to read the encrypted root partition from the given partition and to prompt for its password in the given keyboard layout (french here). Now let's install the grub UEFI boot files and setup the UEFI BIOS partition. # grub-install --efi-directory=/boot --target=x86_64-efi /dev/nvme0n1 Installing for x86_64-efi platform. Installation finished. No error reported It should report no error, then we can generate the grub boot config: # grub-mkconfig -o /boot/grub/grub.cfg You're all set! You will get a gentoo UEFI boot option, you can disable the Microsoft Windows one from your BIOS to get straight to the point. Hope this helped!","title":"Gentoo Linux on DELL XPS 13 9365 and 9360"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#grub-uefi-luks-installation","text":"This installation is a fully UEFI one using grub and booting an encrypted root partition (and home partition). I was happy to see that since my previous post, everything got smoother. So even if you can have this installation working using MBR, I don't really see a point in avoiding UEFI now.","title":"Grub &amp; UEFI &amp; Luks installation"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#bios-configuration","text":"Just like with its ancestor, you should: Turn off Secure Boot Set SATA Operation to AHCI","title":"BIOS configuration"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#live-cd","text":"Once again, go for the latest SystemRescueCD (it\u2019s Gentoo based, you won\u2019t be lost) as it\u2019s quite more up to date and supports booting on UEFI. Make it a Live USB for example using unetbootin and the ISO on a vfat formatted USB stick .","title":"Live CD"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#nvme-ssd-disk-partitioning","text":"We\u2019ll obviously use GPT with UEFI . I found that using gdisk was the easiest. The disk itself is found on /dev/nvme0n1 . Here it is the partition table I used : 10Mo UEFI BIOS partition (type EF02) 500Mo UEFI boot partition (type EF00) 2Go Swap partition 475Go Linux root partition The corresponding gdisk commands : # gdisk /dev/nvme0n1 Command: o \u21b5 This option deletes all partitions and creates a new protective MBR. Proceed? (Y/N): y \u21b5 Command: n \u21b5 Partition Number: 1 \u21b5 First sector: \u21b5 Last sector: +10M \u21b5 Hex Code: EF02 \u21b5 Command: n \u21b5 Partition Number: 2 \u21b5 First sector: \u21b5 Last sector: +500M \u21b5 Hex Code: EF00 \u21b5 Command: n \u21b5 Partition Number: 3 \u21b5 First sector: \u21b5 Last sector: +2G \u21b5 Hex Code: 8200 \u21b5 Command: n \u21b5 Partition Number: 4 \u21b5 First sector: \u21b5 Last sector: \u21b5 (for rest of disk) Hex Code: \u21b5 Command: p \u21b5 Disk /dev/nvme0n1: 1000215216 sectors, 476.9 GiB Logical sector size: 512 bytes Disk identifier (GUID): A73970B7-FF37-4BA7-92BE-2EADE6DDB66E Partition table holds up to 128 entries First usable sector is 34, last usable sector is 1000215182 Partitions will be aligned on 2048-sector boundaries Total free space is 2014 sectors (1007.0 KiB) Number Start (sector) End (sector) Size Code Name 1 2048 22527 10.0 MiB EF02 BIOS boot partition 2 22528 1046527 500.0 MiB EF00 EFI System 3 1046528 5240831 2.0 GiB 8200 Linux swap 4 5240832 1000215182 474.4 GiB 8300 Linux filesystem Command: w \u21b5 Do you want to proceed? (Y/N): Y \u21b5","title":"NVME SSD disk partitioning"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#no-wifi-on-live-cd-no-panic","text":"Once again on my (old?) SystemRescueCD stick, the integrated Intel 8265/8275 wifi card is not detected. So I used my old trick with my Android phone connected to my local WiFi as a USB modem which was detected directly by the live CD. get your Android phone connected on your local WiFi (unless you want to use your cellular data) plug in your phone using USB to your XPS on your phone, go to Settings / More / Tethering & portable hotspot enable USB tethering Running ip addr will show the network card enp0s20f0u2 (for me at least), then if no IP address is set on the card, just ask for one : # dhcpcd enp0s20f0u2 You have now access to the internet.","title":"No WiFi on Live CD ? no panic"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#proceed-with-installation","text":"The only thing to prepare is to format the UEFI boot partition as FAT32. Do not worry about the UEFI BIOS partition (/dev/nvme0n1p1), grub will take care of it later. # mkfs.vfat -F 32 /dev/nvme0n1p2 Do not forget to use cryptsetup to encrypt your /dev/nvme0n1p4 partition! In the rest of the article, I'll be using its device mapper representation. # cryptsetup luksFormat -s 512 /dev/nvme0n1p4","title":"Proceed with installation"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#cryptsetup-luksopen-devnvme0n1p4-root","text":"","title":"cryptsetup luksOpen /dev/nvme0n1p4 root"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#mkfsext4-devmapperroot","text":"Then follow the Gentoo handbook as usual for the stage3 related next steps. Make sure you mount and bind the following to your /mnt/gentoo LiveCD installation folder (the /sys binding is important for grub UEFI): # mount -t proc none /mnt/gentoo/proc","title":"mkfs.ext4 /dev/mapper/root"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#mount-o-bind-dev-mntgentoodev","text":"","title":"mount -o bind /dev /mnt/gentoo/dev"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#mount-o-bind-sys-mntgentoosys","text":"","title":"mount -o bind /sys /mnt/gentoo/sys"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#makeconf-settings","text":"I strongly recommend using at least the following on your /etc/portage/make.conf : GRUB_PLATFORM=\"efi-64\" INPUT_DEVICES=\"evdev synaptics\" VIDEO_CARDS=\"intel i965\" USE=\"bindist cryptsetup\" The GRUB_PLATFORM one is important for later grub setup and the cryptsetup USE flag will help you along the way.","title":"make.conf settings"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#fstab-for-ssd","text":"Don't forget to make sure the noatime option is used on your fstab for / and /home. /dev/nvme0n1p2 /boot vfat noauto,noatime 1 2 /dev/nvme0n1p3 none swap sw 0 0 /dev/mapper/root / ext4 noatime 0 1","title":"fstab for SSD"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#kernel-configuration-and-compilation","text":"I suggest you use a recent >=sys-kernel/gentoo-sources-4.13.0 along with genkernel . You can download my kernel configuration file (iptables, docker, luks & stuff included) Put the kernel configuration file into the /etc/kernels/ directory (with a training s) Rename the configuration file with the exact version of your kernel Then you'll need to configure genkernel to add luks support, firmware files support and keymap support if your keyboard layout is not QWERTY. In your /etc/genkernel.conf , change the following options: LUKS=\"yes\" FIRMWARE=\"yes\" KEYMAP=\"1\" Then run genkernel all to build your kernel and luks+firmware+keymap aware initramfs.","title":"Kernel configuration and compilation"},{"location":"Tech%20Blog/2017/2017-11-02-gentoo-linux-on-dell-xps-13-9365-and-9360/#grub-uefi-bootloader-with-luks-and-custom-keymap-support","text":"Now it's time for the grub magic to happen so you can boot your wonderful Gentoo installation using UEFI and type your password using your favourite keyboard layout. make sure your boot vfat partition is mounted on /boot edit your /etc/default/grub configuration file with the following: GRUB_CMDLINE_LINUX=\"crypt_root=/dev/nvme0n1p4 keymap=fr\" This will allow your initramfs to know it has to read the encrypted root partition from the given partition and to prompt for its password in the given keyboard layout (french here). Now let's install the grub UEFI boot files and setup the UEFI BIOS partition. # grub-install --efi-directory=/boot --target=x86_64-efi /dev/nvme0n1 Installing for x86_64-efi platform. Installation finished. No error reported It should report no error, then we can generate the grub boot config: # grub-mkconfig -o /boot/grub/grub.cfg You're all set! You will get a gentoo UEFI boot option, you can disable the Microsoft Windows one from your BIOS to get straight to the point. Hope this helped!","title":"Grub UEFI bootloader with LUKS and custom keymap support"},{"location":"Tech%20Blog/2017/2017-12-12-py3status-v3-7/","text":"This important release has been long awaited as it focused on improving overall performance of py3status as well as dramatically decreasing its memory footprint ! I want once again to salute the impressive work of @ lasers, our amazing contributors from the USA who has become top one contributor of 2017 in term of commits and PRs. Thanks to him, this release brings a whole batch of improvements and QA clean ups on various modules. I encourage you to go through the changelog to see everything. Highlights \u00b6 Deep rework of the usage and scheduling of threads to run modules has been done by @tobes. now py3status does not keep one thread per module running permanently but instead uses a queue to spawn a thread to execute the module only when its cache expires this new scheduling and usage of threads allows py3status to run under asynchronous event loops and gevent will be supported on the upcoming 3.8 memory footprint of py3status got largely reduced thanks to the threads modifications and thanks to a nice hunt on ever growing and useless variables modules error reporting is now more detailed Milestone 3.8 \u00b6 The next release will bring some awesome new features such as gevent support , environment variable support in config file and per module persistent data storage as well as new modules! Thanks contributors! \u00b6 This release is their work, thanks a lot guys! JohnAZoidberg lasers maximbaz pcewing tobes","title":"py3status v3.7"},{"location":"Tech%20Blog/2017/2017-12-12-py3status-v3-7/#highlights","text":"Deep rework of the usage and scheduling of threads to run modules has been done by @tobes. now py3status does not keep one thread per module running permanently but instead uses a queue to spawn a thread to execute the module only when its cache expires this new scheduling and usage of threads allows py3status to run under asynchronous event loops and gevent will be supported on the upcoming 3.8 memory footprint of py3status got largely reduced thanks to the threads modifications and thanks to a nice hunt on ever growing and useless variables modules error reporting is now more detailed","title":"Highlights"},{"location":"Tech%20Blog/2017/2017-12-12-py3status-v3-7/#milestone-38","text":"The next release will bring some awesome new features such as gevent support , environment variable support in config file and per module persistent data storage as well as new modules!","title":"Milestone 3.8"},{"location":"Tech%20Blog/2017/2017-12-12-py3status-v3-7/#thanks-contributors","text":"This release is their work, thanks a lot guys! JohnAZoidberg lasers maximbaz pcewing tobes","title":"Thanks contributors!"},{"location":"Tech%20Blog/2017/2017-12-22-talks-page/","text":"I finally decided to put up a page on this website referencing the talks I've been giving over the last few years. You'll see that I'm quite obsessed with fault tolerance and scalability designs... I guess I can't deny it any more :)","title":"Talks page"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/","text":"I have recently been conducting a quite deep evaluation of ScyllaDB to find out if we could benefit from this database in some of our intensive and latency critical data streams and jobs. I'll try to share this great experience within two posts: The first one (you're reading) will walk through how to prepare yourself for a successful Proof Of Concept based evaluation with the help of the ScyllaDB team. The second post will cover the technical aspects and details of the POC I've conducted with the various approaches I've followed to find the most optimal solution. But let's start with how I got into this in the first place... Selecting ScyllaDB \u00b6 I got interested in ScyllaDB because of its philosophy and engagement and I quickly got into it by being a modest contributor and its Gentoo Linux packager (not in portage yet). Of course, I didn't pick an interest in that technology by chance: We've been using MongoDB in (mass) production at work for a very very long time now. I can easily say we were early MongoDB adopters. But there's no wisdom in saying that MongoDB is not suited for every use case and the Hadoop stack has come very strong in our data centers since then, with a predominance of Hive for the heavy duty and data hungry workflows. One thing I was never satisfied with MongoDB was its primary/secondary architecture which makes you lose write throughput and is even more horrible when you want to set up what they call a \"cluster\" which is in fact some mediocre abstraction they add on top of replica-sets. To say the least, it is inefficient and cumbersome to operate and maintain. So I obviously had Cassandra on my radar for a long time, but I was pushed back by its Java stack, heap size and silly tuning... Also, coming from the versatile MongoDB world, Cassandra's CQL limitations looked dreadful at that time... The day I found myself on ScyllaDB's webpage and read their promises, I was sure to be challenging our current use cases with this interesting sea monster. Setting up a POC with the people at ScyllaDB \u00b6 Through my contributions around my packaging of ScyllaDB for Gentoo Linux, I got to know a bit about the people behind the technology. They got interested in why I was packaging this in the first place and when I explained my not-so-secret goal of challenging our production data workflows using Scylla, they told me that they would love to help! I was a bit surprised at first because this was the first time I ever saw a real engagement of the people behind a technology into someone else's POC. Their pitch is simple, they will help (for free) anyone conducting a serious POC to make sure that the outcome and the comprehension behind it is the best possible. It is a very mature reasoning to me because it is easy to make false assumptions and conclude badly when testing a technology you don't know, even more when your use cases are complex and your expectations are very high like us. Still, to my current knowledge, they're the only ones in the data industry to have this kind of logic in place since the start. So I wanted to take this chance to thank them again for this! The POC includes: no bullshit , simple tech-to-tech relationship a private slack channel with multiple ScyllaDB's engineers video calls to introduce ourselves and discuss our progress later on help in schema design and logic fast answers to every question you have detailed explanations on the internals of the technology hardware sizing help and validation funny comments and French jokes (ok, not suitable for everyone) Lessons for a successful POC \u00b6 As I said before, you've got to be serious in your approach to make sure your POC will be efficient and will lead to an unbiased and fair conclusion. This is a list of the main things I consider important to have prepared before you start. Have some background \u00b6 Make sure to read some literature to have the key concepts and words in mind before you go. It is even more important if like me you do not come from the Cassandra world. I found that the Cassandra: The Definitive Guide book at O'Reilly is a great read. Also, make sure to go around ScyllaDB's documentation . Work with a shared reference document \u00b6 Make sure you share with the ScyllaDB guys a clear and detailed document explaining exactly what you're trying to achieve and how you are doing it today (if you plan on migrating like we did). I made a google document for this because it felt the easiest. This document will be updated as you go and will serve as a reference for everyone participating in the POC. This shared reference document is very important, so if you don't know how to construct it or what to put in it, here is how I structured it: Who's participating at photo + name + speciality Who's participating at ScyllaDB POC hardware if you have your own bare metal machines you want to run your POC on, give every detail about their number and specs if not, explain how you plan to setup and run your scylla cluster Reference infrastructure give every details on the technologies and on the hardware of the servers that are currently responsible for running your workflows explain your clusters and their speciality Use case #1 : Context give context about your use case by explaining it without tech words, think from the business / user point of view Current implementations that's where you get technical technology names and where they come into play in your current stack insightful data volumes and cardinality current schema models Workload related to this use case queries per second per data source / type peek hours or no peek hours? criticality Questions we want to answer to remember, the NoSQL world is lead by query-based-modeling schema design logic, cassandra/scylla is no exception write down the real questions you want your data model(s) to be able to answer to group them and rate them by importance Validated models this one comes during the POC when you have settled on the data models write them down, explain them or relate them to the questions they answer to copy/paste some code showcasing how to work with them Code examples depending on the complexity of your use case, you may have multiple constraints or ways to compare your current implementation with your POC try to explain what you test and copy/paste the best code you came up with to validate each point Have monitoring in place \u00b6 ScyllaDB provides a monitoring platform based on Docker, Prometheus and Grafana that is efficient and simple to set up. I strongly recommend that you set it up, as it provides valuable insights almost immediately, and on an ongoing basis. Also you should strive to give access to your monitoring to the ScyllaDB guys, if that's possible for you. They will provide with a fixed IP which you can authorize to access your grafana dashboards so they can have a look at the performances of your POC cluster as you go. You'll learn a great deal about ScyllaDB's internals by sharing with them. Know when to stop \u00b6 The main trap in a POC is to work without boundaries. Since you're looking for the best of what you can get out of a technology, you'll get tempted to refine indefinitely. So this is good to have at least an idea on the minimal figures you'd like to reach to get satisfied with your tests. You can always push a bit further but not for too long! Plan some high availability tests \u00b6 Even if you first came to ScyllaDB for its speed, make sure to test its high availability capabilities based on your experience. Most importantly, make sure you test it within your code base and guidelines. How will your code react and handle a failure, partial and total? I was very surprised and saddened to discover so little literature on the subject in the Cassandra community. POC != production \u00b6 Remember that even when everything is right on paper, production load will have its share of surprises and unexpected behaviours. So keep a good deal of flexibility in your design and your capacity planning to absorb them. Make time \u00b6 Our POC lasted almost 5 months instead of estimated 3, mostly because of my agenda's unwillingness to cooperate... As you can imagine this interruption was not always optimal, for either me or the ScyllaDB guys, but they were kind not to complain about it. So depending on how thorough you plan to be, make sure you make time matching your degree of demands. The reference document is also helpful to get back to speed. Feedback for the ScyllaDB guys \u00b6 Here are the main points I noted during the POC that the guys from ScyllaDB could improve on. They are subjective of course but it's important to give feedback so here it goes. I'm fully aware that everyone is trying to improve, so I'm not pointing any fingers at all. I shared those comments already with them and they acknowledged them very well. More video meetings on start \u00b6 When starting the POC, try to have some pre-scheduled video meetings to set it right in motion. This will provide a good pace as well as making sure that everyone is on the same page. Make a POC kick starter questionnaire \u00b6 Having a minimal plan to follow with some key points to set up just like the ones I explained before would help. Maybe also a minimal questionnaire to make sure that the key aspects and figures have been given some thought since the start. This will raise awareness on the real answers the POC aims to answer. To put it simpler: some minimal formalism helps to check out the key aspects and questions. Develop a higher client driver expertise \u00b6 This one was the most painful to me, and is likely to be painful for anyone who, like me, is not coming from the Cassandra world. Finding good and strong code examples and guidelines on the client side was hard and that's where I felt the most alone. This was not pleasant because a technology is definitely validated through its usage which means on the client side. Most of my tests were using python and the python-cassandra driver so I had tons of questions about it with no sticking answers. Same thing went with the spark-cassandra-connector when using scala where some key configuration options (not documented) can change the shape of your results drastically (more details on the next post). High Availability guidelines and examples \u00b6 This one still strikes me as the most awkward on the Cassandra community. I literally struggled with finding clear and detailed explanations about how to handle failure more or less gracefully with the python driver (or any other driver). This is kind of a disappointment to me for a technology that position itself as highly available ... I'll get into more details about it on the next post. A clearer sizing documentation \u00b6 Even if there will never be a magic formula, there are some rules of thumb that exist for sizing your hardware for ScyllaDB. They should be written down more clearly in a maybe dedicated documentation ( sizing guide is labeled as admin guide at time of writing ). Some examples: RAM per core ? what is a core ? relation to shard ? Disk / RAM maximal ratio ? Multiple SSDs vs one NMVe ? Hardware RAID vs software RAID ? need a RAID controller at all ? Maybe even provide a bare metal complete example from two different vendors such as DELL and HP. What's next? \u00b6 In the next post, I'll get into more details on the POC itself and the technical learnings we found along the way. This will lead to the final conclusion and the next move we engaged ourselves with.","title":"Evaluating ScyllaDB for production 1/2"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#selecting-scylladb","text":"I got interested in ScyllaDB because of its philosophy and engagement and I quickly got into it by being a modest contributor and its Gentoo Linux packager (not in portage yet). Of course, I didn't pick an interest in that technology by chance: We've been using MongoDB in (mass) production at work for a very very long time now. I can easily say we were early MongoDB adopters. But there's no wisdom in saying that MongoDB is not suited for every use case and the Hadoop stack has come very strong in our data centers since then, with a predominance of Hive for the heavy duty and data hungry workflows. One thing I was never satisfied with MongoDB was its primary/secondary architecture which makes you lose write throughput and is even more horrible when you want to set up what they call a \"cluster\" which is in fact some mediocre abstraction they add on top of replica-sets. To say the least, it is inefficient and cumbersome to operate and maintain. So I obviously had Cassandra on my radar for a long time, but I was pushed back by its Java stack, heap size and silly tuning... Also, coming from the versatile MongoDB world, Cassandra's CQL limitations looked dreadful at that time... The day I found myself on ScyllaDB's webpage and read their promises, I was sure to be challenging our current use cases with this interesting sea monster.","title":"Selecting ScyllaDB"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#setting-up-a-poc-with-the-people-at-scylladb","text":"Through my contributions around my packaging of ScyllaDB for Gentoo Linux, I got to know a bit about the people behind the technology. They got interested in why I was packaging this in the first place and when I explained my not-so-secret goal of challenging our production data workflows using Scylla, they told me that they would love to help! I was a bit surprised at first because this was the first time I ever saw a real engagement of the people behind a technology into someone else's POC. Their pitch is simple, they will help (for free) anyone conducting a serious POC to make sure that the outcome and the comprehension behind it is the best possible. It is a very mature reasoning to me because it is easy to make false assumptions and conclude badly when testing a technology you don't know, even more when your use cases are complex and your expectations are very high like us. Still, to my current knowledge, they're the only ones in the data industry to have this kind of logic in place since the start. So I wanted to take this chance to thank them again for this! The POC includes: no bullshit , simple tech-to-tech relationship a private slack channel with multiple ScyllaDB's engineers video calls to introduce ourselves and discuss our progress later on help in schema design and logic fast answers to every question you have detailed explanations on the internals of the technology hardware sizing help and validation funny comments and French jokes (ok, not suitable for everyone)","title":"Setting up a POC with the people at ScyllaDB"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#lessons-for-a-successful-poc","text":"As I said before, you've got to be serious in your approach to make sure your POC will be efficient and will lead to an unbiased and fair conclusion. This is a list of the main things I consider important to have prepared before you start.","title":"Lessons for a successful POC"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#have-some-background","text":"Make sure to read some literature to have the key concepts and words in mind before you go. It is even more important if like me you do not come from the Cassandra world. I found that the Cassandra: The Definitive Guide book at O'Reilly is a great read. Also, make sure to go around ScyllaDB's documentation .","title":"Have some background"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#work-with-a-shared-reference-document","text":"Make sure you share with the ScyllaDB guys a clear and detailed document explaining exactly what you're trying to achieve and how you are doing it today (if you plan on migrating like we did). I made a google document for this because it felt the easiest. This document will be updated as you go and will serve as a reference for everyone participating in the POC. This shared reference document is very important, so if you don't know how to construct it or what to put in it, here is how I structured it: Who's participating at photo + name + speciality Who's participating at ScyllaDB POC hardware if you have your own bare metal machines you want to run your POC on, give every detail about their number and specs if not, explain how you plan to setup and run your scylla cluster Reference infrastructure give every details on the technologies and on the hardware of the servers that are currently responsible for running your workflows explain your clusters and their speciality Use case #1 : Context give context about your use case by explaining it without tech words, think from the business / user point of view Current implementations that's where you get technical technology names and where they come into play in your current stack insightful data volumes and cardinality current schema models Workload related to this use case queries per second per data source / type peek hours or no peek hours? criticality Questions we want to answer to remember, the NoSQL world is lead by query-based-modeling schema design logic, cassandra/scylla is no exception write down the real questions you want your data model(s) to be able to answer to group them and rate them by importance Validated models this one comes during the POC when you have settled on the data models write them down, explain them or relate them to the questions they answer to copy/paste some code showcasing how to work with them Code examples depending on the complexity of your use case, you may have multiple constraints or ways to compare your current implementation with your POC try to explain what you test and copy/paste the best code you came up with to validate each point","title":"Work with a shared reference document"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#have-monitoring-in-place","text":"ScyllaDB provides a monitoring platform based on Docker, Prometheus and Grafana that is efficient and simple to set up. I strongly recommend that you set it up, as it provides valuable insights almost immediately, and on an ongoing basis. Also you should strive to give access to your monitoring to the ScyllaDB guys, if that's possible for you. They will provide with a fixed IP which you can authorize to access your grafana dashboards so they can have a look at the performances of your POC cluster as you go. You'll learn a great deal about ScyllaDB's internals by sharing with them.","title":"Have monitoring in place"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#know-when-to-stop","text":"The main trap in a POC is to work without boundaries. Since you're looking for the best of what you can get out of a technology, you'll get tempted to refine indefinitely. So this is good to have at least an idea on the minimal figures you'd like to reach to get satisfied with your tests. You can always push a bit further but not for too long!","title":"Know when to stop"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#plan-some-high-availability-tests","text":"Even if you first came to ScyllaDB for its speed, make sure to test its high availability capabilities based on your experience. Most importantly, make sure you test it within your code base and guidelines. How will your code react and handle a failure, partial and total? I was very surprised and saddened to discover so little literature on the subject in the Cassandra community.","title":"Plan some high availability tests"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#poc-production","text":"Remember that even when everything is right on paper, production load will have its share of surprises and unexpected behaviours. So keep a good deal of flexibility in your design and your capacity planning to absorb them.","title":"POC != production"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#make-time","text":"Our POC lasted almost 5 months instead of estimated 3, mostly because of my agenda's unwillingness to cooperate... As you can imagine this interruption was not always optimal, for either me or the ScyllaDB guys, but they were kind not to complain about it. So depending on how thorough you plan to be, make sure you make time matching your degree of demands. The reference document is also helpful to get back to speed.","title":"Make time"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#feedback-for-the-scylladb-guys","text":"Here are the main points I noted during the POC that the guys from ScyllaDB could improve on. They are subjective of course but it's important to give feedback so here it goes. I'm fully aware that everyone is trying to improve, so I'm not pointing any fingers at all. I shared those comments already with them and they acknowledged them very well.","title":"Feedback for the ScyllaDB guys"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#more-video-meetings-on-start","text":"When starting the POC, try to have some pre-scheduled video meetings to set it right in motion. This will provide a good pace as well as making sure that everyone is on the same page.","title":"More video meetings on start"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#make-a-poc-kick-starter-questionnaire","text":"Having a minimal plan to follow with some key points to set up just like the ones I explained before would help. Maybe also a minimal questionnaire to make sure that the key aspects and figures have been given some thought since the start. This will raise awareness on the real answers the POC aims to answer. To put it simpler: some minimal formalism helps to check out the key aspects and questions.","title":"Make a POC kick starter questionnaire"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#develop-a-higher-client-driver-expertise","text":"This one was the most painful to me, and is likely to be painful for anyone who, like me, is not coming from the Cassandra world. Finding good and strong code examples and guidelines on the client side was hard and that's where I felt the most alone. This was not pleasant because a technology is definitely validated through its usage which means on the client side. Most of my tests were using python and the python-cassandra driver so I had tons of questions about it with no sticking answers. Same thing went with the spark-cassandra-connector when using scala where some key configuration options (not documented) can change the shape of your results drastically (more details on the next post).","title":"Develop a higher client driver expertise"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#high-availability-guidelines-and-examples","text":"This one still strikes me as the most awkward on the Cassandra community. I literally struggled with finding clear and detailed explanations about how to handle failure more or less gracefully with the python driver (or any other driver). This is kind of a disappointment to me for a technology that position itself as highly available ... I'll get into more details about it on the next post.","title":"High Availability guidelines and examples"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#a-clearer-sizing-documentation","text":"Even if there will never be a magic formula, there are some rules of thumb that exist for sizing your hardware for ScyllaDB. They should be written down more clearly in a maybe dedicated documentation ( sizing guide is labeled as admin guide at time of writing ). Some examples: RAM per core ? what is a core ? relation to shard ? Disk / RAM maximal ratio ? Multiple SSDs vs one NMVe ? Hardware RAID vs software RAID ? need a RAID controller at all ? Maybe even provide a bare metal complete example from two different vendors such as DELL and HP.","title":"A clearer sizing documentation"},{"location":"Tech%20Blog/2018/2018-01-23-evaluating-scylladb-for-production-1-2/#whats-next","text":"In the next post, I'll get into more details on the POC itself and the technical learnings we found along the way. This will lead to the final conclusion and the next move we engaged ourselves with.","title":"What's next?"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/","text":"In my previous blog post, I shared 7 lessons on our experience in evaluating Scylla for production. Those lessons were focused on the setup and execution of the POC and I promised a more technical blog post with technical details and lessons learned from the POC, here it is! Before you read on, be mindful that our POC was set up to test workloads and workflows, not to benchmark technologies. So even if the Scylla figures are great, they have not been the main drivers of the actual conclusion of the POC. Business context \u00b6 As a data driven company working in the Marketing and Advertising industry, we help our clients make sense of multiple sources of data to build and improve their relationship with their customers and prospects. Dealing with multiple sources of data is nothing new but their volume has dramatically changed during the past decade. I will spare you with the Big-Data-means-nothing term and the technical challenges that comes with it as you already heard enough of it. Still, it is clear that our line of business is tied to our capacity at mixing and correlating a massive amount of different types of events (data sources/types) coming from various sources which all have their own identifiers (think primary keys): Web navigation tracking: identifier is a cookie that's tied to the tracking domain (we have our own) CRM databases: usually the email address or an internal account ID serve as an identifier Partners' digital platform: identifier is also a cookie tied to their tracking domain To try to make things simple, let's take a concrete example: You work for UNICEF and want to optimize their banner ads budget by targeting the donors of their last fundraising campaign. Your reference user database is composed of the donors who registered with their email address on the last campaign: main identifier is the email address. To buy web display ads, you use an Ad Exchange partner such as AppNexus or DoubleClick (Google). From their point of view, users are seen as cookie IDs which are tied to their own domain. So you basically need to be able to translate an email address to a cookie ID for every partner you work with. Use case: ID matching tables \u00b6 We operate and maintain huge ID matching tables for every partner and a great deal of our time is spent translating those IDs from one to another. In SQL terms, we are basically doing JOINs between a dataset and those ID matching tables. You select your reference population You JOIN it with the corresponding ID matching table You get a matched population that your partner can recognize and interact with Those ID matching tables have a pretty high read AND write throughput because they're updated and queried all the time. Usual figures are JOINs between a 10+ Million dataset and 1.5+ Billion ID matching tables. The reference query basically looks like this: SELECT count(m.partnerid) FROM population_10M_rows AS p JOIN partner_id_match_400M_rows AS m ON p.id = m.id Current implementations \u00b6 We operate a lambda architecture where we handle real time ID matching using MongoDB and batch ones using Hive (Apache Hadoop). The first downside to note is that it requires us to maintain two copies of every ID matching table. We also couldn't choose one over the other because neither MongoDB nor Hive can sustain both the read/write lookup/update ratio while performing within the low latencies that we need . This is an operational burden and requires quite a bunch of engineering to ensure data consistency between different data stores. Production hardware overview: \u00b6 MongoDB is running on a 15 nodes (5 shards) cluster 64GB RAM, 2 sockets, RAID10 SAS spinning disks, 10Gbps dual NIC Hive is running on 50+ YARN NodeManager instances 128GB RAM, 2 sockets, JBOD SAS spinning disks, 10Gbps dual NIC Target implementation \u00b6 The key question is simple: is there a technology out there that can sustain our ID matching tables workloads while maintaining consistently low upsert/write and lookup/read latencies? Having one technology to handle both use cases would allow: Simpler data consistency Operational simplicity and efficiency Reduced costs POC hardware overview: \u00b6 So we decided to find out if Scylla could be that technology. For this, we used three decommissioned machines that we had in the basement of our Paris office. 2 DELL R510 19GB RAM, 2 socket 8 cores, RAID0 SAS spinning disks, 1Gbps NIC 1 DELL R710 19GB RAM, 2 socket 4 cores, RAID0 SAS spinning disks, 1Gbps NIC I know, these are not glamorous machines and they are even inconsistent in specs, but we still set up a 3 node Scylla cluster running Gentoo Linux with them. Our take? If those three lousy machines can challenge or beat the production machines on our current workloads, then Scylla can seriously be considered for production. Step 1: Validate a schema model \u00b6 Once the POC document was complete and the ScyllaDB team understood what we were trying to do, we started iterating on the schema model using a query based modeling strategy. So we wrote down and rated the questions that our model(s) should answer to, they included stuff like: What are all our cookie IDs associated to the given partner ID ? What are all the cookie IDs associated to the given partner ID over the last N months ? What is the last cookie ID/date for the given partner ID ? What is the last date we have seen the given cookie ID / partner ID couple ? As you can imagine, the reverse questions are also to be answered so ID translations can be done both ways (ouch!). Prototyping \u00b6 This is no news that I'm a Python addict so I did all my prototyping using Python and the cassandra-driver . I ended up using a test-driven data modelling strategy using pytest . I wrote tests on my dataset so I could concentrate on the model while making sure that all my questions were being answered correctly and consistently. Schema \u00b6 In our case, we ended up with three denormalized tables to answer all the questions we had. To answer the first three questions above, you could use the schema below: CREATE TABLE IF NOT EXISTS ids_by_partnerid( partnerid text, id text, date timestamp, PRIMARY KEY ((partnerid), date, id) ) WITH CLUSTERING ORDER BY (date DESC) Note on clustering key ordering \u00b6 One important learning I got in the process of validating the model is about the internals of Cassandra's file format that resulted in the choice of using a descending order DESC on the date clustering key as you can see above. If your main use case of querying is to look for the latest value of an history-like table design like ours, then make sure to change the default ASC order of your clustering key to DESC. This will ensure that the latest values (rows) are stored at the beginning of the sstable file effectively reducing the read latency when the row is not in cache! Let me quote Glauber Costa's detailed explanation on this: Basically in Cassandra's file format, the index points to an entire partition (for very large partitions there is a hack to avoid that, but the logic is mostly the same). So if you want to read the first row, that's easy you get the index to the partition and read the first row. I__f you want to read the last row, then you get the index to the partition and do a linear scan to the next. This is the kind of learning you can only get from experts like Glauber and that can justify the whole POC on its own! Step 2: Set up scylla-grafana-monitoring \u00b6 As I said before, make sure to set up and run the scylla-grafana-monitoring project before running your test workloads. This easy to run solution will be of great help to understand the performance of your cluster and to tune your workload for optimal performances . If you can, also discuss with the ScyllaDB team to allow them to access the Grafana dashboard. This will be very valuable since they know where to look better than we usually do... I gained a lot of understandings thanks to this! Note on scrape interval \u00b6 I advise you to lower the Prometheus scrape interval to have a shorter and finer sampling of your metrics. This will allow your dashboard to be more reactive when you start your test workloads. For this, change the prometheus/prometheus.yml file like this: scrape_interval: 2s # Scrape targets every 2 seconds (5s default) scrape_timeout: 1s # Timeout before trying to scrape a target again (4s default) Test your monitoring \u00b6 Before going any further, I strongly advise you to run a stress test on your POC cluster using the cassandra-stress tool and share the results and their monitoring graphs with the ScyllaDB team. This will give you a common understanding of the general performances of your cluster as well as help in outlining any obvious misconfiguration or hardware problem. Key graphs to look at \u00b6 There are a lot of interesting graphs so I'd like to share the ones that I have been mainly looking at. Remember that depending on your test workloads, some other graphs may be more relevant for you. number of open connections You'll want to see a steady and high enough number of open connections which will prove that your clients are pushed at their maximum (at the time of testing this graph was not on Grafana and you had to add it yourself) cache hits / misses Depending on your reference dataset, you'll obviously see that cache hits and misses will have a direct correlation with disk I/O and overall performances. Running your test workloads multiple times should trigger higher cache hits if your RAM is big enough. per shard/node distribution The Requests Served per shard graph should display a nicely distributed load between your shards and nodes so that you're sure that you're getting the best out of your cluster. The same is true for almost every other \"per shard/node\" graph: you're looking for evenly distributed load. sstable reads Directly linked with your disk performances, you'll be trying to make sure that you have almost no queued sstable reads. Step 3: Get your reference data and metrics \u00b6 We obviously need to have some reference metrics on our current production stack so we can compare them with the results on our POC Scylla cluster. Whether you choose to use your current production machines or set up a similar stack on the side to run your test workloads is up to you. We chose to run the vast majority of our tests on our current production machines to be as close to our real workloads as possible. Prepare a reference dataset \u00b6 During your work on the POC document, you should have detailed the usual data cardinality and volume you work with. Use this information to set up a reference dataset that you can use on all of the platforms that you plan to compare. In our case, we chose a 10 Million reference dataset that we JOINed with a 400+ Million extract of an ID matching table. Those volumes seemed easy enough to work with and allowed some nice ratio for memory bound workloads. Measure on your current stack \u00b6 Then it's time to load this reference datasets on your current platforms. If you run a MongoDB cluster like we do, make sure to shard and index the dataset just like you do on the production collections. On Hive, make sure to respect the storage file format of your current implementations as well as their partitioning . If you chose to run your test workloads on your production machines, make sure to run them multiple times and at different hours of the day and night so you can correlate the measures with the load on the cluster at the time of the tests. Reference metrics \u00b6 For the sake of simplicity I'll focus on the Hive-only batch workloads. I performed a count on the JOIN of the dataset and the ID matching table using Spark 2 and then I also ran the JOIN using a simple Hive query through Beeline. I gave the following definitions on the reference load: IDLE : YARN available containers and free resources are optimal, parallelism is very limited NORMAL : YARN sustains some casual load, parallelism exists but we are not bound by anything still HIGH : YARN has pending containers, parallelism is high and applications have to wait for containers before executing There's always an error margin on the results you get and I found that there was not significant enough differences between the results using Spark 2 and Beeline so I stuck with a simple set of results: IDLE: 2 minutes, 15 seconds NORMAL: 4 minutes HIGH: 15 minutes Step 4: Get Scylla in the mix \u00b6 It's finally time to do your best to break Scylla or at least to push it to its limits on your hardware... But most importantly, you'll be looking to understand what those limits are depending on your test workloads as well as outlining out all the required tuning that you will be required to do on the client side to reach those limits. Speaking about the results, we will have to differentiate two cases: The Scylla cluster is fresh and its cache is empty (cold start): performance is mostly Disk I/O bound The Scylla cluster has been running some test workload already and its cache is hot : performance is mostly Memory bound with some Disk I/O depending on the size of your RAM Spark 2 / Scala test workload \u00b6 Here I used Scala (yes, I did) and DataStax's spark-cassandra-connector so I could use the magic joinWithCassandraTable function. spark-cassandra-connector-2.0.1-s_2.11.jar Java 7 I had to stick with the 2.0.1 version of the spark-cassandra-connector because newer version (2.0.5 at the time of testing) were performing bad with no apparent reason. The ScyllaDB team couldn't help on this. You can interact with your test workload using the spark2-shell: spark2-shell --jars jars/commons-beanutils_commons-beanutils-1.9.3.jar,jars/com.twitter_jsr166e-1.1.0.jar,jars/io.netty_netty-all-4.0.33.Final.jar,jars/org.joda_joda-convert-1.2.jar,jars/commons-collections_commons-collections-3.2.2.jar,jars/joda-time_joda-time-2.3.jar,jars/org.scala-lang_scala-reflect-2.11.8.jar,jars/spark-cassandra-connector-2.0.1-s_2.11.jar Then use the following Scala imports: // main connector import import com.datastax.spark.connector._ // the joinWithCassandraTable failed without this (dunno why, I'm no Scala guy) import com.datastax.spark.connector.writer._ implicit val rowWriter = SqlRowWriter.Factory Finally I could run my test workload to select the data from Hive and JOIN it with Scylla easily: val df_population = spark.sql(\"SELECT id FROM population_10M_rows\") val join_rdd = df_population.rdd.repartitionByCassandraReplica(\"test_keyspace\", \"partner_id_match_400M_rows\").joinWithCassandraTable(\"test_keyspace\", \"partner_id_match_400M_rows\") val joined_count = join_rdd.count() Notes on tuning spark-cassandra-connector \u00b6 I experienced pretty crappy performances at first . Thanks to the easy Grafana monitoring, I could see that Scylla was not being the bottleneck at all and that I instead had trouble getting some real load on it. So I engaged in a thorough tuning of the spark-cassandra-connector with the help of Glauber... and it was pretty painful but we finally made it and got the best parameters to get the load on the Scylla cluster close to 100% when running the test workloads. This tuning was done in the spark-defaults.conf file: have a fixed set of executors and boost their overhead memory This will increase test results reliability by making sure you always have a reserved number of available workers at your disposal. spark.dynamicAllocation.enabled=false spark.executor.instances=30 spark.yarn.executor.memoryOverhead=1024 set the split size to 1MB Default is 8MB but Scylla uses a split size of 1MB so you'll see a great boost of performance and stability by setting this setting to the right number. spark.cassandra.input.split.size_in_mb=1 align driver timeouts with server timeouts It is advised to make sure that your read request timeouts are the same on the driver and the server so you do not get stalled states waiting for a timeout to happen on one hand. You can do the same with write timeouts if your test workloads are write intensive. /etc/scylla/scylla.yaml read_request_timeout_in_ms: 150000 spark-defaults.conf spark.cassandra.connection.timeout_ms=150000 spark.cassandra.read.timeout_ms=150000 // optional if you want to fail / retry faster for HA scenarios spark.cassandra.connection.reconnection_delay_ms.max=5000 spark.cassandra.connection.reconnection_delay_ms.min=1000 spark.cassandra.query.retry.count=100 adjust your reads per second rate Last but surely not least, this setting you will need to try and find out the best value for yourself since it has a direct impact on the load on your Scylla cluster. You will be looking at pushing your POC cluster to almost 100% load. spark.cassandra.input.reads_per_sec=6666 As I said before, I could only get this to work perfectly using the 2.0.1 version of the spark-cassandra-connector driver. But then it worked very well and with great speed. Spark 2 results \u00b6 Once tuned, the best results I was able to reach on this hardware are listed below. It's interesting to see that with spinning disks, the cold start result can compete with the results of a heavily loaded Hadoop cluster where pending containers and parallelism are knocking down its performances. hot cache: 2min cold cache: 12min Wow! Those three refurbished machines can compete with our current production machines and implementations , they can even match an idle Hive cluster of a medium size! Python test workload \u00b6 I couldn't conclude on a Scala/Spark 2 only test workload. So I obviously went back to my language of choice Python only to discover at my disappointment that there is no joinWithCassandraTable equivalent available on pyspark ... I tried with some projects claiming otherwise with no success until I changed my mind and decided that I probably didn't need Spark 2 at all. So I went into the crazy quest of beating Spark 2 performances using a pure Python implementation . This basically means that instead of having a JOIN like helper, I had to do a massive amount of single \"id -> partnerid\" lookups. Simple but greatly inefficient you say? Really? When I broke down the pieces, I was left with the following steps to implement and optimize: Load the 10M rows worth of population data from Hive For every row, lookup the corresponding partnerid in the ID matching table from Scylla Count the resulting number of matches The main problem to compete with Spark 2 is that it is a distributed framework and Python by itself is not. So you can't possibly imagine outperforming Spark 2 with your single machine . However, let's remember that Spark 2 is shipped and ran on executors using YARN so we are firing up JVMs and dispatching containers all the time. This is a quite expensive process that we have a chance to avoid using Python! So what I needed was a distributed computation framework that would allow to load data in a partitioned way and run the lookups on all the partitions in parallel before merging the results. In Python, this framework exists and is named Dask! You will obviously need to have to deploy a dask topology (that's easy and well documented ) to have a comparable number of dask workers than of Spark 2 executors (30 in my case) . The corresponding Python code samples are here . Hive + Scylla results \u00b6 Reading the population id's from Hive, the workload can be split and executed concurrently on multiple dask workers. read the 10M population rows from Hive in a partitioned manner for each partition (slice of 10M), query Scylla to lookup the possibly matching partnerid create a dataframe from the resulting matches gather back all the dataframes and merge them count the number of matches The results showed that it is possible to compete with Spark 2 with Dask : hot cache: 2min (rounded up) cold cache: 6min Interestingly, those almost two minutes can be broken down like this: distributed read data from Hive: 50s distributed lookup from Scylla: 60s merge + count: 10s This meant that if I could cut down the reading of data from Hive I could go even faster ! Parquet + Scylla results \u00b6 Going further on my previous remark I decided to get rid of Hive and put the 10M rows population data in a parquet file instead. I ended up trying to find out the most efficient way to read and load a parquet file from HDFS. My conclusion so far is that you can't be the amazing libhdfs3 + pyarrow combo. It is faster to load everything on a single machine than loading from Hive on multiple ones! The results showed that I could almost get rid of a whole minute in the total process, effectively and easily beating Spark 2 ! hot cache: 1min 5s cold cache: 5min Notes on the Python cassandra-driver \u00b6 Tests using Python showed robust queries experiencing far less failures than the spark-cassandra-connector, even more during the cold start scenario. The usage of execute_concurrent() provides a clean and linear interface to submit a large number of queries while providing some level of concurrency control Increasing the concurrency parameter from 100 to 512 provided additional throughput, but increasing it more looked useless Protocol version 4 forbids the tuning of connection requests / number to some sort of auto configuration. All tentative to hand tune it (by lowering protocol version to 2) failed to achieve higher throughput Installation of libev on the system allows the cassandra-driver to use it to handle concurrency instead of asyncore with a somewhat lower load footprint on the worker node but no noticeable change on the throughput When reading a parquet file stored on HDFS, the hdfs3 + pyarrow combo provides an insane speed (less than 10s to fully load 10M rows of a single column) Step 5: Play with High Availability \u00b6 I was quite disappointed and surprised by the lack of maturity of the Cassandra community on this critical topic. Maybe the main reason is that the cassandra-driver allows for too many levels of configuration and strategies. I wrote this simple bash script to allow me to simulate node failures. Then I could play with handling those failures and retries on the Python client code. !/bin/bash \u00b6 iptables -t filter -X iptables -t filter -F ip=\"0.0.0.0/0\" for port in 9042 9160 9180 10000 7000; do iptables -t filter -A INPUT -p tcp --dport ${port} -s ${ip} -j DROP iptables -t filter -A OUTPUT -p tcp --sport ${port} -d ${ip} -j DROP done while true; do trap break INT clear iptables -t filter -vnL sleep 1 done iptables -t filter -X iptables -t filter -F iptables -t filter -vnL This topic is worth going in more details on a dedicated blog post that I shall write later on while providing code samples. Concluding the evaluation \u00b6 I'm happy to say that Scylla passed our production evaluation and will soon go live on our infrastructure! As I said at the beginning of this post, the conclusion of the evaluation has not been driven by the good figures we got out of our test workloads. Those are no benchmarks and never pretended to be but we could still prove that performances were solid enough to not be a blocker in the adoption of Scylla. Instead we decided on the following points of interest (in no particular order): data consistency production reliability datacenter awareness ease of operation infrastructure rationalisation developer friendliness costs On the side, I tried Scylla on two other different use cases which proved interesting to follow later on to displace MongoDB again... Moving to production \u00b6 Since our relationship was great we also decided to partner with ScyllaDB and support them by subscribing to their enterprise offerings . They also accepted to support us using Gentoo Linux ! We are starting with a three nodes heavy duty cluster: DELL R640 dual socket 2,6GHz 14C, 512GB RAM, Samsung 17xxx NVMe 3,2 TB I'm eager to see ScyllaDB building up and will continue to help with my modest contributions. Thanks again to the ScyllaDB team for their patience and support during the POC!","title":"Evaluating ScyllaDB for production 2/2"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#business-context","text":"As a data driven company working in the Marketing and Advertising industry, we help our clients make sense of multiple sources of data to build and improve their relationship with their customers and prospects. Dealing with multiple sources of data is nothing new but their volume has dramatically changed during the past decade. I will spare you with the Big-Data-means-nothing term and the technical challenges that comes with it as you already heard enough of it. Still, it is clear that our line of business is tied to our capacity at mixing and correlating a massive amount of different types of events (data sources/types) coming from various sources which all have their own identifiers (think primary keys): Web navigation tracking: identifier is a cookie that's tied to the tracking domain (we have our own) CRM databases: usually the email address or an internal account ID serve as an identifier Partners' digital platform: identifier is also a cookie tied to their tracking domain To try to make things simple, let's take a concrete example: You work for UNICEF and want to optimize their banner ads budget by targeting the donors of their last fundraising campaign. Your reference user database is composed of the donors who registered with their email address on the last campaign: main identifier is the email address. To buy web display ads, you use an Ad Exchange partner such as AppNexus or DoubleClick (Google). From their point of view, users are seen as cookie IDs which are tied to their own domain. So you basically need to be able to translate an email address to a cookie ID for every partner you work with.","title":"Business context"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#use-case-id-matching-tables","text":"We operate and maintain huge ID matching tables for every partner and a great deal of our time is spent translating those IDs from one to another. In SQL terms, we are basically doing JOINs between a dataset and those ID matching tables. You select your reference population You JOIN it with the corresponding ID matching table You get a matched population that your partner can recognize and interact with Those ID matching tables have a pretty high read AND write throughput because they're updated and queried all the time. Usual figures are JOINs between a 10+ Million dataset and 1.5+ Billion ID matching tables. The reference query basically looks like this: SELECT count(m.partnerid) FROM population_10M_rows AS p JOIN partner_id_match_400M_rows AS m ON p.id = m.id","title":"Use case: ID matching tables"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#current-implementations","text":"We operate a lambda architecture where we handle real time ID matching using MongoDB and batch ones using Hive (Apache Hadoop). The first downside to note is that it requires us to maintain two copies of every ID matching table. We also couldn't choose one over the other because neither MongoDB nor Hive can sustain both the read/write lookup/update ratio while performing within the low latencies that we need . This is an operational burden and requires quite a bunch of engineering to ensure data consistency between different data stores.","title":"Current implementations"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#production-hardware-overview","text":"MongoDB is running on a 15 nodes (5 shards) cluster 64GB RAM, 2 sockets, RAID10 SAS spinning disks, 10Gbps dual NIC Hive is running on 50+ YARN NodeManager instances 128GB RAM, 2 sockets, JBOD SAS spinning disks, 10Gbps dual NIC","title":"Production hardware overview:"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#target-implementation","text":"The key question is simple: is there a technology out there that can sustain our ID matching tables workloads while maintaining consistently low upsert/write and lookup/read latencies? Having one technology to handle both use cases would allow: Simpler data consistency Operational simplicity and efficiency Reduced costs","title":"Target implementation"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#poc-hardware-overview","text":"So we decided to find out if Scylla could be that technology. For this, we used three decommissioned machines that we had in the basement of our Paris office. 2 DELL R510 19GB RAM, 2 socket 8 cores, RAID0 SAS spinning disks, 1Gbps NIC 1 DELL R710 19GB RAM, 2 socket 4 cores, RAID0 SAS spinning disks, 1Gbps NIC I know, these are not glamorous machines and they are even inconsistent in specs, but we still set up a 3 node Scylla cluster running Gentoo Linux with them. Our take? If those three lousy machines can challenge or beat the production machines on our current workloads, then Scylla can seriously be considered for production.","title":"POC hardware overview:"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-1-validate-a-schema-model","text":"Once the POC document was complete and the ScyllaDB team understood what we were trying to do, we started iterating on the schema model using a query based modeling strategy. So we wrote down and rated the questions that our model(s) should answer to, they included stuff like: What are all our cookie IDs associated to the given partner ID ? What are all the cookie IDs associated to the given partner ID over the last N months ? What is the last cookie ID/date for the given partner ID ? What is the last date we have seen the given cookie ID / partner ID couple ? As you can imagine, the reverse questions are also to be answered so ID translations can be done both ways (ouch!).","title":"Step 1: Validate a schema model"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#prototyping","text":"This is no news that I'm a Python addict so I did all my prototyping using Python and the cassandra-driver . I ended up using a test-driven data modelling strategy using pytest . I wrote tests on my dataset so I could concentrate on the model while making sure that all my questions were being answered correctly and consistently.","title":"Prototyping"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#schema","text":"In our case, we ended up with three denormalized tables to answer all the questions we had. To answer the first three questions above, you could use the schema below: CREATE TABLE IF NOT EXISTS ids_by_partnerid( partnerid text, id text, date timestamp, PRIMARY KEY ((partnerid), date, id) ) WITH CLUSTERING ORDER BY (date DESC)","title":"Schema"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#note-on-clustering-key-ordering","text":"One important learning I got in the process of validating the model is about the internals of Cassandra's file format that resulted in the choice of using a descending order DESC on the date clustering key as you can see above. If your main use case of querying is to look for the latest value of an history-like table design like ours, then make sure to change the default ASC order of your clustering key to DESC. This will ensure that the latest values (rows) are stored at the beginning of the sstable file effectively reducing the read latency when the row is not in cache! Let me quote Glauber Costa's detailed explanation on this: Basically in Cassandra's file format, the index points to an entire partition (for very large partitions there is a hack to avoid that, but the logic is mostly the same). So if you want to read the first row, that's easy you get the index to the partition and read the first row. I__f you want to read the last row, then you get the index to the partition and do a linear scan to the next. This is the kind of learning you can only get from experts like Glauber and that can justify the whole POC on its own!","title":"Note on clustering key ordering"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-2-set-up-scylla-grafana-monitoring","text":"As I said before, make sure to set up and run the scylla-grafana-monitoring project before running your test workloads. This easy to run solution will be of great help to understand the performance of your cluster and to tune your workload for optimal performances . If you can, also discuss with the ScyllaDB team to allow them to access the Grafana dashboard. This will be very valuable since they know where to look better than we usually do... I gained a lot of understandings thanks to this!","title":"Step 2: Set up scylla-grafana-monitoring"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#note-on-scrape-interval","text":"I advise you to lower the Prometheus scrape interval to have a shorter and finer sampling of your metrics. This will allow your dashboard to be more reactive when you start your test workloads. For this, change the prometheus/prometheus.yml file like this: scrape_interval: 2s # Scrape targets every 2 seconds (5s default) scrape_timeout: 1s # Timeout before trying to scrape a target again (4s default)","title":"Note on scrape interval"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#test-your-monitoring","text":"Before going any further, I strongly advise you to run a stress test on your POC cluster using the cassandra-stress tool and share the results and their monitoring graphs with the ScyllaDB team. This will give you a common understanding of the general performances of your cluster as well as help in outlining any obvious misconfiguration or hardware problem.","title":"Test your monitoring"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#key-graphs-to-look-at","text":"There are a lot of interesting graphs so I'd like to share the ones that I have been mainly looking at. Remember that depending on your test workloads, some other graphs may be more relevant for you. number of open connections You'll want to see a steady and high enough number of open connections which will prove that your clients are pushed at their maximum (at the time of testing this graph was not on Grafana and you had to add it yourself) cache hits / misses Depending on your reference dataset, you'll obviously see that cache hits and misses will have a direct correlation with disk I/O and overall performances. Running your test workloads multiple times should trigger higher cache hits if your RAM is big enough. per shard/node distribution The Requests Served per shard graph should display a nicely distributed load between your shards and nodes so that you're sure that you're getting the best out of your cluster. The same is true for almost every other \"per shard/node\" graph: you're looking for evenly distributed load. sstable reads Directly linked with your disk performances, you'll be trying to make sure that you have almost no queued sstable reads.","title":"Key graphs to look at"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-3-get-your-reference-data-and-metrics","text":"We obviously need to have some reference metrics on our current production stack so we can compare them with the results on our POC Scylla cluster. Whether you choose to use your current production machines or set up a similar stack on the side to run your test workloads is up to you. We chose to run the vast majority of our tests on our current production machines to be as close to our real workloads as possible.","title":"Step 3: Get your reference data and metrics"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#prepare-a-reference-dataset","text":"During your work on the POC document, you should have detailed the usual data cardinality and volume you work with. Use this information to set up a reference dataset that you can use on all of the platforms that you plan to compare. In our case, we chose a 10 Million reference dataset that we JOINed with a 400+ Million extract of an ID matching table. Those volumes seemed easy enough to work with and allowed some nice ratio for memory bound workloads.","title":"Prepare a reference dataset"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#measure-on-your-current-stack","text":"Then it's time to load this reference datasets on your current platforms. If you run a MongoDB cluster like we do, make sure to shard and index the dataset just like you do on the production collections. On Hive, make sure to respect the storage file format of your current implementations as well as their partitioning . If you chose to run your test workloads on your production machines, make sure to run them multiple times and at different hours of the day and night so you can correlate the measures with the load on the cluster at the time of the tests.","title":"Measure on your current stack"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#reference-metrics","text":"For the sake of simplicity I'll focus on the Hive-only batch workloads. I performed a count on the JOIN of the dataset and the ID matching table using Spark 2 and then I also ran the JOIN using a simple Hive query through Beeline. I gave the following definitions on the reference load: IDLE : YARN available containers and free resources are optimal, parallelism is very limited NORMAL : YARN sustains some casual load, parallelism exists but we are not bound by anything still HIGH : YARN has pending containers, parallelism is high and applications have to wait for containers before executing There's always an error margin on the results you get and I found that there was not significant enough differences between the results using Spark 2 and Beeline so I stuck with a simple set of results: IDLE: 2 minutes, 15 seconds NORMAL: 4 minutes HIGH: 15 minutes","title":"Reference metrics"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-4-get-scylla-in-the-mix","text":"It's finally time to do your best to break Scylla or at least to push it to its limits on your hardware... But most importantly, you'll be looking to understand what those limits are depending on your test workloads as well as outlining out all the required tuning that you will be required to do on the client side to reach those limits. Speaking about the results, we will have to differentiate two cases: The Scylla cluster is fresh and its cache is empty (cold start): performance is mostly Disk I/O bound The Scylla cluster has been running some test workload already and its cache is hot : performance is mostly Memory bound with some Disk I/O depending on the size of your RAM","title":"Step 4: Get Scylla in the mix"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#spark-2-scala-test-workload","text":"Here I used Scala (yes, I did) and DataStax's spark-cassandra-connector so I could use the magic joinWithCassandraTable function. spark-cassandra-connector-2.0.1-s_2.11.jar Java 7 I had to stick with the 2.0.1 version of the spark-cassandra-connector because newer version (2.0.5 at the time of testing) were performing bad with no apparent reason. The ScyllaDB team couldn't help on this. You can interact with your test workload using the spark2-shell: spark2-shell --jars jars/commons-beanutils_commons-beanutils-1.9.3.jar,jars/com.twitter_jsr166e-1.1.0.jar,jars/io.netty_netty-all-4.0.33.Final.jar,jars/org.joda_joda-convert-1.2.jar,jars/commons-collections_commons-collections-3.2.2.jar,jars/joda-time_joda-time-2.3.jar,jars/org.scala-lang_scala-reflect-2.11.8.jar,jars/spark-cassandra-connector-2.0.1-s_2.11.jar Then use the following Scala imports: // main connector import import com.datastax.spark.connector._ // the joinWithCassandraTable failed without this (dunno why, I'm no Scala guy) import com.datastax.spark.connector.writer._ implicit val rowWriter = SqlRowWriter.Factory Finally I could run my test workload to select the data from Hive and JOIN it with Scylla easily: val df_population = spark.sql(\"SELECT id FROM population_10M_rows\") val join_rdd = df_population.rdd.repartitionByCassandraReplica(\"test_keyspace\", \"partner_id_match_400M_rows\").joinWithCassandraTable(\"test_keyspace\", \"partner_id_match_400M_rows\") val joined_count = join_rdd.count()","title":"Spark 2 / Scala test workload"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#notes-on-tuning-spark-cassandra-connector","text":"I experienced pretty crappy performances at first . Thanks to the easy Grafana monitoring, I could see that Scylla was not being the bottleneck at all and that I instead had trouble getting some real load on it. So I engaged in a thorough tuning of the spark-cassandra-connector with the help of Glauber... and it was pretty painful but we finally made it and got the best parameters to get the load on the Scylla cluster close to 100% when running the test workloads. This tuning was done in the spark-defaults.conf file: have a fixed set of executors and boost their overhead memory This will increase test results reliability by making sure you always have a reserved number of available workers at your disposal. spark.dynamicAllocation.enabled=false spark.executor.instances=30 spark.yarn.executor.memoryOverhead=1024 set the split size to 1MB Default is 8MB but Scylla uses a split size of 1MB so you'll see a great boost of performance and stability by setting this setting to the right number. spark.cassandra.input.split.size_in_mb=1 align driver timeouts with server timeouts It is advised to make sure that your read request timeouts are the same on the driver and the server so you do not get stalled states waiting for a timeout to happen on one hand. You can do the same with write timeouts if your test workloads are write intensive. /etc/scylla/scylla.yaml read_request_timeout_in_ms: 150000 spark-defaults.conf spark.cassandra.connection.timeout_ms=150000 spark.cassandra.read.timeout_ms=150000 // optional if you want to fail / retry faster for HA scenarios spark.cassandra.connection.reconnection_delay_ms.max=5000 spark.cassandra.connection.reconnection_delay_ms.min=1000 spark.cassandra.query.retry.count=100 adjust your reads per second rate Last but surely not least, this setting you will need to try and find out the best value for yourself since it has a direct impact on the load on your Scylla cluster. You will be looking at pushing your POC cluster to almost 100% load. spark.cassandra.input.reads_per_sec=6666 As I said before, I could only get this to work perfectly using the 2.0.1 version of the spark-cassandra-connector driver. But then it worked very well and with great speed.","title":"Notes on tuning spark-cassandra-connector"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#spark-2-results","text":"Once tuned, the best results I was able to reach on this hardware are listed below. It's interesting to see that with spinning disks, the cold start result can compete with the results of a heavily loaded Hadoop cluster where pending containers and parallelism are knocking down its performances. hot cache: 2min cold cache: 12min Wow! Those three refurbished machines can compete with our current production machines and implementations , they can even match an idle Hive cluster of a medium size!","title":"Spark 2 results"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#python-test-workload","text":"I couldn't conclude on a Scala/Spark 2 only test workload. So I obviously went back to my language of choice Python only to discover at my disappointment that there is no joinWithCassandraTable equivalent available on pyspark ... I tried with some projects claiming otherwise with no success until I changed my mind and decided that I probably didn't need Spark 2 at all. So I went into the crazy quest of beating Spark 2 performances using a pure Python implementation . This basically means that instead of having a JOIN like helper, I had to do a massive amount of single \"id -> partnerid\" lookups. Simple but greatly inefficient you say? Really? When I broke down the pieces, I was left with the following steps to implement and optimize: Load the 10M rows worth of population data from Hive For every row, lookup the corresponding partnerid in the ID matching table from Scylla Count the resulting number of matches The main problem to compete with Spark 2 is that it is a distributed framework and Python by itself is not. So you can't possibly imagine outperforming Spark 2 with your single machine . However, let's remember that Spark 2 is shipped and ran on executors using YARN so we are firing up JVMs and dispatching containers all the time. This is a quite expensive process that we have a chance to avoid using Python! So what I needed was a distributed computation framework that would allow to load data in a partitioned way and run the lookups on all the partitions in parallel before merging the results. In Python, this framework exists and is named Dask! You will obviously need to have to deploy a dask topology (that's easy and well documented ) to have a comparable number of dask workers than of Spark 2 executors (30 in my case) . The corresponding Python code samples are here .","title":"Python test workload"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#hive-scylla-results","text":"Reading the population id's from Hive, the workload can be split and executed concurrently on multiple dask workers. read the 10M population rows from Hive in a partitioned manner for each partition (slice of 10M), query Scylla to lookup the possibly matching partnerid create a dataframe from the resulting matches gather back all the dataframes and merge them count the number of matches The results showed that it is possible to compete with Spark 2 with Dask : hot cache: 2min (rounded up) cold cache: 6min Interestingly, those almost two minutes can be broken down like this: distributed read data from Hive: 50s distributed lookup from Scylla: 60s merge + count: 10s This meant that if I could cut down the reading of data from Hive I could go even faster !","title":"Hive + Scylla results"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#parquet-scylla-results","text":"Going further on my previous remark I decided to get rid of Hive and put the 10M rows population data in a parquet file instead. I ended up trying to find out the most efficient way to read and load a parquet file from HDFS. My conclusion so far is that you can't be the amazing libhdfs3 + pyarrow combo. It is faster to load everything on a single machine than loading from Hive on multiple ones! The results showed that I could almost get rid of a whole minute in the total process, effectively and easily beating Spark 2 ! hot cache: 1min 5s cold cache: 5min","title":"Parquet + Scylla results"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#notes-on-the-python-cassandra-driver","text":"Tests using Python showed robust queries experiencing far less failures than the spark-cassandra-connector, even more during the cold start scenario. The usage of execute_concurrent() provides a clean and linear interface to submit a large number of queries while providing some level of concurrency control Increasing the concurrency parameter from 100 to 512 provided additional throughput, but increasing it more looked useless Protocol version 4 forbids the tuning of connection requests / number to some sort of auto configuration. All tentative to hand tune it (by lowering protocol version to 2) failed to achieve higher throughput Installation of libev on the system allows the cassandra-driver to use it to handle concurrency instead of asyncore with a somewhat lower load footprint on the worker node but no noticeable change on the throughput When reading a parquet file stored on HDFS, the hdfs3 + pyarrow combo provides an insane speed (less than 10s to fully load 10M rows of a single column)","title":"Notes on the Python cassandra-driver"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#step-5-play-with-high-availability","text":"I was quite disappointed and surprised by the lack of maturity of the Cassandra community on this critical topic. Maybe the main reason is that the cassandra-driver allows for too many levels of configuration and strategies. I wrote this simple bash script to allow me to simulate node failures. Then I could play with handling those failures and retries on the Python client code.","title":"Step 5: Play with High Availability"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#binbash","text":"iptables -t filter -X iptables -t filter -F ip=\"0.0.0.0/0\" for port in 9042 9160 9180 10000 7000; do iptables -t filter -A INPUT -p tcp --dport ${port} -s ${ip} -j DROP iptables -t filter -A OUTPUT -p tcp --sport ${port} -d ${ip} -j DROP done while true; do trap break INT clear iptables -t filter -vnL sleep 1 done iptables -t filter -X iptables -t filter -F iptables -t filter -vnL This topic is worth going in more details on a dedicated blog post that I shall write later on while providing code samples.","title":"!/bin/bash"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#concluding-the-evaluation","text":"I'm happy to say that Scylla passed our production evaluation and will soon go live on our infrastructure! As I said at the beginning of this post, the conclusion of the evaluation has not been driven by the good figures we got out of our test workloads. Those are no benchmarks and never pretended to be but we could still prove that performances were solid enough to not be a blocker in the adoption of Scylla. Instead we decided on the following points of interest (in no particular order): data consistency production reliability datacenter awareness ease of operation infrastructure rationalisation developer friendliness costs On the side, I tried Scylla on two other different use cases which proved interesting to follow later on to displace MongoDB again...","title":"Concluding the evaluation"},{"location":"Tech%20Blog/2018/2018-02-28-evaluating-scylladb-for-production-2-2/#moving-to-production","text":"Since our relationship was great we also decided to partner with ScyllaDB and support them by subscribing to their enterprise offerings . They also accepted to support us using Gentoo Linux ! We are starting with a three nodes heavy duty cluster: DELL R640 dual socket 2,6GHz 14C, 512GB RAM, Samsung 17xxx NVMe 3,2 TB I'm eager to see ScyllaDB building up and will continue to help with my modest contributions. Thanks again to the ScyllaDB team for their patience and support during the POC!","title":"Moving to production"},{"location":"Tech%20Blog/2018/2018-04-03-py3status-v3-8/","text":"Another long awaited release has come true thanks to our community ! The changelog is so huge that I had to open an issue and cry for help to make it happen... thanks again @lasers for stepping up once again :) Highlights \u00b6 gevent support (-g option) to switch from threads scheduling to greenlets and reduce resources consumption environment variables support in i3status.conf to remove sensible information from your config modules can now leverage a persistent data store hundreds of improvements for various modules we now have an official debian package we reached 500 stars on github #vanity Milestone 3.9 \u00b6 try to release a version faster than every 4 months (j/k) ;) The next release will focus on bugs and modules improvements / standardization. Thanks contributors! \u00b6 This release is their work, thanks a lot guys! alex o'neill anubiann00b cypher1 daniel foerster daniel schaefer girst igor grebenkov james curtis lasers maxim baz nollain raspbeguy regnat robert ricci s\u00e9bastien delafond themistokle benetatos tobes woland","title":"py3status v3.8"},{"location":"Tech%20Blog/2018/2018-04-03-py3status-v3-8/#highlights","text":"gevent support (-g option) to switch from threads scheduling to greenlets and reduce resources consumption environment variables support in i3status.conf to remove sensible information from your config modules can now leverage a persistent data store hundreds of improvements for various modules we now have an official debian package we reached 500 stars on github #vanity","title":"Highlights"},{"location":"Tech%20Blog/2018/2018-04-03-py3status-v3-8/#milestone-39","text":"try to release a version faster than every 4 months (j/k) ;) The next release will focus on bugs and modules improvements / standardization.","title":"Milestone 3.9"},{"location":"Tech%20Blog/2018/2018-04-03-py3status-v3-8/#thanks-contributors","text":"This release is their work, thanks a lot guys! alex o'neill anubiann00b cypher1 daniel foerster daniel schaefer girst igor grebenkov james curtis lasers maxim baz nollain raspbeguy regnat robert ricci s\u00e9bastien delafond themistokle benetatos tobes woland","title":"Thanks contributors!"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/","text":"I felt like sharing a recent story that allowed us identify a bot in a haystack thanks to Scylla. The scenario \u00b6 While working on loading up 2B+ of rows into Scylla from Hive (using Spark), we noticed a strange behaviour in the performances of one of our nodes: So we started wondering why that server in blue was having those peaks of load and was clearly diverging from the two others... As we obviously expect the three nodes to behave the same , there were two options on the table: hardware problem on the node bad data distribution (bad schema design? consistent hash problem?) We shared this with our pals from ScyllaDB and started working on finding out what was going on. The investigation \u00b6 Hardware? \u00b6 Hardware problem was pretty quickly evicted, nothing showed up on the monitoring and on the kernel logs. I/O queues and throughput were good: Data distribution? \u00b6 Avi Kivity (ScyllaDB's CTO) quickly got the feeling that something was wrong with the data distribution and that we could be facing a hotspot situation . He quickly nailed it down to shard 44 thanks to the scylla-grafana-monitoring platform. Data is distributed between shards that are stored on nodes (consistent hash ring). This distribution is done by hashing the primary key of your data which dictates the shard it belongs to (and thus the node(s) where the shard is stored). If one of your keys is over represented in your original data set, then the shard it belongs to can be overly populated and the related node overloaded. This is called a hotspot situation . tracing queries \u00b6 The first step was to trace queries in Scylla to try to get deeper into the hotspot analysis. So we enabled tracing using the following formula to get about 1 trace per second in the system_traces namespace. tracing probability = 1 / expected requests per second throughput In our case, we were doing between 90K req/s and 150K req/s so we settled for 100K req/s to be safe and enabled tracing on our nodes like this: # nodetool settraceprobability 0.00001 Turns out tracing didn't help very much in our case because the traces do not include the query parameters in Scylla 2.1, it is becoming available in the soon to be released 2.2 version. NOTE : traces expire on the tables, make sure your TRUNCATE the events and sessions tables while iterating. Else you will have to wait for the next gc_grace_period (10 days by default) before they are actually removed. If you do not do that and generate millions of traces like we did, querying the mentioned tables will likely time out because of the \"tombstoned\" rows even if there is no trace inside any more. looking at cfhistograms \u00b6 Glauber Costa was also helping on the case and got us looking at the cfhistograms of the tables we were pushing data to. That proved to be clearly highlighting a hotspot problem: histograms Percentile SSTables Write Latency Read Latency Partition Size Cell Count (micros) (micros) (bytes) 50% 0,00 6,00 0,00 258 2 75% 0,00 6,00 0,00 535 5 95% 0,00 8,00 0,00 1916 24 98% 0,00 11,72 0,00 3311 50 99% 0,00 28,46 0,00 5722 72 Min 0,00 2,00 0,00 104 0 Max 0,00 45359,00 0,00 14530764 182785 What this basically means is that 99% percentile of our partitions are small (5KB) while the biggest is 14MB! That's a huge difference and clearly shows that we have a hotspot on a partition somewhere. So now we know for sure that we have an over represented key in our data set, but what key is it and why? The culprit \u00b6 So we looked at the cardinality of our data set keys which are SHA256 hashes and found out that indeed we had one with more than 1M occurrences while the second highest one was around 100K!... Now that we had the main culprit hash, we turned to our data streaming pipeline to figure out what kind of event was generating the data associated to the given SHA256 hash... and surprise! It was a client's quality assurance bot that was constantly browsing their own website with legitimate behaviour and identity credentials associated to it . So we modified our pipeline to detect this bot and discard its events so that it stops polluting our databases with fake data. Then we cleaned up the million of events worth of mess and traces we stored about the bot. The aftermath \u00b6 Finally, we cleared out the data in Scylla and tried again from scratch. Needless to say that the curves got way better and are exactly what we should expect from a well balanced cluster : Thanks a lot to the ScyllaDB team for their thorough help and high spirited support! I'll quote them conclude this quick blog post:","title":"A botspot story"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#the-scenario","text":"While working on loading up 2B+ of rows into Scylla from Hive (using Spark), we noticed a strange behaviour in the performances of one of our nodes: So we started wondering why that server in blue was having those peaks of load and was clearly diverging from the two others... As we obviously expect the three nodes to behave the same , there were two options on the table: hardware problem on the node bad data distribution (bad schema design? consistent hash problem?) We shared this with our pals from ScyllaDB and started working on finding out what was going on.","title":"The scenario"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#the-investigation","text":"","title":"The investigation"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#hardware","text":"Hardware problem was pretty quickly evicted, nothing showed up on the monitoring and on the kernel logs. I/O queues and throughput were good:","title":"Hardware?"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#data-distribution","text":"Avi Kivity (ScyllaDB's CTO) quickly got the feeling that something was wrong with the data distribution and that we could be facing a hotspot situation . He quickly nailed it down to shard 44 thanks to the scylla-grafana-monitoring platform. Data is distributed between shards that are stored on nodes (consistent hash ring). This distribution is done by hashing the primary key of your data which dictates the shard it belongs to (and thus the node(s) where the shard is stored). If one of your keys is over represented in your original data set, then the shard it belongs to can be overly populated and the related node overloaded. This is called a hotspot situation .","title":"Data distribution?"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#tracing-queries","text":"The first step was to trace queries in Scylla to try to get deeper into the hotspot analysis. So we enabled tracing using the following formula to get about 1 trace per second in the system_traces namespace. tracing probability = 1 / expected requests per second throughput In our case, we were doing between 90K req/s and 150K req/s so we settled for 100K req/s to be safe and enabled tracing on our nodes like this: # nodetool settraceprobability 0.00001 Turns out tracing didn't help very much in our case because the traces do not include the query parameters in Scylla 2.1, it is becoming available in the soon to be released 2.2 version. NOTE : traces expire on the tables, make sure your TRUNCATE the events and sessions tables while iterating. Else you will have to wait for the next gc_grace_period (10 days by default) before they are actually removed. If you do not do that and generate millions of traces like we did, querying the mentioned tables will likely time out because of the \"tombstoned\" rows even if there is no trace inside any more.","title":"tracing queries"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#looking-at-cfhistograms","text":"Glauber Costa was also helping on the case and got us looking at the cfhistograms of the tables we were pushing data to. That proved to be clearly highlighting a hotspot problem: histograms Percentile SSTables Write Latency Read Latency Partition Size Cell Count (micros) (micros) (bytes) 50% 0,00 6,00 0,00 258 2 75% 0,00 6,00 0,00 535 5 95% 0,00 8,00 0,00 1916 24 98% 0,00 11,72 0,00 3311 50 99% 0,00 28,46 0,00 5722 72 Min 0,00 2,00 0,00 104 0 Max 0,00 45359,00 0,00 14530764 182785 What this basically means is that 99% percentile of our partitions are small (5KB) while the biggest is 14MB! That's a huge difference and clearly shows that we have a hotspot on a partition somewhere. So now we know for sure that we have an over represented key in our data set, but what key is it and why?","title":"looking at cfhistograms"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#the-culprit","text":"So we looked at the cardinality of our data set keys which are SHA256 hashes and found out that indeed we had one with more than 1M occurrences while the second highest one was around 100K!... Now that we had the main culprit hash, we turned to our data streaming pipeline to figure out what kind of event was generating the data associated to the given SHA256 hash... and surprise! It was a client's quality assurance bot that was constantly browsing their own website with legitimate behaviour and identity credentials associated to it . So we modified our pipeline to detect this bot and discard its events so that it stops polluting our databases with fake data. Then we cleaned up the million of events worth of mess and traces we stored about the bot.","title":"The culprit"},{"location":"Tech%20Blog/2018/2018-07-06-a-botspot-story/#the-aftermath","text":"Finally, we cleared out the data in Scylla and tried again from scratch. Needless to say that the curves got way better and are exactly what we should expect from a well balanced cluster : Thanks a lot to the ScyllaDB team for their thorough help and high spirited support! I'll quote them conclude this quick blog post:","title":"The aftermath"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/","text":"This quick article is a wrap up for reference on how to connect to ScyllaDB using Spark 2 when authentication and SSL are enforced for the clients on the Scylla cluster. We encountered multiple problems, even more since we distribute our workload using a YARN cluster so that our worker nodes should have everything they need to connect properly to Scylla. We found very little help online so I hope it will serve anyone facing similar issues (that's also why I copy/pasted them here). The authentication part is easy going by itself and was not the source of our problems, SSL on the client side was. Environment \u00b6 (py)spark: 2.1.0.cloudera2 spark-cassandra-connector: datastax:spark-cassandra-connector: 2.0.1-s_2.11 python: 3.5.5 java: 1.8.0_144 scylladb: 2.1.5 SSL cipher setup \u00b6 The Datastax spark cassandra driver uses default the TLS_RSA_WITH_AES_256_CBC_SHA cipher that the JVM does not support by default. This raises the following error when connecting to Scylla: 18/07/18 13:13:41 WARN channel.ChannelInitializer: Failed to initialize a channel. Closing: [id: 0x8d6f78a7] java.lang.IllegalArgumentException: Cannot support TLS_RSA_WITH_AES_256_CBC_SHA with currently installed providers According to the ssl documentation we have two ciphers available: TLS_RSA_WITH_AES_256_CBC_SHA TLS_RSA_WITH_AES_128_CBC_SHA We can get get rid of the error by lowering the cipher to TLS_RSA_WITH_AES_128_CBC_SHA using the following configuration: .config(\"spark.cassandra.connection.ssl.enabledAlgorithms\", \"TLS_RSA_WITH_AES_128_CBC_SHA\")\\ However, this is not really a good solution and instead we'd be inclined to use the TLS_RSA_WITH_AES_256_CBC_SHA version. For this we need to follow this Datastax's procedure . Then we need to deploy the JCE security jars on our all client nodes , if using YARN like us this means that you have to deploy these jars to all your NodeManager nodes. For example by hand: # unzip jce_policy-8.zip cp UnlimitedJCEPolicyJDK8/*.jar /opt/oracle-jdk-bin-1.8.0.144/jre/lib/security/ \u00b6 Java trust store \u00b6 When connecting, the clients need to be able to validate the Scylla cluster's self-signed CA. This is done by setting up a trustStore JKS file and providing it to the spark connector configuration (note that you protect this file with a password). keyStore vs trustStore \u00b6 In SSL handshake purpose of trustStore is to verify credentials and purpose of keyStore is to provide credentials . keyStore in Java stores private key and certificates corresponding to the public keys and is required if you are a SSL Server or SSL requires client authentication. TrustStore stores certificates from third parties or your own self-signed certificates, your application identify and validates them using this trustStore. The spark-cassandra-connector documentation has two options to handle keyStore and trustStore. When we did not use the trustStore option, we would get some obscure error when connecting to Scylla: com.datastax.driver.core.exceptions.TransportException: [node/1.1.1.1:9042] Channel has been closed When enabling DEBUG logging, we get a clearer error which indicated a failure in validating the SSL certificate provided by the Scylla server node: Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target setting up the trustStore JKS \u00b6 You need to have the self-signed CA public certificate file, then issue the following command: # keytool -importcert -file /usr/local/share/ca-certificates/MY_SELF_SIGNED_CA.crt -keystore COMPANY_TRUSTSTORE.jks -noprompt Enter keystore password: Re-enter new password: Certificate was added to keystore using the trustStore \u00b6 Now you need to configure spark to use the trustStore like this: .config(\"spark.cassandra.connection.ssl.trustStore.password\", \"PASSWORD\")\\ .config(\"spark.cassandra.connection.ssl.trustStore.path\", \"COMPANY_TRUSTSTORE.jks\")\\ Spark SSL configuration example \u00b6 This wraps up the SSL connection configuration used for spark. This example uses pyspark2 and reads a table in Scylla from a YARN cluster: $ pyspark2 --packages datastax:spark-cassandra-connector:2.0.1-s_2.11 --files COMPANY_TRUSTSTORE.jks spark = SparkSession.builder.appName(\"scylla_app\")\\ .config(\"spark.cassandra.auth.password\", \"test\")\\ .config(\"spark.cassandra.auth.username\", \"test\")\\ .config(\"spark.cassandra.connection.host\", \"node1,node2,node3\")\\ .config(\"spark.cassandra.connection.ssl.clientAuth.enabled\", True)\\ .config(\"spark.cassandra.connection.ssl.enabled\", True)\\ .config(\"spark.cassandra.connection.ssl.trustStore.password\", \"PASSWORD\")\\ .config(\"spark.cassandra.connection.ssl.trustStore.path\", \"COMPANY_TRUSTSTORE.jks\")\\ .config(\"spark.cassandra.input.split.size_in_mb\", 1)\\ .config(\"spark.yarn.queue\", \"scylla_queue\").getOrCreate() df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_table\", keyspace=\"test\").load() df.show()","title":"Authenticating and connecting to a SSL enabled Scylla cluster using Spark 2"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#environment","text":"(py)spark: 2.1.0.cloudera2 spark-cassandra-connector: datastax:spark-cassandra-connector: 2.0.1-s_2.11 python: 3.5.5 java: 1.8.0_144 scylladb: 2.1.5","title":"Environment"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#ssl-cipher-setup","text":"The Datastax spark cassandra driver uses default the TLS_RSA_WITH_AES_256_CBC_SHA cipher that the JVM does not support by default. This raises the following error when connecting to Scylla: 18/07/18 13:13:41 WARN channel.ChannelInitializer: Failed to initialize a channel. Closing: [id: 0x8d6f78a7] java.lang.IllegalArgumentException: Cannot support TLS_RSA_WITH_AES_256_CBC_SHA with currently installed providers According to the ssl documentation we have two ciphers available: TLS_RSA_WITH_AES_256_CBC_SHA TLS_RSA_WITH_AES_128_CBC_SHA We can get get rid of the error by lowering the cipher to TLS_RSA_WITH_AES_128_CBC_SHA using the following configuration: .config(\"spark.cassandra.connection.ssl.enabledAlgorithms\", \"TLS_RSA_WITH_AES_128_CBC_SHA\")\\ However, this is not really a good solution and instead we'd be inclined to use the TLS_RSA_WITH_AES_256_CBC_SHA version. For this we need to follow this Datastax's procedure . Then we need to deploy the JCE security jars on our all client nodes , if using YARN like us this means that you have to deploy these jars to all your NodeManager nodes. For example by hand: # unzip jce_policy-8.zip","title":"SSL cipher setup"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#cp-unlimitedjcepolicyjdk8jar-optoracle-jdk-bin-180144jrelibsecurity","text":"","title":"cp UnlimitedJCEPolicyJDK8/*.jar /opt/oracle-jdk-bin-1.8.0.144/jre/lib/security/"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#java-trust-store","text":"When connecting, the clients need to be able to validate the Scylla cluster's self-signed CA. This is done by setting up a trustStore JKS file and providing it to the spark connector configuration (note that you protect this file with a password).","title":"Java trust store"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#keystore-vs-truststore","text":"In SSL handshake purpose of trustStore is to verify credentials and purpose of keyStore is to provide credentials . keyStore in Java stores private key and certificates corresponding to the public keys and is required if you are a SSL Server or SSL requires client authentication. TrustStore stores certificates from third parties or your own self-signed certificates, your application identify and validates them using this trustStore. The spark-cassandra-connector documentation has two options to handle keyStore and trustStore. When we did not use the trustStore option, we would get some obscure error when connecting to Scylla: com.datastax.driver.core.exceptions.TransportException: [node/1.1.1.1:9042] Channel has been closed When enabling DEBUG logging, we get a clearer error which indicated a failure in validating the SSL certificate provided by the Scylla server node: Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target","title":"keyStore vs trustStore"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#setting-up-the-truststore-jks","text":"You need to have the self-signed CA public certificate file, then issue the following command: # keytool -importcert -file /usr/local/share/ca-certificates/MY_SELF_SIGNED_CA.crt -keystore COMPANY_TRUSTSTORE.jks -noprompt Enter keystore password: Re-enter new password: Certificate was added to keystore","title":"setting up the trustStore JKS"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#using-the-truststore","text":"Now you need to configure spark to use the trustStore like this: .config(\"spark.cassandra.connection.ssl.trustStore.password\", \"PASSWORD\")\\ .config(\"spark.cassandra.connection.ssl.trustStore.path\", \"COMPANY_TRUSTSTORE.jks\")\\","title":"using the trustStore"},{"location":"Tech%20Blog/2018/2018-07-19-authenticating-and-connecting-to-a-ssl-enabled-scylla-cluster-using-spark-2/#spark-ssl-configuration-example","text":"This wraps up the SSL connection configuration used for spark. This example uses pyspark2 and reads a table in Scylla from a YARN cluster: $ pyspark2 --packages datastax:spark-cassandra-connector:2.0.1-s_2.11 --files COMPANY_TRUSTSTORE.jks spark = SparkSession.builder.appName(\"scylla_app\")\\ .config(\"spark.cassandra.auth.password\", \"test\")\\ .config(\"spark.cassandra.auth.username\", \"test\")\\ .config(\"spark.cassandra.connection.host\", \"node1,node2,node3\")\\ .config(\"spark.cassandra.connection.ssl.clientAuth.enabled\", True)\\ .config(\"spark.cassandra.connection.ssl.enabled\", True)\\ .config(\"spark.cassandra.connection.ssl.trustStore.password\", \"PASSWORD\")\\ .config(\"spark.cassandra.connection.ssl.trustStore.path\", \"COMPANY_TRUSTSTORE.jks\")\\ .config(\"spark.cassandra.input.split.size_in_mb\", 1)\\ .config(\"spark.yarn.queue\", \"scylla_queue\").getOrCreate() df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_table\", keyspace=\"test\").load() df.show()","title":"Spark SSL configuration example"},{"location":"Tech%20Blog/2018/2018-09-28-py3status-v3-13/","text":"I am once again lagging behind the release blog posts but this one is an important one. I'm proud to announce that our long time contributor @lasers has become an official collaborator of the py3status project! Dear @ lasers , your amazing energy and overwhelming ideas have served our little community for a while. I'm sure we'll have a great way forward as we learn to work together with @tobes :) Thank you again very much for everything you do! This release is as much dedicated to you as it is yours :) IMPORTANT notice \u00b6 After this release, py3status coding style CI will enforce the ' black ' formatter style. Highlights \u00b6 Needless to say that the changelog is huge, as usual, here is a very condensed view: documentation updates, especially on the formatter (thanks @L0ric0) py3 storage: use $XDG_CACHE_HOME or ~/.cache formatter: multiple variable and feature fixes and enhancements better config parser new modules: lm_sensors, loadavg, mail, nvidia_smi, sql, timewarrior, wanda_the_fish Thank you contributors! \u00b6 lasers tobes maximbaz cyrinux Lorenz Steinert @L0ric0 wojtex horgix su8 Maikel Punie","title":"py3status v3.13"},{"location":"Tech%20Blog/2018/2018-09-28-py3status-v3-13/#important-notice","text":"After this release, py3status coding style CI will enforce the ' black ' formatter style.","title":"IMPORTANT notice"},{"location":"Tech%20Blog/2018/2018-09-28-py3status-v3-13/#highlights","text":"Needless to say that the changelog is huge, as usual, here is a very condensed view: documentation updates, especially on the formatter (thanks @L0ric0) py3 storage: use $XDG_CACHE_HOME or ~/.cache formatter: multiple variable and feature fixes and enhancements better config parser new modules: lm_sensors, loadavg, mail, nvidia_smi, sql, timewarrior, wanda_the_fish","title":"Highlights"},{"location":"Tech%20Blog/2018/2018-09-28-py3status-v3-13/#thank-you-contributors","text":"lasers tobes maximbaz cyrinux Lorenz Steinert @L0ric0 wojtex horgix su8 Maikel Punie","title":"Thank you contributors!"},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/","text":"I'm happy to announce this release as it contains some very interesting developments in the project. This release was focused on core changes. IMPORTANT notice \u00b6 There are now two optional dependencies to py3status: gevent will monkey patch the code to make it concurrent the main benefit is to use an asynchronous loop instead of threads pyudev will enable a udev monitor if a module asks for it (only xrandr so far) the benefit is described below To install them all using pip, simply do: pip install py3status[all] Modules can now react/refresh on udev events \u00b6 When pyudev is available, py3status will allow modules to subscribe and react to udev events ! The xrandr module uses this feature by default which allows the module to instantly refresh when you plug in or off a secondary monitor. This also allows to stop running the xrandr command in the background and saves a lot of CPU! Highlights \u00b6 py3status core uses black formatter fix default i3status.conf detection add ~/.config/i3 as a default config directory, closes #1548 add .config/i3/py3status in default user modules include directories add markup (pango) support for modules (#1408), by @MikaYuoadas py3: notify_user module name in the title (#1556), by @lasers print module information to sdtout instead of stderr (#1565), by @robertnf battery_level module: default to using sys instead of acpi (#1562), by @eddie-dunn imap module: fix output formatting issue (#1559), by @girst Thank you contributors! \u00b6 eddie-dunn girst MikaYuoadas robertnf lasers maximbaz tobes","title":"py3status v3.14"},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/#important-notice","text":"There are now two optional dependencies to py3status: gevent will monkey patch the code to make it concurrent the main benefit is to use an asynchronous loop instead of threads pyudev will enable a udev monitor if a module asks for it (only xrandr so far) the benefit is described below To install them all using pip, simply do: pip install py3status[all]","title":"IMPORTANT notice"},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/#modules-can-now-reactrefresh-on-udev-events","text":"When pyudev is available, py3status will allow modules to subscribe and react to udev events ! The xrandr module uses this feature by default which allows the module to instantly refresh when you plug in or off a secondary monitor. This also allows to stop running the xrandr command in the background and saves a lot of CPU!","title":"Modules can now react/refresh on udev events"},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/#highlights","text":"py3status core uses black formatter fix default i3status.conf detection add ~/.config/i3 as a default config directory, closes #1548 add .config/i3/py3status in default user modules include directories add markup (pango) support for modules (#1408), by @MikaYuoadas py3: notify_user module name in the title (#1556), by @lasers print module information to sdtout instead of stderr (#1565), by @robertnf battery_level module: default to using sys instead of acpi (#1562), by @eddie-dunn imap module: fix output formatting issue (#1559), by @girst","title":"Highlights"},{"location":"Tech%20Blog/2018/2018-11-10-py3status-v3-14/#thank-you-contributors","text":"eddie-dunn girst MikaYuoadas robertnf lasers maximbaz tobes","title":"Thank you contributors!"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/","text":"It's been almost one month since I had the chance to attend and speak at Scylla Summit 2018 so I'm relieved to finally publish a short write-up on the key things I wanted to share about this wonderful event! Make Scylla boring \u00b6 This statement of Glauber Costa sums up what looked to me to be the main driver of the engineering efforts put into Scylla lately: making it work so consistently well on any kind of workload that it's boring to operate :) I will follow up on this statement to highlight the things I heard and (hopefully) understood during the summit. I hope you'll find it insightful. Reduced operational efforts \u00b6 The thread-per-core and queues design still has a lot of possibilities to be leveraged. The recent addition of RPC streaming capabilities to seastar allows a drastic reduction in the time it takes the cluster to grow or shrink (data rebalancing / resynchronization). Incremental compaction is also very promising as this background process is one of the most expensive there is in the database's design. I was happy to hear that scylla-manager will soon be made available and free to use with basic features while retaining more advanced ones for enterprise version (like backup/restore). I also noticed that the current version was not supporting SSL enabled clusters to store its configuration. So I directly asked Micha\u0142 for it and I'm glad that it will be released on version 1.3.1. Performant multi-tenancy \u00b6 Why choose between real-time OLTP & analytics OLAP workloads? The goal here is to be able to run both on the same cluster by giving users the ability to assign \"SLA\" shares to ROLES. That's basically like pools on Hadoop at a much finer grain since it will create dedicated queues that will be weighted by their share. Having one queue per usage and full accounting will allow to limit resources efficiently and users to have their say on their latency SLAs. But Scylla also has a lot to do in the background to run smoothly. So while this design pattern was already applied to tamper compactions, a lot of work has also been done on automatic flow control and back pressure. For instance, Materialized Views are updated asynchronously which means that while we can interact and put a lot of pressure on the table its based on (called the Main Table), we could overwhelm the background work that's needed to keep MVs View Tables in sync. To mitigate this, a smart back pressure approach was developed and will throttle the clients to make sure that Scylla can manage to do everything at the best performance the hardware allows! I was happy to hear that work on tiered storage is also planned to better optimize disk space costs for certain workloads. Last but not least, columnar storage optimized for time series and analytics workloads are also something the developers are looking at. Latency is expensive \u00b6 If you care for latency, you might be happy to hear that a new polling API (named IOCB_CMD_POLL ) has been contributed by Christoph Hellwig and Avi Kivity to the 4.19 Linux kernel which avoids context switching I/O by using a shared ring between kernel and userspace. Scylla will be using it by default if the kernel supports it. The iotune utility has been upgraded since 2.3 to generate an enhanced I/O configuration. Also, persistent (disk backed) in-memory tables are getting ready and are very promising for latency sensitive workloads! A word on drivers \u00b6 ScyllaDB has been relying on the Datastax drivers since the start. While it's a good thing for the whole community, it's important to note that the shard-per-CPU approach on data that Scylla is using is not known and leveraged by the current drivers. Discussions took place and it seems that Datastax will not allow the protocol to evolve so that drivers could discover if the connected cluster is shard aware or not and then use this information to be more clever in which write/read path to use. So for now ScyllaDB has been forking and developing their shard aware drivers for Java and Go (no Python yet... I was disappointed). Kubernetes & containers \u00b6 The ScyllaDB guys of course couldn't avoid the Kubernetes frenzy so Moreno Garcia gave a lot of feedback and tips on how to operate Scylla on docker with minimal performance degradation. Kubernetes has been designed for stateless applications, not stateful ones and Docker does some automatic magic that have rather big performance hits on Scylla. You will basically have to play with affinities to dedicate one Scylla instance to run on one server with a \"retain\" reclaim policy. Remember that the official Scylla docker image runs with dev-mode enabled by default which turns off all performance checks on start. So start by disabling that and look at all the tips and literature that Moreno has put online! Scylla 3.0 \u00b6 A lot has been written on it already so I will just be short on things that important to understand in my point of view. Materialized Views do back fill the whole data set this job is done by the view building process you can watch its progress in the system_distributed.view_build_status table Secondary Indexes are Materialized Views under the hood it's like a reverse pointer to the primary key of the Main Table so if you read the whole row by selecting on the indexed column, two reads will be issued under the hood: one on the indexed MV view table to get the primary key and one on the main table to get the rest of the columns so if your workload is mostly interested by the whole row, you're better off creating a complete MV to read from than using a SI this is even more true if you plan to do range scans as this double query could lead you to read from multiple nodes instead of one Range scan is way more performant ALLOW FILTERING finally allows a great flexibility by providing server-side filtering ! Random notes \u00b6 Support for LWT (lightweight transactions) will be relying on a future implementation of the Raft consensus algorithm inside Scylla. This work will also benefits Materialized Views consistency. Duarte Nunes will be the one working on this and I envy him very much! Support for search workloads is high in the ScyllaDB devs priorities so we should definitely hear about it in the coming months. Support for \" mc \" sstables (new generation format) is done and will reduce storage requirements thanks to metadata / data compression. Migration will be transparent because Scylla can read previous formats as well so it will upgrade your sstables as it compacts them. ScyllaDB developers have not settled on how to best implement CDC . I hope they do rather soon because it is crucial in their ability to integrate well with Kafka! Materialized Views, Secondary Indexes and filtering will benefit from the work on partition key and indexes intersections to avoid server side filtering on the coordinator. That's an important optimization to come! Last but not least, I've had the pleasure to discuss with Takuya Asada who is the packager of Scylla for RedHat/CentOS & Debian/Ubuntu. We discussed Gentoo Linux packaging requirements as well as the recent and promising work on a relocatable package. We will collaborate more closely in the future!","title":"Scylla Summit 2018 write-up"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#make-scylla-boring","text":"This statement of Glauber Costa sums up what looked to me to be the main driver of the engineering efforts put into Scylla lately: making it work so consistently well on any kind of workload that it's boring to operate :) I will follow up on this statement to highlight the things I heard and (hopefully) understood during the summit. I hope you'll find it insightful.","title":"Make Scylla boring"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#reduced-operational-efforts","text":"The thread-per-core and queues design still has a lot of possibilities to be leveraged. The recent addition of RPC streaming capabilities to seastar allows a drastic reduction in the time it takes the cluster to grow or shrink (data rebalancing / resynchronization). Incremental compaction is also very promising as this background process is one of the most expensive there is in the database's design. I was happy to hear that scylla-manager will soon be made available and free to use with basic features while retaining more advanced ones for enterprise version (like backup/restore). I also noticed that the current version was not supporting SSL enabled clusters to store its configuration. So I directly asked Micha\u0142 for it and I'm glad that it will be released on version 1.3.1.","title":"Reduced operational efforts"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#performant-multi-tenancy","text":"Why choose between real-time OLTP & analytics OLAP workloads? The goal here is to be able to run both on the same cluster by giving users the ability to assign \"SLA\" shares to ROLES. That's basically like pools on Hadoop at a much finer grain since it will create dedicated queues that will be weighted by their share. Having one queue per usage and full accounting will allow to limit resources efficiently and users to have their say on their latency SLAs. But Scylla also has a lot to do in the background to run smoothly. So while this design pattern was already applied to tamper compactions, a lot of work has also been done on automatic flow control and back pressure. For instance, Materialized Views are updated asynchronously which means that while we can interact and put a lot of pressure on the table its based on (called the Main Table), we could overwhelm the background work that's needed to keep MVs View Tables in sync. To mitigate this, a smart back pressure approach was developed and will throttle the clients to make sure that Scylla can manage to do everything at the best performance the hardware allows! I was happy to hear that work on tiered storage is also planned to better optimize disk space costs for certain workloads. Last but not least, columnar storage optimized for time series and analytics workloads are also something the developers are looking at.","title":"Performant multi-tenancy"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#latency-is-expensive","text":"If you care for latency, you might be happy to hear that a new polling API (named IOCB_CMD_POLL ) has been contributed by Christoph Hellwig and Avi Kivity to the 4.19 Linux kernel which avoids context switching I/O by using a shared ring between kernel and userspace. Scylla will be using it by default if the kernel supports it. The iotune utility has been upgraded since 2.3 to generate an enhanced I/O configuration. Also, persistent (disk backed) in-memory tables are getting ready and are very promising for latency sensitive workloads!","title":"Latency is expensive"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#a-word-on-drivers","text":"ScyllaDB has been relying on the Datastax drivers since the start. While it's a good thing for the whole community, it's important to note that the shard-per-CPU approach on data that Scylla is using is not known and leveraged by the current drivers. Discussions took place and it seems that Datastax will not allow the protocol to evolve so that drivers could discover if the connected cluster is shard aware or not and then use this information to be more clever in which write/read path to use. So for now ScyllaDB has been forking and developing their shard aware drivers for Java and Go (no Python yet... I was disappointed).","title":"A word on drivers"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#kubernetes-containers","text":"The ScyllaDB guys of course couldn't avoid the Kubernetes frenzy so Moreno Garcia gave a lot of feedback and tips on how to operate Scylla on docker with minimal performance degradation. Kubernetes has been designed for stateless applications, not stateful ones and Docker does some automatic magic that have rather big performance hits on Scylla. You will basically have to play with affinities to dedicate one Scylla instance to run on one server with a \"retain\" reclaim policy. Remember that the official Scylla docker image runs with dev-mode enabled by default which turns off all performance checks on start. So start by disabling that and look at all the tips and literature that Moreno has put online!","title":"Kubernetes &amp; containers"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#scylla-30","text":"A lot has been written on it already so I will just be short on things that important to understand in my point of view. Materialized Views do back fill the whole data set this job is done by the view building process you can watch its progress in the system_distributed.view_build_status table Secondary Indexes are Materialized Views under the hood it's like a reverse pointer to the primary key of the Main Table so if you read the whole row by selecting on the indexed column, two reads will be issued under the hood: one on the indexed MV view table to get the primary key and one on the main table to get the rest of the columns so if your workload is mostly interested by the whole row, you're better off creating a complete MV to read from than using a SI this is even more true if you plan to do range scans as this double query could lead you to read from multiple nodes instead of one Range scan is way more performant ALLOW FILTERING finally allows a great flexibility by providing server-side filtering !","title":"Scylla 3.0"},{"location":"Tech%20Blog/2018/2018-12-06-scylla-summit-2018-write-up/#random-notes","text":"Support for LWT (lightweight transactions) will be relying on a future implementation of the Raft consensus algorithm inside Scylla. This work will also benefits Materialized Views consistency. Duarte Nunes will be the one working on this and I envy him very much! Support for search workloads is high in the ScyllaDB devs priorities so we should definitely hear about it in the coming months. Support for \" mc \" sstables (new generation format) is done and will reduce storage requirements thanks to metadata / data compression. Migration will be transparent because Scylla can read previous formats as well so it will upgrade your sstables as it compacts them. ScyllaDB developers have not settled on how to best implement CDC . I hope they do rather soon because it is crucial in their ability to integrate well with Kafka! Materialized Views, Secondary Indexes and filtering will benefit from the work on partition key and indexes intersections to avoid server side filtering on the coordinator. That's an important optimization to come! Last but not least, I've had the pleasure to discuss with Takuya Asada who is the packager of Scylla for RedHat/CentOS & Debian/Ubuntu. We discussed Gentoo Linux packaging requirements as well as the recent and promising work on a relocatable package. We will collaborate more closely in the future!","title":"Random notes"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/","text":"Two py3status versions in less than a month? That's the holidays effect but not only! Our community has been busy discussing our way forward to 4.0 (see below) and organization so it was time I wrote a bit about that. Community \u00b6 A new collaborator \u00b6 First of all we have the great pleasure and honor to welcome Maxim Baz @ maximbaz as a new collaborator on the project! His engagement, numerous contributions and insightful reviews to py3status has made him a well known community member, not to mention his IRC support :) Once again, thank you for being there Maxim! Zen of py3status \u00b6 As a result of an interesting discussion, we worked on defining better how to contribute to py3status as well as a set of guidelines we agree on to get the project moving on smoothly. Here is born the zen of py3status which extends the philosophy from the user point of view to the contributor point of view! This allowed us to handle the numerous open pull requests and get their number down to 5 at the time of writing this post! Even our dear @lasers don't have any open PR anymore :) 3.15 + 3.16 versions \u00b6 Our magic @ lasers has worked a lot on general modules options as well as adding support for i3-gaps added features such as border coloring and fine tuning. Also interesting is the work of Thiago Kenji Okada @ m45t3r around NixOS packaging of py3status . Thanks a lot for this work and for sharing Thiago! I also liked the question of Andreas Lundblad @ aioobe asking if we could have a feature allowing to display a custom graphical output , such as a small PNG or anything upon clicking on the i3bar, you might be interested in following up the i3 issue he opened . Make sure to read the amazing changelog for details, a lot of modules have been enhanced! Highlights \u00b6 You can now set a background, border colors and their urgent counterparts on a global scale or per module CI now checks for black format on modules, so now all the code base obey the black format style! All HTTP requests based modules now have a standard way to define HTTP timeout as well as the same 10 seconds default timeout py3-cmd now allows sending click events with modifiers The py3status -n / --interval command line argument has been removed as it was obsolete. We will ignore it if you have set it up, but better remove it to be clean You can specify your own i3status binary path using the new -u, --i3status command line argument thanks to @Dettorer and @lasers Since Yahoo! decided to retire its public & free weather API, the weather_yahoo module has been removed New modules \u00b6 new conky module: display conky system monitoring (#1664), by lasers new module emerge_status : display information about running gentoo emerge (#1275), by AnwariasEu new module hueshift : change your screen color temperature (#1142), by lasers new module mega_sync : to check for MEGA service synchronization (#1458), by Maxim Baz new module speedtest : to check your internet bandwidth (#1435), by cyrinux new module usbguard : control usbguard from your bar (#1376), by cyrinux new module velib_metropole : display velib metropole stations and (e)bikes (#1515), by cyrinux A word on 4.0 \u00b6 Do you wonder what's gonna be in the 4.0 release? Do you have ideas that you'd like to share? Do you have dreams that you'd love to become true? Then make sure to read and participate in the open RFC on 4.0 version ! Development has not started yet; we really want to hear from you. Thank you contributors! \u00b6 There would be no py3status release without our amazing contributors, so thank you guys! AnwariasEu cyrinux Dettorer ecks flyingapfopenguin girst Jack Doan justin j lin Keith Hughitt L0ric0 lasers Maxim Baz oceyral Simon Legner sridhars Thiago Kenji Okada Thomas F. Duellmann Till Backhaus","title":"py3status v3.16"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#community","text":"","title":"Community"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#a-new-collaborator","text":"First of all we have the great pleasure and honor to welcome Maxim Baz @ maximbaz as a new collaborator on the project! His engagement, numerous contributions and insightful reviews to py3status has made him a well known community member, not to mention his IRC support :) Once again, thank you for being there Maxim!","title":"A new collaborator"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#zen-of-py3status","text":"As a result of an interesting discussion, we worked on defining better how to contribute to py3status as well as a set of guidelines we agree on to get the project moving on smoothly. Here is born the zen of py3status which extends the philosophy from the user point of view to the contributor point of view! This allowed us to handle the numerous open pull requests and get their number down to 5 at the time of writing this post! Even our dear @lasers don't have any open PR anymore :)","title":"Zen of py3status"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#315-316-versions","text":"Our magic @ lasers has worked a lot on general modules options as well as adding support for i3-gaps added features such as border coloring and fine tuning. Also interesting is the work of Thiago Kenji Okada @ m45t3r around NixOS packaging of py3status . Thanks a lot for this work and for sharing Thiago! I also liked the question of Andreas Lundblad @ aioobe asking if we could have a feature allowing to display a custom graphical output , such as a small PNG or anything upon clicking on the i3bar, you might be interested in following up the i3 issue he opened . Make sure to read the amazing changelog for details, a lot of modules have been enhanced!","title":"3.15 + 3.16 versions"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#highlights","text":"You can now set a background, border colors and their urgent counterparts on a global scale or per module CI now checks for black format on modules, so now all the code base obey the black format style! All HTTP requests based modules now have a standard way to define HTTP timeout as well as the same 10 seconds default timeout py3-cmd now allows sending click events with modifiers The py3status -n / --interval command line argument has been removed as it was obsolete. We will ignore it if you have set it up, but better remove it to be clean You can specify your own i3status binary path using the new -u, --i3status command line argument thanks to @Dettorer and @lasers Since Yahoo! decided to retire its public & free weather API, the weather_yahoo module has been removed","title":"Highlights"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#new-modules","text":"new conky module: display conky system monitoring (#1664), by lasers new module emerge_status : display information about running gentoo emerge (#1275), by AnwariasEu new module hueshift : change your screen color temperature (#1142), by lasers new module mega_sync : to check for MEGA service synchronization (#1458), by Maxim Baz new module speedtest : to check your internet bandwidth (#1435), by cyrinux new module usbguard : control usbguard from your bar (#1376), by cyrinux new module velib_metropole : display velib metropole stations and (e)bikes (#1515), by cyrinux","title":"New modules"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#a-word-on-40","text":"Do you wonder what's gonna be in the 4.0 release? Do you have ideas that you'd like to share? Do you have dreams that you'd love to become true? Then make sure to read and participate in the open RFC on 4.0 version ! Development has not started yet; we really want to hear from you.","title":"A word on 4.0"},{"location":"Tech%20Blog/2019/2019-01-20-py3status-v3-16/#thank-you-contributors","text":"There would be no py3status release without our amazing contributors, so thank you guys! AnwariasEu cyrinux Dettorer ecks flyingapfopenguin girst Jack Doan justin j lin Keith Hughitt L0ric0 lasers Maxim Baz oceyral Simon Legner sridhars Thiago Kenji Okada Thomas F. Duellmann Till Backhaus","title":"Thank you contributors!"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/","text":"A few days ago, I removed Google Analytics from my blog and trashed the associated account. I've been part of the Marketing Tech and Advertising Tech industries for over 15 years. I design and operate data processing platforms (including web navigation trackers) for a living. So I thought that maybe sharing the reasons of why I took this decision might be of interest for some people. I'll keep it short. MY convenience is not a enough reason to send YOUR data to Google \u00b6 The first and obvious question I asked myself is why did I (and so many people) set up this tracker on my web site? My initial answer was a mix of: convenience : it's easy to set up, there's a nice interface, you get a lot of details, you don't have to ask yourself how it's done, it just works insight : it sounded somewhat important to know who was visiting what content and somehow know about the interest of people visiting With also a (hopefully not too much) of: pride : are some blog posts popular? if so which one and let's try to do more like this! I don't think those are good enough reasons to add a tracker that sends YOUR data to Google. Convenience kills diversity \u00b6 I'm old enough to have witnessed the rise of internet and its availability to (almost) everyone. The first things I did when I could connect was create and host my own web site, it was great and looked ugly! But while Internet could have been a catalyst for diversity, it turned out to create an over concentration on services and tools that we think are hard to live without because of their convenience (and a human tendency for mimicry). When your choices are reduced and the mass adoption defines your standards, it's easy to let it go and pretend you don't care that much. I decided to stop pretending that I don't care. I don't want to participate in the concentration of web navigation tracking to Google. Open Internet is at risk \u00b6 When diversity is endangered so is Open Internet. This idea that a rich ecosystem can bring their own value and be free to grow by using the data they generate or collect is threatened by the GAFA who are building walled gardens around OUR data. For instance, Google used the GDPR regulation as an excuse to close down the access to the data collected by their (so convenient) services. If a company (or you) wants to access / query this data ( YOUR data) then you can only by using their billed tools . What should have been only a clear win for us people turned out to also benefit those super blocks and threaten diversity and Open Internet even more. Adding Google Analytics to your web site helps Google have a larger reach and tracking footprint on the whole web: imagine all those millions of web sites visits added together, that's where the value is for them. No wonder GA is free. So in this regard too, I decided to stop contributing to the empowerment of Google. This blog is Tracking Free \u00b6 So from now on if you want to share your thoughts of just let me know you enjoyed a post on this blog, take the lead on YOUR data and use the comment box. The choice is yours!","title":"Bye bye Google Analytics"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/#my-convenience-is-not-a-enough-reason-to-send-your-data-to-google","text":"The first and obvious question I asked myself is why did I (and so many people) set up this tracker on my web site? My initial answer was a mix of: convenience : it's easy to set up, there's a nice interface, you get a lot of details, you don't have to ask yourself how it's done, it just works insight : it sounded somewhat important to know who was visiting what content and somehow know about the interest of people visiting With also a (hopefully not too much) of: pride : are some blog posts popular? if so which one and let's try to do more like this! I don't think those are good enough reasons to add a tracker that sends YOUR data to Google.","title":"MY convenience is not a enough reason to send YOUR data to Google"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/#convenience-kills-diversity","text":"I'm old enough to have witnessed the rise of internet and its availability to (almost) everyone. The first things I did when I could connect was create and host my own web site, it was great and looked ugly! But while Internet could have been a catalyst for diversity, it turned out to create an over concentration on services and tools that we think are hard to live without because of their convenience (and a human tendency for mimicry). When your choices are reduced and the mass adoption defines your standards, it's easy to let it go and pretend you don't care that much. I decided to stop pretending that I don't care. I don't want to participate in the concentration of web navigation tracking to Google.","title":"Convenience kills diversity"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/#open-internet-is-at-risk","text":"When diversity is endangered so is Open Internet. This idea that a rich ecosystem can bring their own value and be free to grow by using the data they generate or collect is threatened by the GAFA who are building walled gardens around OUR data. For instance, Google used the GDPR regulation as an excuse to close down the access to the data collected by their (so convenient) services. If a company (or you) wants to access / query this data ( YOUR data) then you can only by using their billed tools . What should have been only a clear win for us people turned out to also benefit those super blocks and threaten diversity and Open Internet even more. Adding Google Analytics to your web site helps Google have a larger reach and tracking footprint on the whole web: imagine all those millions of web sites visits added together, that's where the value is for them. No wonder GA is free. So in this regard too, I decided to stop contributing to the empowerment of Google.","title":"Open Internet is at risk"},{"location":"Tech%20Blog/2019/2019-03-01-bye-bye-google-analytics/#this-blog-is-tracking-free","text":"So from now on if you want to share your thoughts of just let me know you enjoyed a post on this blog, take the lead on YOUR data and use the comment box. The choice is yours!","title":"This blog is Tracking Free"},{"location":"Tech%20Blog/2019/2019-03-25-py3status-v3-17/","text":"I'm glad to announce a new (awaited) release of py3status featuring support for the sway window manager which allows py3status to enter the wayland environment! Updated configuration and custom modules paths detection \u00b6 The configuration section of the documentation explains the updated detection of the py3status configuration file (with respect of XDG_CONFIG environment variables): ~/.config/py3status/config ~/.config/i3status/config ~/.config/i3/i3status.conf ~/.i3status.conf ~/.i3/i3status.conf /etc/xdg/i3status/config /etc/i3status.conf Regarding custom modules paths detection , py3status does as described in the documentation: ~/.config/py3status/modules ~/.config/i3status/py3status ~/.config/i3/py3status ~/.i3/py3status Highlights \u00b6 Lots of modules improvements and clean ups, see changelog . we worked on the documentation sections and content which allowed us to fix a bunch of typos our magic @lasers have worked a lot on harmonizing thresholds on modules along with a lot of code clean ups new module: scroll to scroll modules on your bar (#1748) @lasers has worked a lot on a more granular pango support for modules output (still work to do as it breaks some composites) Thanks contributors \u00b6 Ajeet D'Souza @boucman Cody Hiar @cyriunx @duffydack @lasers Maxim Baz Thiago Kenji Okada Yaroslav Dronskii","title":"py3status v3.17"},{"location":"Tech%20Blog/2019/2019-03-25-py3status-v3-17/#updated-configuration-and-custom-modules-paths-detection","text":"The configuration section of the documentation explains the updated detection of the py3status configuration file (with respect of XDG_CONFIG environment variables): ~/.config/py3status/config ~/.config/i3status/config ~/.config/i3/i3status.conf ~/.i3status.conf ~/.i3/i3status.conf /etc/xdg/i3status/config /etc/i3status.conf Regarding custom modules paths detection , py3status does as described in the documentation: ~/.config/py3status/modules ~/.config/i3status/py3status ~/.config/i3/py3status ~/.i3/py3status","title":"Updated configuration and custom modules paths detection"},{"location":"Tech%20Blog/2019/2019-03-25-py3status-v3-17/#highlights","text":"Lots of modules improvements and clean ups, see changelog . we worked on the documentation sections and content which allowed us to fix a bunch of typos our magic @lasers have worked a lot on harmonizing thresholds on modules along with a lot of code clean ups new module: scroll to scroll modules on your bar (#1748) @lasers has worked a lot on a more granular pango support for modules output (still work to do as it breaks some composites)","title":"Highlights"},{"location":"Tech%20Blog/2019/2019-03-25-py3status-v3-17/#thanks-contributors","text":"Ajeet D'Souza @boucman Cody Hiar @cyriunx @duffydack @lasers Maxim Baz Thiago Kenji Okada Yaroslav Dronskii","title":"Thanks contributors"},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/","text":"We recently had to face free disk space outages on some of our scylla clusters and we learnt some very interesting things while outlining some improvements that could be made to the ScyllaDB guys. 100% disk space usage? \u00b6 First of all I wanted to give a bit of a heads up about what happened when some of our scylla nodes reached (almost) 100% disk space usage. Basically they: stopped listening to client requests complained in the logs wouldn't flush commitlog (expected) abort their compaction work (which actually gave back a few GB of space) stay in a stuck / unable to stop state (unexpected, this has been reported) After restarting your scylla server, the first and obvious thing you can try to do to get out of this situation is to run the nodetool clearsnapshot command which will remove any data snapshot that could be lying around. That's a handy command to reclaim space usually. Reminder: depending on your compaction strategy, it is usually not advised to allow your data to grow over 50% of disk space. .. But that's only a patch so let's go down the rabbit hole and look at the optimization options we have. Optimize your schemas \u00b6 Schema design and the types your choose for your columns have a huge impact on disk space usage! And in our case we indeed overlooked some of the optimizations that we could have done from the start and that did cost us a lot of wasted disk space. Fortunately it was easy and fast to change. To illustrate this, I'll take a sample of 100,000 rows of a simple and naive schema associating readings of 50 integers to a user ID: Note: all those operations were done using Scylla 3.0.3 on Gentoo Linux. CREATE TABLE IF NOT EXISTS test.not_optimized ( uid text, readings list , PRIMARY KEY(uid) ) WITH compression = {}; Once inserted on disk, this takes about 250MB of disk space: 250M not_optimized-00cf1500520b11e9ae38000000000004 Now depending on your use case, if those readings at not meant to be updated for example you could use a frozen list instead, which will allow a huge storage optimization: CREATE TABLE IF NOT EXISTS test.mid_optimized ( uid text, readings frozen >, PRIMARY KEY(uid) ) WITH compression = {}; With this frozen list we now consume 54MB of disk space for the same data ! 54M mid_optimized-011bae60520b11e9ae38000000000004 There's another optimization that we could do since our user ID are UUIDs. Let's switch to the uuid type instead of text : CREATE TABLE IF NOT EXISTS test.optimized ( uid uuid, readings frozen >, PRIMARY KEY(uid) ) WITH compression = {}; By switching to uuid , we now consume 50MB of disk space: that's a 80% reduced disk space consumption compared to the naive schema for the same data! 50M optimized-01f74150520b11e9ae38000000000004 Enable compression \u00b6 All those examples were not using compression. If your workload latencies allows it, you should probably enable compression on your sstables. Let's see its impact on our tables: ALTER TABLE test.not_optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}; ALTER TABLE test.mid_optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}; ALTER TABLE test.optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}; Then we run a nodetool compact test to force a (re)compaction of all the sstables and we get: 63M not_optimized-00cf1500520b11e9ae38000000000004 28M mid_optimized-011bae60520b11e9ae38000000000004 24M optimized-01f74150520b11e9ae38000000000004 Compression is really a great gain here allowing another 50% reduced disk space usage reduction on our optimized table ! Switch to the new \"mc\" sstable format \u00b6 Since the Scylla 3.0 release you can use the latest \"mc\" sstable storage format on your scylla clusters. It promises a greater efficiency for usually a way more reduced disk space consumption! It is not enabled by default, you have to add the enable_sstables_mc_format: true parameter to your scylla.yaml for it to be taken into account. Since it's backward compatible, you have nothing else to do as new compactions will start being made using the \"mc\" storage format and the scylla server will seamlessly read from old sstables as well. But in our case of immediate disk space outage, we switched to the new format one node at a time, dropped the data from it and ran a nodetool rebuild to reconstruct the whole node using the new sstable format. Let's demonstrate its impact on our test tables: we add the option to the scylla.yaml file, restart scylla-server and run n odetool compact test again: 49M not_optimized-00cf1500520b11e9ae38000000000004 26M mid_optimized-011bae60520b11e9ae38000000000004 22M optimized-01f74150520b11e9ae38000000000004 That's a pretty cool gain of disk space, even more for the not optimized version of our schema! So if you're in great need of disk space or it is hard for you to change your schemas, switching to the new \"mc\" sstable format is a simple and efficient way to free up some space without effort. Consider using secondary indexes \u00b6 While denormalization is the norm ( yep.. legitimate pun ) in the NoSQL world this does not mean we have to duplicate everything all the time. A good example lies in the internals of secondary indexes if your workload can compromise with its moderate impact on latency. Secondary indexes on scylla are built on top of Materialized Views that basically stores an up to date pointer from your indexed column to your main table partition key. That means that secondary indexes MVs are not duplicating all the columns (and thus the data) from your main table as you would have to do when denormalizing a table to query by another column: this saves disk space! This of course comes with a latency drawback because if your workload is interested in the other columns than the partition key of the main table, the coordinator node will actually issue two queries to get all your data: query the secondary index MV to get the pointer to the partition key of the main table query the main table with the partition key to get the rest of the columns you asked for This has been an effective trick to avoid duplicating a table and save disk space for some of our workloads! (not a tip) Move the commitlog to another disk / partition? \u00b6 This should only be considered as a sort of emergency procedure or for cost efficiency (cheap disk tiering) on non critical clusters . While this is possible even if the disk is not formatted using XFS, it not advised to separate the commitlog from data on modern SSD/NVMe disks but... you technically can do it (as we did) on non production clusters . Switching is simple, you just need to change the commitlog_directory parameter in your scylla.yaml file.","title":"Scylla: four ways to optimize your disk space consumption"},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#100-disk-space-usage","text":"First of all I wanted to give a bit of a heads up about what happened when some of our scylla nodes reached (almost) 100% disk space usage. Basically they: stopped listening to client requests complained in the logs wouldn't flush commitlog (expected) abort their compaction work (which actually gave back a few GB of space) stay in a stuck / unable to stop state (unexpected, this has been reported) After restarting your scylla server, the first and obvious thing you can try to do to get out of this situation is to run the nodetool clearsnapshot command which will remove any data snapshot that could be lying around. That's a handy command to reclaim space usually. Reminder: depending on your compaction strategy, it is usually not advised to allow your data to grow over 50% of disk space. .. But that's only a patch so let's go down the rabbit hole and look at the optimization options we have.","title":"100% disk space usage?"},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#optimize-your-schemas","text":"Schema design and the types your choose for your columns have a huge impact on disk space usage! And in our case we indeed overlooked some of the optimizations that we could have done from the start and that did cost us a lot of wasted disk space. Fortunately it was easy and fast to change. To illustrate this, I'll take a sample of 100,000 rows of a simple and naive schema associating readings of 50 integers to a user ID: Note: all those operations were done using Scylla 3.0.3 on Gentoo Linux. CREATE TABLE IF NOT EXISTS test.not_optimized ( uid text, readings list , PRIMARY KEY(uid) ) WITH compression = {}; Once inserted on disk, this takes about 250MB of disk space: 250M not_optimized-00cf1500520b11e9ae38000000000004 Now depending on your use case, if those readings at not meant to be updated for example you could use a frozen list instead, which will allow a huge storage optimization: CREATE TABLE IF NOT EXISTS test.mid_optimized ( uid text, readings frozen >, PRIMARY KEY(uid) ) WITH compression = {}; With this frozen list we now consume 54MB of disk space for the same data ! 54M mid_optimized-011bae60520b11e9ae38000000000004 There's another optimization that we could do since our user ID are UUIDs. Let's switch to the uuid type instead of text : CREATE TABLE IF NOT EXISTS test.optimized ( uid uuid, readings frozen >, PRIMARY KEY(uid) ) WITH compression = {}; By switching to uuid , we now consume 50MB of disk space: that's a 80% reduced disk space consumption compared to the naive schema for the same data! 50M optimized-01f74150520b11e9ae38000000000004","title":"Optimize your schemas"},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#enable-compression","text":"All those examples were not using compression. If your workload latencies allows it, you should probably enable compression on your sstables. Let's see its impact on our tables: ALTER TABLE test.not_optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}; ALTER TABLE test.mid_optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}; ALTER TABLE test.optimized WITH compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}; Then we run a nodetool compact test to force a (re)compaction of all the sstables and we get: 63M not_optimized-00cf1500520b11e9ae38000000000004 28M mid_optimized-011bae60520b11e9ae38000000000004 24M optimized-01f74150520b11e9ae38000000000004 Compression is really a great gain here allowing another 50% reduced disk space usage reduction on our optimized table !","title":"Enable compression"},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#switch-to-the-new-mc-sstable-format","text":"Since the Scylla 3.0 release you can use the latest \"mc\" sstable storage format on your scylla clusters. It promises a greater efficiency for usually a way more reduced disk space consumption! It is not enabled by default, you have to add the enable_sstables_mc_format: true parameter to your scylla.yaml for it to be taken into account. Since it's backward compatible, you have nothing else to do as new compactions will start being made using the \"mc\" storage format and the scylla server will seamlessly read from old sstables as well. But in our case of immediate disk space outage, we switched to the new format one node at a time, dropped the data from it and ran a nodetool rebuild to reconstruct the whole node using the new sstable format. Let's demonstrate its impact on our test tables: we add the option to the scylla.yaml file, restart scylla-server and run n odetool compact test again: 49M not_optimized-00cf1500520b11e9ae38000000000004 26M mid_optimized-011bae60520b11e9ae38000000000004 22M optimized-01f74150520b11e9ae38000000000004 That's a pretty cool gain of disk space, even more for the not optimized version of our schema! So if you're in great need of disk space or it is hard for you to change your schemas, switching to the new \"mc\" sstable format is a simple and efficient way to free up some space without effort.","title":"Switch to the new \"mc\" sstable format"},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#consider-using-secondary-indexes","text":"While denormalization is the norm ( yep.. legitimate pun ) in the NoSQL world this does not mean we have to duplicate everything all the time. A good example lies in the internals of secondary indexes if your workload can compromise with its moderate impact on latency. Secondary indexes on scylla are built on top of Materialized Views that basically stores an up to date pointer from your indexed column to your main table partition key. That means that secondary indexes MVs are not duplicating all the columns (and thus the data) from your main table as you would have to do when denormalizing a table to query by another column: this saves disk space! This of course comes with a latency drawback because if your workload is interested in the other columns than the partition key of the main table, the coordinator node will actually issue two queries to get all your data: query the secondary index MV to get the pointer to the partition key of the main table query the main table with the partition key to get the rest of the columns you asked for This has been an effective trick to avoid duplicating a table and save disk space for some of our workloads!","title":"Consider using secondary indexes"},{"location":"Tech%20Blog/2019/2019-03-29-scylla-four-ways-to-optimize-your-disk-space-consumption/#not-a-tip-move-the-commitlog-to-another-disk-partition","text":"This should only be considered as a sort of emergency procedure or for cost efficiency (cheap disk tiering) on non critical clusters . While this is possible even if the disk is not formatted using XFS, it not advised to separate the commitlog from data on modern SSD/NVMe disks but... you technically can do it (as we did) on non production clusters . Switching is simple, you just need to change the commitlog_directory parameter in your scylla.yaml file.","title":"(not a tip) Move the commitlog to another disk / partition?"},{"location":"Tech%20Blog/2019/2019-04-17-meet-the-py3status-logo/","text":"I'm proud and very pleased to introduce the py3status logo that Tobaloidee has created for our beloved project! We've been discussing and dreaming about this for a while in the dedicated logo issue . So when Tobaloidee came with his awesome concept and I first saw the logo I was amazed at how he perfectly gave life to the poor brief that I expressed. Concept \u00b6 Thanks again Tobaloidee and of course all of the others who participated (with a special mention to @cyrinux's girlfriend)! Variants \u00b6 We have a few other variants that exist, I'm putting some of them here for quick download & use.","title":"Meet the py3status logo"},{"location":"Tech%20Blog/2019/2019-04-17-meet-the-py3status-logo/#concept","text":"Thanks again Tobaloidee and of course all of the others who participated (with a special mention to @cyrinux's girlfriend)!","title":"Concept"},{"location":"Tech%20Blog/2019/2019-04-17-meet-the-py3status-logo/#variants","text":"We have a few other variants that exist, I'm putting some of them here for quick download & use.","title":"Variants"},{"location":"Tech%20Blog/2019/2019-09-22-py3status-v3-20-europython-2019-edition/","text":"Shame on me to post this so long after it happened... Still, that's a funny story to tell and a lot of thank you to give so let's go! The py3status EuroPython 2019 sprint \u00b6 I've attended all EuroPython conferences since 2013. It's a great event and I encourage everyone to get there! The last two days of the conference week are meant for Open Source projects collaboration: this is called sprints . I don't know why but this year I decided that I would propose a sprint to welcome anyone willing to work on py3status to come and help... To be honest I was expecting that nobody would be interested so when I sat down at an empty table on saturday I thought that it would remain empty... but hey, I would have worked on py3status anyway so every option was okay! Then two students came. They ran Windows and Mac OS and never heard of i3wm or py3status but were curious so I showed them. They could read C so I asked them if they could understand how i3status was reading its horrible configuration file... and they did! Then Oliver Bestwalter (main maintainer of tox) came and told me he was a long time py3status user... followed by Hubert Bry\u0142kowski and \u00d3lafur Bjarni! Wow.. We joined forces to create a py3status module that allows the use of the great PewPew hardware device created by Radomir Dopieralski (which was given to all attendees) to control i3! And we did it and had a lot of fun! https://www.youtube.com/watch?v=0Oy2CE2GZ7s Oliver's major contribution \u00b6 The module itself is awesome okay... but thanks to Oliver's experience with tox he proposed and contributed one of the most significant feature py3status has had: the ability to import modules from other pypi packages ! The idea is that you have your module or set of modules. Instead of having to contribute them to py3status you could just publish them to pypi and py3status will automatically be able to detect and load them! The usage of entry points allow custom and more distributed modules creation for our project! Read more about this amazing feature on the docs. All of this happened during EuroPython 2019 and I want to extend once again my gratitude to everyone who participated! Thank you contributors \u00b6 Version 3.20 is also the work of cool contributors. See the changelog . Daniel Peukert Kevin Pulo Maxim Baz Piotr Miller Rodrigo Leite lasers luto","title":"py3status v3.20 - EuroPython 2019 edition"},{"location":"Tech%20Blog/2019/2019-09-22-py3status-v3-20-europython-2019-edition/#the-py3status-europython-2019-sprint","text":"I've attended all EuroPython conferences since 2013. It's a great event and I encourage everyone to get there! The last two days of the conference week are meant for Open Source projects collaboration: this is called sprints . I don't know why but this year I decided that I would propose a sprint to welcome anyone willing to work on py3status to come and help... To be honest I was expecting that nobody would be interested so when I sat down at an empty table on saturday I thought that it would remain empty... but hey, I would have worked on py3status anyway so every option was okay! Then two students came. They ran Windows and Mac OS and never heard of i3wm or py3status but were curious so I showed them. They could read C so I asked them if they could understand how i3status was reading its horrible configuration file... and they did! Then Oliver Bestwalter (main maintainer of tox) came and told me he was a long time py3status user... followed by Hubert Bry\u0142kowski and \u00d3lafur Bjarni! Wow.. We joined forces to create a py3status module that allows the use of the great PewPew hardware device created by Radomir Dopieralski (which was given to all attendees) to control i3! And we did it and had a lot of fun! https://www.youtube.com/watch?v=0Oy2CE2GZ7s","title":"The py3status EuroPython 2019 sprint"},{"location":"Tech%20Blog/2019/2019-09-22-py3status-v3-20-europython-2019-edition/#olivers-major-contribution","text":"The module itself is awesome okay... but thanks to Oliver's experience with tox he proposed and contributed one of the most significant feature py3status has had: the ability to import modules from other pypi packages ! The idea is that you have your module or set of modules. Instead of having to contribute them to py3status you could just publish them to pypi and py3status will automatically be able to detect and load them! The usage of entry points allow custom and more distributed modules creation for our project! Read more about this amazing feature on the docs. All of this happened during EuroPython 2019 and I want to extend once again my gratitude to everyone who participated!","title":"Oliver's major contribution"},{"location":"Tech%20Blog/2019/2019-09-22-py3status-v3-20-europython-2019-edition/#thank-you-contributors","text":"Version 3.20 is also the work of cool contributors. See the changelog . Daniel Peukert Kevin Pulo Maxim Baz Piotr Miller Rodrigo Leite lasers luto","title":"Thank you contributors"},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/","text":"I've had the pleasure to attend again and present at the Scylla Summit in San Francisco and the honor to be awarded the Most innovative use case of Scylla . It was a great event, full of friendly people and passionate conversations. Peter did a great full write-up of it already so I wanted to share some of my notes instead... This a curated set of topics that I happened to question or discuss in depth so this post is not meant to be taken as a full coverage of the conference. Scylla Manager version 2 \u00b6 The upcoming version of scylla-manager is dropping its dependency on SSH setup which will be replaced by an agent, most likely shipped as a separate package. On the features side, I was a bit puzzled by the fact that ScyllaDB is advertising that its manager will provide a repair scheduling window so that you can control when it's running or not. Why did it struck me you ask? Because MongoDB does the same thing within its balancer process and I always thought of this as a patch to a feature that the database should be able to cope with by itself. And that database-do-it-better-than-you motto is exactly one of the promises of Scylla, the boring database, so smart at handling workload impacts on performance that you shouldn't have to start playing tricks to mitigate them... I don't want this time window feature on scylla-manager to be a trojan horse on the demise of that promise! Kubernetes \u00b6 They almost got late on this but are working hard to play well with the new toy of every tech around the world. Helm charts are also being worked on! The community developed scylla operator by Yannis is now being worked on and backed by ScyllaDB. It can deploy, scale up and down a cluster. Few things to note: it's using a configmap to store the scylla config no TLS support yet no RBAC support yet kubernetes networking is lighter on the network performance hit that was seen on Docker use placement strategies to dedicate kubernetes nodes to scylla! Change Data Capture \u00b6 Oh boy this one was awaited... but it's now coming soon! I inquired about it's performance impact since every operation will be written to a table. Clearly my questioning was a bit alpha since CDC is still being worked on. I had the chance to discuss ideas with Kamil, Tzach and Dor: one of the thing that one of my colleague Julien asked for was the ability for the CDC to generate an event when a tombstone is written so we could actually know when a specific data expired ! I want to stress a few other things too: default TTL on CDC table is 24H expect I/O impact (logical) TTL tombstones can have a hidden disk space cost and nobody was able to tell me if the CDC table was going to be configured with a lower gc_grace_period than the default 10 days so that's something we need to keep in mind and check for there was no plan to add user information that would allow us to know who actually did the operation, so that's something I asked for because it could be used as a cheap and open source way to get auditing! LightWeight Transactions \u00b6 Another so long awaited feature is also coming from the amazing work and knowledge of Konstantin. We had a great conversation about the differences between the currently worked on Paxos based LWT implementation and the maybe later Raft one. So yes, the first LWT implementation will be using Paxos as a consensus algorithm. This will make the LWT feature very consistent while having it slower that what could be achieved using Raft. That's why ScyllaDB have plans on another implementation that could be faster with less data consistency guarantees. User Defined Functions / Aggregations \u00b6 This one is bringing the Lua language inside Scylla! To be precise, it will be a Lua JIT as its footprint is low and Lua can be cooperative enough but the ScyllaDB people made sure to monitor its violations (when it should yield but does not) and act strongly upon them. I got into implementation details with Avi, this is what I noted: lua function return type is not checked at creation but at execution, so expect runtime errors if your lua code is bad since lua is lightweight, there's no need to assign a core to lua execution I found UDA examples, like top-k rows, to be very similar to the Map/Reduce logic UDF will allow simpler token range full table scans thanks to syntax sugar there will be memory limits applied to result sets from UDA, and they will be tunable Text search \u00b6 Dejan is the text search guy at ScyllaDB and the one who kindly implemented the LIKE feature we asked for and that will be released in the upcoming 3.2 version. We discussed ideas and projected use cases to make sure that what's going to be worked on will be used! Redis API \u00b6 I've always been frustrated about Redis because while I love the technology I never trusted its clustering and scaling capabilities. What if you could scale your Redis like Scylla without giving up on performance? That's what the implementation of the Redis API backed by Scylla will get us! I'm desperately looking forward to see this happen!","title":"Scylla Summit 2019"},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#scylla-manager-version-2","text":"The upcoming version of scylla-manager is dropping its dependency on SSH setup which will be replaced by an agent, most likely shipped as a separate package. On the features side, I was a bit puzzled by the fact that ScyllaDB is advertising that its manager will provide a repair scheduling window so that you can control when it's running or not. Why did it struck me you ask? Because MongoDB does the same thing within its balancer process and I always thought of this as a patch to a feature that the database should be able to cope with by itself. And that database-do-it-better-than-you motto is exactly one of the promises of Scylla, the boring database, so smart at handling workload impacts on performance that you shouldn't have to start playing tricks to mitigate them... I don't want this time window feature on scylla-manager to be a trojan horse on the demise of that promise!","title":"Scylla Manager version 2"},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#kubernetes","text":"They almost got late on this but are working hard to play well with the new toy of every tech around the world. Helm charts are also being worked on! The community developed scylla operator by Yannis is now being worked on and backed by ScyllaDB. It can deploy, scale up and down a cluster. Few things to note: it's using a configmap to store the scylla config no TLS support yet no RBAC support yet kubernetes networking is lighter on the network performance hit that was seen on Docker use placement strategies to dedicate kubernetes nodes to scylla!","title":"Kubernetes"},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#change-data-capture","text":"Oh boy this one was awaited... but it's now coming soon! I inquired about it's performance impact since every operation will be written to a table. Clearly my questioning was a bit alpha since CDC is still being worked on. I had the chance to discuss ideas with Kamil, Tzach and Dor: one of the thing that one of my colleague Julien asked for was the ability for the CDC to generate an event when a tombstone is written so we could actually know when a specific data expired ! I want to stress a few other things too: default TTL on CDC table is 24H expect I/O impact (logical) TTL tombstones can have a hidden disk space cost and nobody was able to tell me if the CDC table was going to be configured with a lower gc_grace_period than the default 10 days so that's something we need to keep in mind and check for there was no plan to add user information that would allow us to know who actually did the operation, so that's something I asked for because it could be used as a cheap and open source way to get auditing!","title":"Change Data Capture"},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#lightweight-transactions","text":"Another so long awaited feature is also coming from the amazing work and knowledge of Konstantin. We had a great conversation about the differences between the currently worked on Paxos based LWT implementation and the maybe later Raft one. So yes, the first LWT implementation will be using Paxos as a consensus algorithm. This will make the LWT feature very consistent while having it slower that what could be achieved using Raft. That's why ScyllaDB have plans on another implementation that could be faster with less data consistency guarantees.","title":"LightWeight Transactions"},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#user-defined-functions-aggregations","text":"This one is bringing the Lua language inside Scylla! To be precise, it will be a Lua JIT as its footprint is low and Lua can be cooperative enough but the ScyllaDB people made sure to monitor its violations (when it should yield but does not) and act strongly upon them. I got into implementation details with Avi, this is what I noted: lua function return type is not checked at creation but at execution, so expect runtime errors if your lua code is bad since lua is lightweight, there's no need to assign a core to lua execution I found UDA examples, like top-k rows, to be very similar to the Map/Reduce logic UDF will allow simpler token range full table scans thanks to syntax sugar there will be memory limits applied to result sets from UDA, and they will be tunable","title":"User Defined Functions / Aggregations"},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#text-search","text":"Dejan is the text search guy at ScyllaDB and the one who kindly implemented the LIKE feature we asked for and that will be released in the upcoming 3.2 version. We discussed ideas and projected use cases to make sure that what's going to be worked on will be used!","title":"Text search"},{"location":"Tech%20Blog/2019/2019-12-28-scylla-summit-2019/#redis-api","text":"I've always been frustrated about Redis because while I love the technology I never trusted its clustering and scaling capabilities. What if you could scale your Redis like Scylla without giving up on performance? That's what the implementation of the Redis API backed by Scylla will get us! I'm desperately looking forward to see this happen!","title":"Redis API"},{"location":"Tech%20Blog/2020/2020-04-14-py3status-v3-28-goodbye-py2-6-3-4/","text":"The newest version of py3status starts to enforce the deprecation of Python 2.6 to 3.4 (included) initiated by Thiago Kenji Okada more than a year ago and orchestrated by Hugo van Kemenade via #1904 and #1896 . Thanks to Hugo, I discovered a nice tool by @ asottile to update your Python code base to recent syntax sugars called pyupgrade ! Debian buster users might be interested in the installation war story that @ TRS-80 kindly described and the final (and documented) solution found. Changelog since v3.26 \u00b6 drop support for EOL Python 2.6-3.4 (#1896), by Hugo van Kemenade i3status: support read_file module (#1909), by @lasers thx to @dohseven clock module: add \"locale\" config parameter to change time representation (#1910), by inemajo docs: update debian instructions fix #1916 mpd_status module: use currentsong command if possible (#1924), by girst networkmanager module: allow using the currently active AP in formats (#1921), by Beno\u00eet Dardenne volume_status module: change amixer flag ordering fix #1914 (#1920) Thank you contributors \u00b6 Thiago Kenji Okada Hugo van Kemenade Beno\u00eet Dardenne @dohseven @inemajo @girst @lasers","title":"py3status v3.28 - goodbye py2.6-3.4"},{"location":"Tech%20Blog/2020/2020-04-14-py3status-v3-28-goodbye-py2-6-3-4/#changelog-since-v326","text":"drop support for EOL Python 2.6-3.4 (#1896), by Hugo van Kemenade i3status: support read_file module (#1909), by @lasers thx to @dohseven clock module: add \"locale\" config parameter to change time representation (#1910), by inemajo docs: update debian instructions fix #1916 mpd_status module: use currentsong command if possible (#1924), by girst networkmanager module: allow using the currently active AP in formats (#1921), by Beno\u00eet Dardenne volume_status module: change amixer flag ordering fix #1914 (#1920)","title":"Changelog since v3.26"},{"location":"Tech%20Blog/2020/2020-04-14-py3status-v3-28-goodbye-py2-6-3-4/#thank-you-contributors","text":"Thiago Kenji Okada Hugo van Kemenade Beno\u00eet Dardenne @dohseven @inemajo @girst @lasers","title":"Thank you contributors"},{"location":"Tech%20Blog/2020/2020-08-24-python-scylla-driver-how-we-unleashed-the-scylla-monsters-performance/","text":"At Scylla summit 2019 I had the chance to meet Israel Fruchter and we dreamed of working on adding shard-awareness support to the Python cassandra-driver which would be known as scylla-driver . A few months later, when Israel reached out to me on the Scylla-Users #pythonistas Slack channel with the first draft PR I was so excited that I jumped in the wagon to help! The efforts we put into the scylla-driver paid off and significantly improved the performance of the production applications that I had the chance to switch to using it by 15 to 25%! Before we reached those numbers and even released the scylla-driver to PyPi, EuroPython 2020 RFP was open and I submitted a talk proposal which was luckily accepted by the community. So I had the chance to deep-dive into Cassandra vs Scylla architecture differences, explain the rationale behind creating the scylla-driver and give Python code details on how we implemented it and the challenges we faced doing so. Check my talk spage for I also explained that I wrote an RFC on the scylla-dev mailing list which lead the developers of Scylla to implement a new connection-to-shard algorithm that will allow clients connecting to a new listening port to select the actual shard they want a connection to. This is an expected major optimization from the current mostly random way of connecting to all shards and I'm happy to say that it's been implemented and is ready to be put to use by all the scylla drivers. I've recently been contacted by PyCon India and other Python related conferences organizers for a talk so I've submitted one to PyCon India where I hope I'll be able to showcase even better numbers thanks to the new algorithm! After my Europython talk we also had very interesting discussions with Pau Freixes about his work on a fully asynchronous Python driver that wraps the C++ driver to get the best possible performance. First results are absolutely amazing so if you're interested in this, make sure to give it a try and contribute to the driver! Stay tuned for more amazing query latencies ;)","title":"Python scylla-driver: how we unleashed the Scylla monster's performance"},{"location":"Tech%20Blog/2020/2020-09-07-py3status-v3-29/","text":"Almost 5 months after the latest release (thank you COVID) I'm pleased and relieved to have finally packaged and pushed py3status v3.29 to PyPi and Gentoo portage! This release comes with a lot of interesting contributions from quite a bunch of first-time contributors so I thought that I'd thank them first for a change! Thank you contributors! \u00b6 Jacotsu lasers Marc Poulhi\u00e8s Markus Sommer raphaunix Ricardo P\u00e9rez vmoyankov Wilmer van der Gaast Yaroslav Dronskii So what's new in v3.29? \u00b6 Two new exciting modules are in! prometheus module: to display your promQL queries on your bar watson module: for the watson time-tracking tool Then some interesting bug fixes and enhancements are to be noted py3.requests: return empty json on remote server problem fix #1401 core modules: remove deprectated function, fix type annotation support (#1942) Some modules also got improved battery_level module: add power consumption placeholder (#1939) + support more battery paths detection (#1946) do_not_disturb module: change pause default from False to True mpris module: implement broken chromium mpris interface workaround (#1943) sysdata module: add {mem,swap}_free, {mem,swap}_free_unit, {mem,swap}_free_percent + try to use default intel/amd sensors first google_calendar module: fix imports for newer google-python-client-api versions (#1948) Next version of py3status will certainly drop support for EOL Python 3.5!","title":"py3status v3.29"},{"location":"Tech%20Blog/2020/2020-09-07-py3status-v3-29/#thank-you-contributors","text":"Jacotsu lasers Marc Poulhi\u00e8s Markus Sommer raphaunix Ricardo P\u00e9rez vmoyankov Wilmer van der Gaast Yaroslav Dronskii","title":"Thank you contributors!"},{"location":"Tech%20Blog/2020/2020-09-07-py3status-v3-29/#so-whats-new-in-v329","text":"Two new exciting modules are in! prometheus module: to display your promQL queries on your bar watson module: for the watson time-tracking tool Then some interesting bug fixes and enhancements are to be noted py3.requests: return empty json on remote server problem fix #1401 core modules: remove deprectated function, fix type annotation support (#1942) Some modules also got improved battery_level module: add power consumption placeholder (#1939) + support more battery paths detection (#1946) do_not_disturb module: change pause default from False to True mpris module: implement broken chromium mpris interface workaround (#1943) sysdata module: add {mem,swap}_free, {mem,swap}_free_unit, {mem,swap}_free_percent + try to use default intel/amd sensors first google_calendar module: fix imports for newer google-python-client-api versions (#1948) Next version of py3status will certainly drop support for EOL Python 3.5!","title":"So what's new in v3.29?"},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/","text":"We have recently faced a problem where some of the first Scylla tables we created on our main production cluster were not in line any more with the evolved schemas that recent tables are using. This typical engineering problem requires either to keep those legacy tables and data queries or to migrate it to the more optimal model with the bandwagon of applications to be modified to query the data the new way... That's something nobody likes doing but hey, we don't like legacy at Numberly so let's kill that one! To overcome this challenge we used the scylla-migrator project and I thought it could be useful to share this experience. How and why our schema evolved \u00b6 When we first approached ID matching tables we chose to answer two problems at the same time: query the most recent data and keep the history of the changes per source ID. This means that those tables included a date as part of their PRIMARY KEY while the partition key was obviously the matching table ID we wanted to lookup from: CREATE TABLE IF NOT EXISTS ids_by_partnerid( partnerid text, id text, date timestamp, PRIMARY KEY ((partnerid), date, id) ) WITH CLUSTERING ORDER BY (date DESC) Making a table with an ever changing date in the clustering key creates what we call a history table . In the schema above the uniqueness of a row is not only defined by a partner_id / id couple but also by its date! Quick caveat: you have to be careful about the actual date timestamp resolution since you may not want to create a row for every second of the same partner_id / id couple (we use an hour resolution). History tables are good for analytics and we also figured we could use them for batch and real time queries where we would be interested in the \" most recent ids for the given partner_id \" (sometimes flavored with a LIMIT): SELECT id FROM ids_by_partnerid WHERE partner_id = \"AXZAZLKDJ\" ORDER BY date DESC; As time passed, real time Kafka pipelines started to query these tables hard and were mostly interested in \" all the ids known for the given partner_id \". A sort of DISTINCT(id) is out of the scope of our table! For this we need a table schema that represents a condensed view of the data. We call them compact tables and the only difference with the history table is that the date timestamp is simply not part of the PRIMARY KEY: CREATE TABLE IF NOT EXISTS ids_by_partnerid( partnerid text, id text, seen_date timestamp, PRIMARY KEY ((partnerid), id) ) To make that transition happen we thus wanted to: rename history tables with an __history_ suffix so that they are clearly identified as such get a compacted version of the tables (by keeping their old name) while renaming the date column name to seen_date do it as fast as possible since we will need to stop our feeding pipeline and most of our applications during the process... STOP: it's not possible to rename a table in CQL! Scylla-migrator to the rescue \u00b6 We decided to abuse the scylla-migrator to perform this perilous migration. As it was originally designed to help users migrate from Cassandra to Scylla by leveraging Spark it seemed like a good fit for the task since we happen to own Spark clusters powered by Hadoop YARN. Building scylla-migrator for Spark < 2.4 \u00b6 Recent scylla-migrator does not support older Spark versions. The trick is to look at the README.md git log and checkout the hopefully right commit that supports your Spark cluster version. In our case for Spark 2.3 we used git commit bc82a57e4134452f19a11cd127bd4c6a25f75020. On Gentoo, make sure to use dev-java/sbt-bin since the non binary version is vastly out of date and won't build the project. You need at least version 1.3. The scylla-migrator plan \u00b6 The documentation explains that we need a config file that points to a source cluster+table and a destination cluster+table as long as they have the same schema structure... Renaming is then as simple as duplicating the schema using CQLSH and running the migrator! But what about our compacted version of our original table? The schema is different from the source table!... Good news is that as long as all your columns remain present, you can also change the PRIMARY KEY of your destination table and it will still work! This make the scylla-migrator an amazing tool to reshape or pivot tables! the column date is renamed to seen_date : that's okay, scylla-migrator supports column renaming (it's a Spark dataframe after all)! the PRIMARY KEY is different in the compacted table since we removed the 'date ' from the clustering columns: we'll get a compacted table for free! Using scylla-migrator \u00b6 The documentation is a bit poor on how to submit your application to a Hadoop YARN cluster but that's kind of expected. It also did not mention how to connect to a SSL enabled cluster (are there people really not using SSL on the wire in their production environment?)... anyway let's not start a flame war :) The trick that will save you is to know that you can append all the usual Spark options that are available in the spark-cassandra-connector ! Submitting to a Kerberos protected Hadoop YARN cluster targeting a SSL enabled Scylla cluster then looks like this: export JAR_NAME=target/scala-2.11/scylla-migrator-assembly-0.0.1.jar export KRB_PRINCIPAL=USERNAME spark2-submit \\ --name ScyllaMigratorApplication \\ --class com.scylladb.migrator.Migrator \\ --conf spark.cassandra.connection.ssl.clientAuth.enabled=True \\ --conf spark.cassandra.connection.ssl.enabled=True \\ --conf spark.cassandra.connection.ssl.trustStore.path=jssecacerts \\ --conf spark.cassandra.connection.ssl.trustStore.password=JKS_PASSWORD \\ --conf spark.cassandra.input.consistency.level=LOCAL_QUORUM \\ --conf spark.cassandra.output.consistency.level=LOCAL_QUORUM \\ --conf spark.scylla.config=config.yaml \\ --conf spark.yarn.executor.memoryOverhead=1g \\ --conf spark.blacklist.enabled=true \\ --conf spark.blacklist.task.maxTaskAttemptsPerExecutor=1 \\ --conf spark.blacklist.task.maxTaskAttemptsPerNode=1 \\ --conf spark.blacklist.stage.maxFailedTasksPerExecutor=1 \\ --conf spark.blacklist.stage.maxFailedExecutorsPerNode=1 \\ --conf spark.executor.cores=16 \\ --deploy-mode client \\ --files jssecacerts \\ --jars ${JAR_NAME} \\ --keytab ${KRB_PRINCIPAL}.keytab \\ --master yarn \\ --principal ${KRB_PRINCIPAL} \\ ${JAR_NAME} Note that we chose to apply a higher consistency level to our reads using a LOCAL_QUORUM instead of the default LOCAL_ONE. I strongly encourage you to do the same since it's appropriate when you're using this kind of tool! Column renaming is simply expressed in the configuration file like this: # Column renaming configuration. renames: - from: date to: seen_date Tuning scylla-migrator \u00b6 While easy to use, tuning scylla-migrator to operate those migrations as fast as possible turned out to be a real challenge (remember we have some production applications shut down during the process). Even using 300+ Spark executors I couldn't get my Scylla cluster utilization to more than 50% and migrating a single table with a bit more than 1B rows took almost 2 hours... We found the best knobs to play with thanks to the help of Lubos Kosco and this blog post from ScyllaDB : Increase the splitCount setting: more splits means more Spark executors will be spawned and more tasks out of it. While it might be magic on a pure Spark deployment it's not that amazing on a Hadoop YARN one where executors are scheduled in containers with 1 core by default. We simply moved it from 256 to 384. Disable compaction on destination tables schemas . This gave us a big boost and saved the day since it avoids adding the overhead of compacting while you're pushing down data hard! To disable compaction on a table simply: ALTER TABLE ids_by_partnerid_history WITH compaction = {'class': 'NullCompactionStrategy'}; Remember to run a manual compaction using nodetool compact <keyspace> <table> and to enable compaction back on your tables once you're done! Happy Scylla tables mangling!","title":"Renaming and reshaping Scylla tables using scylla-migrator"},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#how-and-why-our-schema-evolved","text":"When we first approached ID matching tables we chose to answer two problems at the same time: query the most recent data and keep the history of the changes per source ID. This means that those tables included a date as part of their PRIMARY KEY while the partition key was obviously the matching table ID we wanted to lookup from: CREATE TABLE IF NOT EXISTS ids_by_partnerid( partnerid text, id text, date timestamp, PRIMARY KEY ((partnerid), date, id) ) WITH CLUSTERING ORDER BY (date DESC) Making a table with an ever changing date in the clustering key creates what we call a history table . In the schema above the uniqueness of a row is not only defined by a partner_id / id couple but also by its date! Quick caveat: you have to be careful about the actual date timestamp resolution since you may not want to create a row for every second of the same partner_id / id couple (we use an hour resolution). History tables are good for analytics and we also figured we could use them for batch and real time queries where we would be interested in the \" most recent ids for the given partner_id \" (sometimes flavored with a LIMIT): SELECT id FROM ids_by_partnerid WHERE partner_id = \"AXZAZLKDJ\" ORDER BY date DESC; As time passed, real time Kafka pipelines started to query these tables hard and were mostly interested in \" all the ids known for the given partner_id \". A sort of DISTINCT(id) is out of the scope of our table! For this we need a table schema that represents a condensed view of the data. We call them compact tables and the only difference with the history table is that the date timestamp is simply not part of the PRIMARY KEY: CREATE TABLE IF NOT EXISTS ids_by_partnerid( partnerid text, id text, seen_date timestamp, PRIMARY KEY ((partnerid), id) ) To make that transition happen we thus wanted to: rename history tables with an __history_ suffix so that they are clearly identified as such get a compacted version of the tables (by keeping their old name) while renaming the date column name to seen_date do it as fast as possible since we will need to stop our feeding pipeline and most of our applications during the process... STOP: it's not possible to rename a table in CQL!","title":"How and why our schema evolved"},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#scylla-migrator-to-the-rescue","text":"We decided to abuse the scylla-migrator to perform this perilous migration. As it was originally designed to help users migrate from Cassandra to Scylla by leveraging Spark it seemed like a good fit for the task since we happen to own Spark clusters powered by Hadoop YARN.","title":"Scylla-migrator to the rescue"},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#building-scylla-migrator-for-spark-24","text":"Recent scylla-migrator does not support older Spark versions. The trick is to look at the README.md git log and checkout the hopefully right commit that supports your Spark cluster version. In our case for Spark 2.3 we used git commit bc82a57e4134452f19a11cd127bd4c6a25f75020. On Gentoo, make sure to use dev-java/sbt-bin since the non binary version is vastly out of date and won't build the project. You need at least version 1.3.","title":"Building scylla-migrator for Spark &lt; 2.4"},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#the-scylla-migrator-plan","text":"The documentation explains that we need a config file that points to a source cluster+table and a destination cluster+table as long as they have the same schema structure... Renaming is then as simple as duplicating the schema using CQLSH and running the migrator! But what about our compacted version of our original table? The schema is different from the source table!... Good news is that as long as all your columns remain present, you can also change the PRIMARY KEY of your destination table and it will still work! This make the scylla-migrator an amazing tool to reshape or pivot tables! the column date is renamed to seen_date : that's okay, scylla-migrator supports column renaming (it's a Spark dataframe after all)! the PRIMARY KEY is different in the compacted table since we removed the 'date ' from the clustering columns: we'll get a compacted table for free!","title":"The scylla-migrator plan"},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#using-scylla-migrator","text":"The documentation is a bit poor on how to submit your application to a Hadoop YARN cluster but that's kind of expected. It also did not mention how to connect to a SSL enabled cluster (are there people really not using SSL on the wire in their production environment?)... anyway let's not start a flame war :) The trick that will save you is to know that you can append all the usual Spark options that are available in the spark-cassandra-connector ! Submitting to a Kerberos protected Hadoop YARN cluster targeting a SSL enabled Scylla cluster then looks like this: export JAR_NAME=target/scala-2.11/scylla-migrator-assembly-0.0.1.jar export KRB_PRINCIPAL=USERNAME spark2-submit \\ --name ScyllaMigratorApplication \\ --class com.scylladb.migrator.Migrator \\ --conf spark.cassandra.connection.ssl.clientAuth.enabled=True \\ --conf spark.cassandra.connection.ssl.enabled=True \\ --conf spark.cassandra.connection.ssl.trustStore.path=jssecacerts \\ --conf spark.cassandra.connection.ssl.trustStore.password=JKS_PASSWORD \\ --conf spark.cassandra.input.consistency.level=LOCAL_QUORUM \\ --conf spark.cassandra.output.consistency.level=LOCAL_QUORUM \\ --conf spark.scylla.config=config.yaml \\ --conf spark.yarn.executor.memoryOverhead=1g \\ --conf spark.blacklist.enabled=true \\ --conf spark.blacklist.task.maxTaskAttemptsPerExecutor=1 \\ --conf spark.blacklist.task.maxTaskAttemptsPerNode=1 \\ --conf spark.blacklist.stage.maxFailedTasksPerExecutor=1 \\ --conf spark.blacklist.stage.maxFailedExecutorsPerNode=1 \\ --conf spark.executor.cores=16 \\ --deploy-mode client \\ --files jssecacerts \\ --jars ${JAR_NAME} \\ --keytab ${KRB_PRINCIPAL}.keytab \\ --master yarn \\ --principal ${KRB_PRINCIPAL} \\ ${JAR_NAME} Note that we chose to apply a higher consistency level to our reads using a LOCAL_QUORUM instead of the default LOCAL_ONE. I strongly encourage you to do the same since it's appropriate when you're using this kind of tool! Column renaming is simply expressed in the configuration file like this: # Column renaming configuration. renames: - from: date to: seen_date","title":"Using scylla-migrator"},{"location":"Tech%20Blog/2020/2020-11-06-renaming-and-reshaping-scylla-tables-using-scylla-migrator/#tuning-scylla-migrator","text":"While easy to use, tuning scylla-migrator to operate those migrations as fast as possible turned out to be a real challenge (remember we have some production applications shut down during the process). Even using 300+ Spark executors I couldn't get my Scylla cluster utilization to more than 50% and migrating a single table with a bit more than 1B rows took almost 2 hours... We found the best knobs to play with thanks to the help of Lubos Kosco and this blog post from ScyllaDB : Increase the splitCount setting: more splits means more Spark executors will be spawned and more tasks out of it. While it might be magic on a pure Spark deployment it's not that amazing on a Hadoop YARN one where executors are scheduled in containers with 1 core by default. We simply moved it from 256 to 384. Disable compaction on destination tables schemas . This gave us a big boost and saved the day since it avoids adding the overhead of compacting while you're pushing down data hard! To disable compaction on a table simply: ALTER TABLE ids_by_partnerid_history WITH compaction = {'class': 'NullCompactionStrategy'}; Remember to run a manual compaction using nodetool compact <keyspace> <table> and to enable compaction back on your tables once you're done! Happy Scylla tables mangling!","title":"Tuning scylla-migrator"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/","text":"From Wordpress to MkDocs \u00b6 The idea of a statically built website has been in my mind for a while but I never found a satisfying stack to make it happen until I discovered and got intimate with MkDocs . Building a static website out of any kind of formatted text file neither new nor hard to do. But when I was interested in the subject a while ago, the ecosystem to support it and make it useful was not as mature as it is today. My website stack wish list \u00b6 What I wanted was the ability to: build a responsive website out of simple text formatted files have a versioned, historized and Open Source view of my website sources on git work on different kind of content, from simple pages to blog posts to tutorials test it locally deploy and host it seamlessly (and at no cost if possible) have everything automated (except the actual writing, ok!) Bonus was to be able to: have the possibility to add dynamic content in the build process MkDocs? \u00b6 Yep that means that I got the crazy idea of using a project initially designed to ease the creation of technical documentation of projects for my website. Quote MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introduction below, then check the User Guide for more info. After you swallow the possible shock or fear, let's reconsider my stack wish list above... After all, a git based website made out of text files that is built by a CI and hosted through CD looks exactly the same to me as a technical documentation build and hosting process! All it needs is some nice sugar to accomodate with specific needs. Converting my Wordpress content to Markdown \u00b6 I used the nice wordpress export to markdown project which did the work perfectly for me as I could run the export to output a file structure fitting my mkdocs file hierarchy needs. The ultrabug.fr stack \u00b6 Git \u00b6 Let's start with the obvious git to get revision control over the sources of the website. Be it text files, media files and configuration files: everything is on git! MkDocs \u00b6 Now MkDocs comes into play as it offers a straightforward way to structurate and configure the resulting website. By itself it does not offer all the features I needed. For example, a blog section needs to sort articles by the \"most recent\" first while some other sections of the website simply need alphabetical ordering. Wait, what if this particular sub-section you wanted first? What about emojis or nice thumbnails? Here is a list of what I'm using to enrich MkDocs and accomodate my needs: mkdocs-material : this is a responsive and good looking theme for MkDocs that offers some nice features to present your content in a lean way. mkdocs-redirects : I did not want to break my old links so I'm using this mkdocs plugin to make sure that my old Wordpress content is still redirected to the new mkdocs structure URLs of this website. python markdown extensions : to have nice markdown extensions like these checkboxes and of course emojis mkdocs-awesome-pages-plugin : this one is the ultimate plugin if like me you need to control the ordering of your navigation! Multi-language support \u00b6 MkDocs does not support language localization and I wanted to be able to propose some of my website sections and content in my mother tongue (French) and in English. My first attempt was to simply add a and flag followed by the localized version of my content on a single page. It was okay, not great but okay enough so I could live with it at start. It did not last long since my friend @Lujeni immediately told me that it would be better without all these flags flying around a same page... So I ended up writing a mkdocs plugin to support pages localization easily! This work took me a bit (too much?) further and I am now also working on adding theme localization support to the whole mkdocs project ! Github \u00b6 Last but not least, I use Github workflow Actions + Pages to build and host my website now! Take a leap, go static! \u00b6 I hope this article inspires you to try and use these cool projects","title":"From Wordpress to MkDocs"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#from-wordpress-to-mkdocs","text":"The idea of a statically built website has been in my mind for a while but I never found a satisfying stack to make it happen until I discovered and got intimate with MkDocs . Building a static website out of any kind of formatted text file neither new nor hard to do. But when I was interested in the subject a while ago, the ecosystem to support it and make it useful was not as mature as it is today.","title":"From Wordpress to MkDocs"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#my-website-stack-wish-list","text":"What I wanted was the ability to: build a responsive website out of simple text formatted files have a versioned, historized and Open Source view of my website sources on git work on different kind of content, from simple pages to blog posts to tutorials test it locally deploy and host it seamlessly (and at no cost if possible) have everything automated (except the actual writing, ok!) Bonus was to be able to: have the possibility to add dynamic content in the build process","title":"My website stack wish list"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#mkdocs","text":"Yep that means that I got the crazy idea of using a project initially designed to ease the creation of technical documentation of projects for my website. Quote MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introduction below, then check the User Guide for more info. After you swallow the possible shock or fear, let's reconsider my stack wish list above... After all, a git based website made out of text files that is built by a CI and hosted through CD looks exactly the same to me as a technical documentation build and hosting process! All it needs is some nice sugar to accomodate with specific needs.","title":"MkDocs?"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#converting-my-wordpress-content-to-markdown","text":"I used the nice wordpress export to markdown project which did the work perfectly for me as I could run the export to output a file structure fitting my mkdocs file hierarchy needs.","title":"Converting my Wordpress content to Markdown"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#the-ultrabugfr-stack","text":"","title":"The ultrabug.fr stack"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#git","text":"Let's start with the obvious git to get revision control over the sources of the website. Be it text files, media files and configuration files: everything is on git!","title":"Git"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#mkdocs_1","text":"Now MkDocs comes into play as it offers a straightforward way to structurate and configure the resulting website. By itself it does not offer all the features I needed. For example, a blog section needs to sort articles by the \"most recent\" first while some other sections of the website simply need alphabetical ordering. Wait, what if this particular sub-section you wanted first? What about emojis or nice thumbnails? Here is a list of what I'm using to enrich MkDocs and accomodate my needs: mkdocs-material : this is a responsive and good looking theme for MkDocs that offers some nice features to present your content in a lean way. mkdocs-redirects : I did not want to break my old links so I'm using this mkdocs plugin to make sure that my old Wordpress content is still redirected to the new mkdocs structure URLs of this website. python markdown extensions : to have nice markdown extensions like these checkboxes and of course emojis mkdocs-awesome-pages-plugin : this one is the ultimate plugin if like me you need to control the ordering of your navigation!","title":"MkDocs"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#multi-language-support","text":"MkDocs does not support language localization and I wanted to be able to propose some of my website sections and content in my mother tongue (French) and in English. My first attempt was to simply add a and flag followed by the localized version of my content on a single page. It was okay, not great but okay enough so I could live with it at start. It did not last long since my friend @Lujeni immediately told me that it would be better without all these flags flying around a same page... So I ended up writing a mkdocs plugin to support pages localization easily! This work took me a bit (too much?) further and I am now also working on adding theme localization support to the whole mkdocs project !","title":"Multi-language support"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#github","text":"Last but not least, I use Github workflow Actions + Pages to build and host my website now!","title":"Github"},{"location":"Tech%20Blog/2021/2021-03-12-from-wordpress-to-mkdocs/#take-a-leap-go-static","text":"I hope this article inspires you to try and use these cool projects","title":"Take a leap, go static!"},{"location":"Tech%20Talks/","text":"Community talks and awards \u00b6 I've been fortunate since some of my talk proposals have been accepted over the last years at tech conferences where I got to participate as a speaker. I've also been very humbled to be interviewed and awarded for my community work and professional experience.","title":"Community talks and awards"},{"location":"Tech%20Talks/#community-talks-and-awards","text":"I've been fortunate since some of my talk proposals have been accepted over the last years at tech conferences where I got to participate as a speaker. I've also been very humbled to be interviewed and awarded for my community work and professional experience.","title":"Community talks and awards"},{"location":"Tech%20Talks/2013/","text":"EuroPython 2013 \u00b6 I gave an informal but still recorded talk about uWSGI + gevent to build a fault tolerant application (my first one ever). Watch the video","title":"EuroPython 2013"},{"location":"Tech%20Talks/2013/#europython-2013","text":"I gave an informal but still recorded talk about uWSGI + gevent to build a fault tolerant application (my first one ever). Watch the video","title":"EuroPython 2013"},{"location":"Tech%20Talks/2014/","text":"Paris.py 5 \u00b6 A talk about fault tolerance in application design (in French). Watch the video","title":"Paris.py 5"},{"location":"Tech%20Talks/2014/#parispy-5","text":"A talk about fault tolerance in application design (in French). Watch the video","title":"Paris.py 5"},{"location":"Tech%20Talks/2015/","text":"EuroPython 2015 \u00b6 Designing a scalable and distributed application \u00b6 Download the slides Watch the video One of the key aspect to keep in mind when developing a scalable application is its faculty to grow easily. But while we're used to take advantage of scalable backend technologies such as mongodb or couchbase, scaling automatically our own application core is usually another story. In this talk I will explain and showcase a distributed web application design based on consul and uWSGI and its consul plugin. This design will cover the key components of a distributed and scalable application : Automatic service registration and discovery will allow your application to grow itself automatically. Health checking and service unregistration will allow your application to be fault tolerant, highly available and to shrink itself automatically. A distributed Key/Value storage will allow you to (re)configure your distributed application nodes at once. Multi-Datacenter awareness will allow your application to scale around the world easily.","title":"EuroPython 2015"},{"location":"Tech%20Talks/2015/#europython-2015","text":"","title":"EuroPython 2015"},{"location":"Tech%20Talks/2015/#designing-a-scalable-and-distributed-application","text":"Download the slides Watch the video One of the key aspect to keep in mind when developing a scalable application is its faculty to grow easily. But while we're used to take advantage of scalable backend technologies such as mongodb or couchbase, scaling automatically our own application core is usually another story. In this talk I will explain and showcase a distributed web application design based on consul and uWSGI and its consul plugin. This design will cover the key components of a distributed and scalable application : Automatic service registration and discovery will allow your application to grow itself automatically. Health checking and service unregistration will allow your application to be fault tolerant, highly available and to shrink itself automatically. A distributed Key/Value storage will allow you to (re)configure your distributed application nodes at once. Multi-Datacenter awareness will allow your application to scale around the world easily.","title":"Designing a scalable and distributed application"},{"location":"Tech%20Talks/2016/","text":"EuroPython 2016 \u00b6 Using Service Discovery to build dynamic python applications \u00b6 Download the slides Watch the video This talk will showcase and compare three Service Discovery technologies and their usage to build a dynamic and distributed python application: consul etcd zookeeper After a short introduction to service discovery, we will iterate and compare how we can address the concrete and somewhat complex design of our python application using each technology. We'll then be able to discuss their strengths, weaknesses and python bindings and finally showcase the application in a demo. All the source code will of course be made available for the audience to benefit and start from for their own use ! Planning for the worst (interactive talk, with Guillaume Gelin) \u00b6 Download the slides Watch the video This talk is about sharing our experience about how we handled production problems on all levels of our applications. We'll begin with common problems, errors and failures and dig on to more obscure ones while sharing concrete tips, good practices and code to address them ! This talk will make you feel the warmth of not being alone facing a problem :)","title":"EuroPython 2016"},{"location":"Tech%20Talks/2016/#europython-2016","text":"","title":"EuroPython 2016"},{"location":"Tech%20Talks/2016/#using-service-discovery-to-build-dynamic-python-applications","text":"Download the slides Watch the video This talk will showcase and compare three Service Discovery technologies and their usage to build a dynamic and distributed python application: consul etcd zookeeper After a short introduction to service discovery, we will iterate and compare how we can address the concrete and somewhat complex design of our python application using each technology. We'll then be able to discuss their strengths, weaknesses and python bindings and finally showcase the application in a demo. All the source code will of course be made available for the audience to benefit and start from for their own use !","title":"Using Service Discovery to build dynamic python applications"},{"location":"Tech%20Talks/2016/#planning-for-the-worst-interactive-talk-with-guillaume-gelin","text":"Download the slides Watch the video This talk is about sharing our experience about how we handled production problems on all levels of our applications. We'll begin with common problems, errors and failures and dig on to more obscure ones while sharing concrete tips, good practices and code to address them ! This talk will make you feel the warmth of not being alone facing a problem :)","title":"Planning for the worst (interactive talk, with Guillaume Gelin)"},{"location":"Tech%20Talks/2017/europython2017/","text":"EuroPython 2017 \u00b6 Leveraging consistent hashing in your python applications \u00b6 Download the slides Watch the video While consistent hashing is largely known and adopted in the NoSQL database clusters to solve data distribution and data access reliability, it is less known and used by the typical developers. This talk will introduce you to consistent hashing and the problems it solves while going through a practical use case in a python application .","title":"EuroPython 2017"},{"location":"Tech%20Talks/2017/europython2017/#europython-2017","text":"","title":"EuroPython 2017"},{"location":"Tech%20Talks/2017/europython2017/#leveraging-consistent-hashing-in-your-python-applications","text":"Download the slides Watch the video While consistent hashing is largely known and adopted in the NoSQL database clusters to solve data distribution and data access reliability, it is less known and used by the typical developers. This talk will introduce you to consistent hashing and the problems it solves while going through a practical use case in a python application .","title":"Leveraging consistent hashing in your python applications"},{"location":"Tech%20Talks/2017/scaling-python/","text":"Scaling Python interview (book) \u00b6 I've had the pleasure to be interviewed by Julien Danjou for his latest book called \"scaling python\" . I answered the following questions: Could you introduce yourself and explain how you came to Python? What do you think make Python great (or not) when building distributed systems? What are the things you consider being advantages or drawbacks? What would be your the top N mistakes to avoid or advice/best practice to follow when building scalable, distributed systems using Python?","title":"Scaling Python interview (book)"},{"location":"Tech%20Talks/2017/scaling-python/#scaling-python-interview-book","text":"I've had the pleasure to be interviewed by Julien Danjou for his latest book called \"scaling python\" . I answered the following questions: Could you introduce yourself and explain how you came to Python? What do you think make Python great (or not) when building distributed systems? What are the things you consider being advantages or drawbacks? What would be your the top N mistakes to avoid or advice/best practice to follow when building scalable, distributed systems using Python?","title":"Scaling Python interview (book)"},{"location":"Tech%20Talks/2018/europython-2018/","text":"EuroPython 2018 \u00b6 The rise of Python in the Data communities \u00b6 Download the slides Watch the video A retrospective and prospective of Python\u2019s adoption in the data-driven industries and how it has and should influence its ecosystem and communities. Thanks to its versatility, Python\u2019s usage and adoption has changed a lot over the last decade to go beyond the very act of software programming. From Developers to SysOps, closely followed by Scientists and Data analysts, Python has spread to become a common tongue for a wide range of people. We will start by looking at how this increased adoption impacted Python ecosystem and is still shaping it today. While this talk is not walk through all the Python technologies around data , some of them will be outlined so you will hear words like Numpy, Pandas or Jupyter. Then we will try to project ourselves in the future and by highlighting the pitfalls Python has to overcome to keep up with its pace and mature in its ability to scale !","title":"EuroPython 2018"},{"location":"Tech%20Talks/2018/europython-2018/#europython-2018","text":"","title":"EuroPython 2018"},{"location":"Tech%20Talks/2018/europython-2018/#the-rise-of-python-in-the-data-communities","text":"Download the slides Watch the video A retrospective and prospective of Python\u2019s adoption in the data-driven industries and how it has and should influence its ecosystem and communities. Thanks to its versatility, Python\u2019s usage and adoption has changed a lot over the last decade to go beyond the very act of software programming. From Developers to SysOps, closely followed by Scientists and Data analysts, Python has spread to become a common tongue for a wide range of people. We will start by looking at how this increased adoption impacted Python ecosystem and is still shaping it today. While this talk is not walk through all the Python technologies around data , some of them will be outlined so you will hear words like Numpy, Pandas or Jupyter. Then we will try to project ourselves in the future and by highlighting the pitfalls Python has to overcome to keep up with its pace and mature in its ability to scale !","title":"The rise of Python in the Data communities"},{"location":"Tech%20Talks/2018/pyconfr-2018/","text":"PyConFR 2018 \u00b6 I had the chance to give three talks there, one of them brand new about my experience in evaluating then getting ScyllaDB in production. My journey into joining billions of rows in seconds using ScyllaDB \u00b6 Download the slides Watch the video The rise of Python in the Data communities \u00b6 Download the slides Watch the video Leveraging consistent hashing in your python applications \u00b6 I updated it from my 2017 talk with a digression on Python 3.6 new dict implementation. Download the slides Watch the video","title":"PyConFR 2018"},{"location":"Tech%20Talks/2018/pyconfr-2018/#pyconfr-2018","text":"I had the chance to give three talks there, one of them brand new about my experience in evaluating then getting ScyllaDB in production.","title":"PyConFR 2018"},{"location":"Tech%20Talks/2018/pyconfr-2018/#my-journey-into-joining-billions-of-rows-in-seconds-using-scylladb","text":"Download the slides Watch the video","title":"My journey into joining billions of rows in seconds using ScyllaDB"},{"location":"Tech%20Talks/2018/pyconfr-2018/#the-rise-of-python-in-the-data-communities","text":"Download the slides Watch the video","title":"The rise of Python in the Data communities"},{"location":"Tech%20Talks/2018/pyconfr-2018/#leveraging-consistent-hashing-in-your-python-applications","text":"I updated it from my 2017 talk with a digression on Python 3.6 new dict implementation. Download the slides Watch the video","title":"Leveraging consistent hashing in your python applications"},{"location":"Tech%20Talks/2018/scylla-summit-2018/","text":"Scylla Summit 2018 \u00b6 Scylla User Award: Contribution to Scylla Open Source \u00b6 I'm proud to have been chosen for the Contribution to Scylla Open Source award during the summit. I've been contributing to Scylla through my Gentoo Linux packaging as well as to the Python code base, writing quite a bunch of blog posts and helping around in the general Slack channel. Replacing MongoDB and Hive with Scylla \u00b6 See the slides Watch the video","title":"Scylla Summit 2018"},{"location":"Tech%20Talks/2018/scylla-summit-2018/#scylla-summit-2018","text":"","title":"Scylla Summit 2018"},{"location":"Tech%20Talks/2018/scylla-summit-2018/#scylla-user-award-contribution-to-scylla-open-source","text":"I'm proud to have been chosen for the Contribution to Scylla Open Source award during the summit. I've been contributing to Scylla through my Gentoo Linux packaging as well as to the Python code base, writing quite a bunch of blog posts and helping around in the general Slack channel.","title":"Scylla User Award: Contribution to Scylla Open Source"},{"location":"Tech%20Talks/2018/scylla-summit-2018/#replacing-mongodb-and-hive-with-scylla","text":"See the slides Watch the video","title":"Replacing MongoDB and Hive with Scylla"},{"location":"Tech%20Talks/2019/europython-2019/","text":"EuroPython 2019 \u00b6 How we run GraphQL APIs in production on our Kubernetes cluster \u00b6 See the slides Watch the video","title":"EuroPython 2019"},{"location":"Tech%20Talks/2019/europython-2019/#europython-2019","text":"","title":"EuroPython 2019"},{"location":"Tech%20Talks/2019/europython-2019/#how-we-run-graphql-apis-in-production-on-our-kubernetes-cluster","text":"See the slides Watch the video","title":"How we run GraphQL APIs in production on our Kubernetes cluster"},{"location":"Tech%20Talks/2019/scylla-summit-2019/","text":"Scylla Summit 2019 \u00b6 Scylla User Award : Most innovative use of Scylla \u00b6 Quote AdTech pioneer Numberly has combined Scylla with Kafka Connect, Kafka Streams, Apache Spark and Python Faust, built on Gentoo Linux and deployed on bare-metal across multiple datacenters, all managed with Kubernetes. All of that resulted in reengineering a calculation process that used to take 72 hours but can now be delivered in just 10 seconds. MongoDB vs Scylla: Production Experience from Both Dev & Ops Standpoint \u00b6 See the slides Watch the video","title":"Scylla Summit 2019"},{"location":"Tech%20Talks/2019/scylla-summit-2019/#scylla-summit-2019","text":"","title":"Scylla Summit 2019"},{"location":"Tech%20Talks/2019/scylla-summit-2019/#scylla-user-award-most-innovative-use-of-scylla","text":"Quote AdTech pioneer Numberly has combined Scylla with Kafka Connect, Kafka Streams, Apache Spark and Python Faust, built on Gentoo Linux and deployed on bare-metal across multiple datacenters, all managed with Kubernetes. All of that resulted in reengineering a calculation process that used to take 72 hours but can now be delivered in just 10 seconds.","title":"Scylla User Award : Most innovative use of Scylla"},{"location":"Tech%20Talks/2019/scylla-summit-2019/#mongodb-vs-scylla-production-experience-from-both-dev-ops-standpoint","text":"See the slides Watch the video","title":"MongoDB vs Scylla: Production Experience from Both Dev &amp; Ops Standpoint"},{"location":"Tech%20Talks/2020/europython-2020/","text":"EuroPython 2020 \u00b6 A deep dive and comparison of Python drivers for Cassandra and Scylla \u00b6 Learn about Cassandra & Scylla architectural differences and how we modified the cassandra Python driver to create the scylla-driver that is capable of routing CQL queries down to nodes CPUs! See the slides Watch the video","title":"EuroPython 2020"},{"location":"Tech%20Talks/2020/europython-2020/#europython-2020","text":"","title":"EuroPython 2020"},{"location":"Tech%20Talks/2020/europython-2020/#a-deep-dive-and-comparison-of-python-drivers-for-cassandra-and-scylla","text":"Learn about Cassandra & Scylla architectural differences and how we modified the cassandra Python driver to create the scylla-driver that is capable of routing CQL queries down to nodes CPUs! See the slides Watch the video","title":"A deep dive and comparison of Python drivers for Cassandra and Scylla"},{"location":"Tech%20Talks/2020/pycon-india-2020/","text":"PyCon India 2020 \u00b6 A deep dive and comparison of Python drivers for Cassandra and Scylla \u00b6 I gave an updated talk from the EuroPython 2020 one sharing the recent developments of the Scylla Python driver. See the slides Watch the video","title":"PyCon India 2020"},{"location":"Tech%20Talks/2020/pycon-india-2020/#pycon-india-2020","text":"","title":"PyCon India 2020"},{"location":"Tech%20Talks/2020/pycon-india-2020/#a-deep-dive-and-comparison-of-python-drivers-for-cassandra-and-scylla","text":"I gave an updated talk from the EuroPython 2020 one sharing the recent developments of the Scylla Python driver. See the slides Watch the video","title":"A deep dive and comparison of Python drivers for Cassandra and Scylla"},{"location":"Tech%20Talks/2020/scylla-university/","text":"Scylla University \u00b6 My presentation of the Scylla Token Ring Architecture and introduction to the Scylla specific drivers are now a part of the Scylla University courses! ScyllaDB has also used my 2020 talks material to write two blog posts about how we wrote the Scylla Python driver. Making a Shard-Aware Python Driver for Scylla, Part 1 Making a Shard-Aware Python Driver for Scylla, Part 2","title":"Scylla University"},{"location":"Tech%20Talks/2020/scylla-university/#scylla-university","text":"My presentation of the Scylla Token Ring Architecture and introduction to the Scylla specific drivers are now a part of the Scylla University courses! ScyllaDB has also used my 2020 talks material to write two blog posts about how we wrote the Scylla Python driver. Making a Shard-Aware Python Driver for Scylla, Part 1 Making a Shard-Aware Python Driver for Scylla, Part 2","title":"Scylla University"},{"location":"Tech%20Talks/2021/scylla-summit-2021/","text":"Scylla Summit 2021 \u00b6 Scylla User Award : Community Member of the Year \u00b6 I'm very proud to have been awared by the Scylla community for the third time! Quote Scylla Community Member of the Year: Alexys Jacob, Numberly Alexys Jacob, CTO of Numberly, is a familiar name for those who track the Scylla User Awards. This year he is recognized for partnering with our engineers for the development of a shard-aware driver written in Python. This new driver provides significantly better database performance for users of the popular Python language. You can follow the course of this development in part one and part two of our blog series on the Python shard-aware driver, and also watch Alexys\u2019 Scylla Summit presentation on Getting the Scylla Shard-Aware Drivers Faster. Through my work and contributions to the Python Scylla shard-aware driver I found some caveats that could benefit all the others Scylla shard-aware drivers. This inspired a new advanced shard-aware CQL port that Scylla 4.3+ now offers to such newer drivers. The benefit? Faster drivers, faster queries! Getting the Scylla Shard-Aware drivers faster \u00b6 This talk is a 10min talk on newer shard-aware drivers !","title":"Scylla Summit 2021"},{"location":"Tech%20Talks/2021/scylla-summit-2021/#scylla-summit-2021","text":"","title":"Scylla Summit 2021"},{"location":"Tech%20Talks/2021/scylla-summit-2021/#scylla-user-award-community-member-of-the-year","text":"I'm very proud to have been awared by the Scylla community for the third time! Quote Scylla Community Member of the Year: Alexys Jacob, Numberly Alexys Jacob, CTO of Numberly, is a familiar name for those who track the Scylla User Awards. This year he is recognized for partnering with our engineers for the development of a shard-aware driver written in Python. This new driver provides significantly better database performance for users of the popular Python language. You can follow the course of this development in part one and part two of our blog series on the Python shard-aware driver, and also watch Alexys\u2019 Scylla Summit presentation on Getting the Scylla Shard-Aware Drivers Faster. Through my work and contributions to the Python Scylla shard-aware driver I found some caveats that could benefit all the others Scylla shard-aware drivers. This inspired a new advanced shard-aware CQL port that Scylla 4.3+ now offers to such newer drivers. The benefit? Faster drivers, faster queries!","title":"Scylla User Award : Community Member of the Year"},{"location":"Tech%20Talks/2021/scylla-summit-2021/#getting-the-scylla-shard-aware-drivers-faster","text":"This talk is a 10min talk on newer shard-aware drivers !","title":"Getting the Scylla Shard-Aware drivers faster"}]}